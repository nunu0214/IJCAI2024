{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","collapsed_sections":["yXwXG1r0yP__","-Vj68rbBEKES","VV4alT_R22se","UQxAwmUYnWkp","SjjX6nDgnKwI","Ktwbrdh7m_gv"],"machine_shape":"hm","mount_file_id":"1z-xBvHhYNBwMhw3JkinEJ8xX6MDi7zlV","authorship_tag":"ABX9TyOCEbwK3jdb4Zo9bmEJFdAV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djpBLPTRl2fW","executionInfo":{"status":"ok","timestamp":1721011860660,"user_tz":-480,"elapsed":1504,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"dcb83215-c0ea-4212-d207-7cc476e49c2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}],"source":["!python -V\n"]},{"cell_type":"markdown","source":["# 导入所需要环境"],"metadata":{"id":"yXwXG1r0yP__"}},{"cell_type":"code","source":["!pip3 install einops matplotlib matplotlib scikit_learn scipy scipy open3d meshio"],"metadata":{"id":"76-vyRjomPMh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721011889856,"user_tz":-480,"elapsed":27995,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"ed897210-9157-4d95-b7a3-e36edf627404"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n","Requirement already satisfied: scikit_learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n","Collecting open3d\n","  Downloading open3d-0.18.0-cp310-cp310-manylinux_2_27_x86_64.whl (399.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting meshio\n","  Downloading meshio-5.3.5-py3-none-any.whl (166 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.2/166.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn) (3.5.0)\n","Collecting dash>=2.6.0 (from open3d)\n","  Downloading dash-2.17.1-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from open3d) (3.0.3)\n","Requirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (5.10.4)\n","Collecting configargparse (from open3d)\n","  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n","Collecting ipywidgets>=8.0.4 (from open3d)\n","  Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting addict (from open3d)\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from open3d) (2.0.3)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from open3d) (6.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open3d) (4.66.4)\n","Collecting pyquaternion (from open3d)\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from meshio) (13.7.1)\n","Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.2.5)\n","Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (5.15.0)\n","Collecting dash-html-components==2.0.0 (from dash>=2.6.0->open3d)\n","  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n","Collecting dash-core-components==2.0.0 (from dash>=2.6.0->open3d)\n","  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n","Collecting dash-table==5.0.0 (from dash>=2.6.0->open3d)\n","  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (8.0.0)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (4.12.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (2.31.0)\n","Collecting retrying (from dash>=2.6.0->open3d)\n","  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.6.0->open3d) (67.7.2)\n","Collecting comm>=0.1.3 (from ipywidgets>=8.0.4->open3d)\n","  Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\n","Collecting widgetsnbextension~=4.0.11 (from ipywidgets>=8.0.4->open3d)\n","  Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0.11 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.11)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (4.19.2)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->open3d) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.5)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->meshio) (2.16.1)\n","Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (2.2.0)\n","Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d) (8.1.7)\n","Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.47)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.19.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.2.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->meshio) (0.1.2)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.19.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.6.0->open3d) (2024.7.4)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\n","Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, retrying, pyquaternion, jedi, einops, configargparse, comm, meshio, ipywidgets, dash, open3d\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.7\n","    Uninstalling widgetsnbextension-3.6.7:\n","      Successfully uninstalled widgetsnbextension-3.6.7\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","Successfully installed addict-2.4.0 comm-0.2.2 configargparse-1.7 dash-2.17.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 einops-0.8.0 ipywidgets-8.1.3 jedi-0.19.1 meshio-5.3.5 open3d-0.18.0 pyquaternion-0.9.9 retrying-1.3.4 widgetsnbextension-4.0.11\n"]}]},{"cell_type":"markdown","source":["# 下载官方数据"],"metadata":{"id":"v8Fimlw_yZj_"}},{"cell_type":"code","source":["# !wget --header=\"Host: drive.usercontent.google.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Cookie: __Secure-ENID=12.SE=Yd0Bj-CLJ14fnd4qzdJHmwUs4B5zz46UaPC1cPJigNqqFV9PtM2CYyBpSbCkOyzUwzlEdZ1nZFf-igtGi7wSdJ_gqQSfQfh84r9egqFQAy9-GKayCRbdQKdera-2mkpuIT-c64CyR9vfNojM3hxZ9Dej-dGvtxlGjal9ttEHybw; __gsas=ID=ae0421b9a34b478c:T=1710758437:RT=1710758437:S=ALNI_MZP13R9ZOHbCzC0rgHSMrGXj6GCsg; HSID=A-4I-ZudDNUIB6EKH; SSID=A7v_1v9un6xAwVNku; APISID=ctK8IbLjeuDUmgys/AFnMSLWt9KddceDI6; SAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; __Secure-1PAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; __Secure-3PAPISID=J7GhTwED67EBqJJT/A9nwK7mr0ijGPw08r; SID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_kzuBV1TvOhAIC8VF1e9fpgACgYKATQSARQSFQHGX2Mi8LXUwWoIwNCEPU8Sy3mXUxoVAUF8yKqGXVfjTGz9gQal7nwGr4Pl0076; __Secure-1PSID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_PDa-DzVmbdGFPyxMQpk9_QACgYKAewSARQSFQHGX2MiAeee4fn0OWglWZfAygqkyBoVAUF8yKp-Sfmtnueimxc-0QbJRF9I0076; __Secure-3PSID=g.a000kgiBabgKCiCYKve9zfoWVgz9eu8sBA6N4XDPPpP5pcW16_C_g9IrMeU98APBo9Stp6wEnAACgYKAQASARQSFQHGX2MiFWtc9ucONXnpxBzlRdudEhoVAUF8yKoeZwCpJDnjfAFjGssHSUGm0076; NID=515=GQhY9nKKFCx3qFDjE0MA4ubjWNdef6xCIY_RfWOPWKEtyfBN3nAUl8WHI2VczjNQ4rVkj1XBAY8WNWHXyqSK10CfT4FxsFlPzrHIJpeTtm1nWRNBd9AAfBKJHz4XpESszntVUTE_59RklZuKo0vk1poReVi2da1PZKC3CTKH2Ll3gB5xuB9wf4bmq8ylVUuIROPJczr0XnCuUHV3qLdBvgy9_870b6UwOq1iOlIxFQFm01EZ4pqF4q1Ub3QRSWpEMLh4LSZFpJ5O255R5OV7krmEdDvH_sHoTEPZAg2PoEpwAyGK6Xp9qcLIlldgx5-5V86N8Wtb93uTlQuA_CFXb5_2eP3bgeX8txwlJ5SrldVjg9ctzYtBU2RwJKTSvdHfIG7lpOkg6XlkvDOcJpR3DihT_OlqnPn7drCAJpvVDv29hZn5XPMXaSrNdbG64OJ9urJEw5odEwsLYkkpC1vmlUcuoo52S5f6RQu0Z8kZiV8iRW6XIqHsSmQHunVaxk6xWCStUg; __Secure-1PSIDTS=sidts-CjEB3EgAEtTS0OazynCofIH4RCBstiRP5flEcvYW3z4Fg9oGd5QOESDOZt1wO2iqUYHjEAA; __Secure-3PSIDTS=sidts-CjEB3EgAEtTS0OazynCofIH4RCBstiRP5flEcvYW3z4Fg9oGd5QOESDOZt1wO2iqUYHjEAA; SIDCC=AKEyXzVI6aMX8lSDja86Yts3FBAtBzPCzVNgaX5BCz78NWsWzlT3yFWKUV7ZE46SFzE1GiBI-cHdTw; __Secure-1PSIDCC=AKEyXzUo4NQAwqqPMxP2eye-MFEbZmBIm_sZqRU1amttg0YoQkc8ZKSNXdHl5jNCMEbhrUHhS9-K; __Secure-3PSIDCC=AKEyXzWf2lIdmDLeZKpXSi9GytVQb6XudrYiNUBA5gW952YuLh8kL6T3IbBlu8zOTfGEcdUp5O1R\" --header=\"Connection: keep-alive\" \"https://drive.usercontent.google.com/download?id=1JwR0Q1ArTg6c47EF2ZuIBpQwCPgXKrO2&export=download&authuser=0&confirm=t&uuid=dc3aa13c-c3a9-458f-983a-8586798cb635&at=APZUnTX25XMxi-z-3wBcgR93IGsL%3A1719235792953\" -c -O 'Dataset.zip'\n","# !unzip Dataset.zip"],"metadata":{"id":"-iCVuuqnQACM","executionInfo":{"status":"ok","timestamp":1721011889856,"user_tz":-480,"elapsed":4,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["\n","!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/1638f9c292b9437bb46885186407a63e584856c91f9f4c18908b87abd46471e0?responseContentDisposition=attachment%3B%20filename%3Dtrack_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A02%3A25Z%2F-1%2F%2Fcfdfd6b6a9e096c761ee8e7d863d586741c69a9e6de89f9c3696706d35f8b265\" -c -O 'track_B.zip'\n","!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/2dddd05e577849ad95e1fe1133d3af29d13085ac0cfd499c853ff5d9df2ac07f?responseContentDisposition=attachment%3B%20filename%3Dtrain_data.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A23%3A33Z%2F-1%2F%2F8540633c7e39fddf8471d6d8206c3b761748c58c06005acb218593a8df19d7f1\" -c -O 'train_data.zip'\n","!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/a96dc8ba8201445b966980a0a48f52705338a48e29e64c53bddb7ef8861c5123?responseContentDisposition=attachment%3B%20filename%3Dtrack_A.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-06T07%3A54%3A51Z%2F-1%2F%2F17b5155bd16a8af1e4af971498082687656af7fcecfc5a8e57591b85053210ec\" -c -O 'track_A.zip'\n","# !wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/dcaba9f0e87549e395e1682a4a0a43c547a896034e3f4417a4d10ba85a949944?responseContentDisposition=attachment%3B%20filename%3DPaddleScience.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-11T16%3A01%3A57Z%2F-1%2F%2Fb2149791689d3b19c02a86782229901cb75743d07c7163dfa185fea286cd1f02\" -c -O 'PaddleScience.zip'\n","!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/38e9adf0fce84527aad3558cc3e82d0e9a251aac4c934297afae9b74d9b3d1e9?responseContentDisposition=attachment%3B%20filename%3Dtrain_track_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-04T03%3A21%3A02Z%2F-1%2F%2Facd359add161bace603a52c7a268467406cb3c1889a7114bbb687de8002b55f6\" -c -O 'train_track_B.zip'\n","!wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/7877c2fd300345599ed3365feda50425f4857caa71bf4af9bf047fb08e35aa97?responseContentDisposition=attachment%3B%20filename%3Dmesh_B_0603.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-04T11%3A48%3A26Z%2F-1%2F%2Fec6cce492ba6afff841ef197860065742a7ba8220def02c1cbc311333aa993b2\" -c -O 'mesh_B_0603.zip'\n","# !wget --header=\"Host: ai-studio-online.bj.bcebos.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\" --header=\"Referer: https://aistudio.baidu.com/\" \"https://ai-studio-online.bj.bcebos.com/v1/a02dba5700974c6a811f579fff216ccf9a4129b849994dcca99390b222c28572?responseContentDisposition=attachment%3B%20filename%3D3rd_lib.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-08T05%3A29%3A20Z%2F-1%2F%2Fef71819149664a3c8438d3dfd02544e77cdb4fda679efbdd402b4e2c77a2c06d\" -c -O '3rd_lib.zip'"],"metadata":{"id":"5mbGmjSnQqYJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721013943648,"user_tz":-480,"elapsed":2053795,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"9263951c-a8d9-40e5-c959-1ed43a6807bf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-15 02:51:29--  https://ai-studio-online.bj.bcebos.com/v1/1638f9c292b9437bb46885186407a63e584856c91f9f4c18908b87abd46471e0?responseContentDisposition=attachment%3B%20filename%3Dtrack_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A02%3A25Z%2F-1%2F%2Fcfdfd6b6a9e096c761ee8e7d863d586741c69a9e6de89f9c3696706d35f8b265\n","Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 103.235.47.176, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n","Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|103.235.47.176|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1012191818 (965M) [application/octet-stream]\n","Saving to: ‘track_B.zip’\n","\n","track_B.zip         100%[===================>] 965.30M  12.3MB/s    in 1m 41s  \n","\n","2024-07-15 02:53:12 (9.55 MB/s) - ‘track_B.zip’ saved [1012191818/1012191818]\n","\n","--2024-07-15 02:53:12--  https://ai-studio-online.bj.bcebos.com/v1/2dddd05e577849ad95e1fe1133d3af29d13085ac0cfd499c853ff5d9df2ac07f?responseContentDisposition=attachment%3B%20filename%3Dtrain_data.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-05T03%3A23%3A33Z%2F-1%2F%2F8540633c7e39fddf8471d6d8206c3b761748c58c06005acb218593a8df19d7f1\n","Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 103.235.47.176, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n","Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|103.235.47.176|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 46031310 (44M) [application/octet-stream]\n","Saving to: ‘train_data.zip’\n","\n","train_data.zip      100%[===================>]  43.90M  10.3MB/s    in 22s     \n","\n","2024-07-15 02:53:36 (2.02 MB/s) - ‘train_data.zip’ saved [46031310/46031310]\n","\n","--2024-07-15 02:53:36--  https://ai-studio-online.bj.bcebos.com/v1/a96dc8ba8201445b966980a0a48f52705338a48e29e64c53bddb7ef8861c5123?responseContentDisposition=attachment%3B%20filename%3Dtrack_A.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-05-06T07%3A54%3A51Z%2F-1%2F%2F17b5155bd16a8af1e4af971498082687656af7fcecfc5a8e57591b85053210ec\n","Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 103.235.47.176, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n","Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|103.235.47.176|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4688102 (4.5M) [application/octet-stream]\n","Saving to: ‘track_A.zip’\n","\n","track_A.zip         100%[===================>]   4.47M  1.01MB/s    in 17s     \n","\n","2024-07-15 02:53:54 (276 KB/s) - ‘track_A.zip’ saved [4688102/4688102]\n","\n","--2024-07-15 02:53:54--  https://ai-studio-online.bj.bcebos.com/v1/38e9adf0fce84527aad3558cc3e82d0e9a251aac4c934297afae9b74d9b3d1e9?responseContentDisposition=attachment%3B%20filename%3Dtrain_track_B.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-04T03%3A21%3A02Z%2F-1%2F%2Facd359add161bace603a52c7a268467406cb3c1889a7114bbb687de8002b55f6\n","Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 103.235.47.176, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n","Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|103.235.47.176|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4740031429 (4.4G) [application/octet-stream]\n","Saving to: ‘train_track_B.zip’\n","\n","train_track_B.zip   100%[===================>]   4.41G  3.64MB/s    in 26m 37s \n","\n","2024-07-15 03:20:33 (2.83 MB/s) - ‘train_track_B.zip’ saved [4740031429/4740031429]\n","\n","--2024-07-15 03:20:33--  https://ai-studio-online.bj.bcebos.com/v1/7877c2fd300345599ed3365feda50425f4857caa71bf4af9bf047fb08e35aa97?responseContentDisposition=attachment%3B%20filename%3Dmesh_B_0603.zip&authorization=bce-auth-v1%2F5cfe9a5e1454405eb2a975c43eace6ec%2F2024-06-04T11%3A48%3A26Z%2F-1%2F%2Fec6cce492ba6afff841ef197860065742a7ba8220def02c1cbc311333aa993b2\n","Resolving ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)... 103.235.47.176, 2409:8c04:1001:1203:0:ff:b0bb:4f27\n","Connecting to ai-studio-online.bj.bcebos.com (ai-studio-online.bj.bcebos.com)|103.235.47.176|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4217480891 (3.9G) [application/octet-stream]\n","Saving to: ‘mesh_B_0603.zip’\n","\n","mesh_B_0603.zip     100%[===================>]   3.93G  14.5MB/s    in 5m 8s   \n","\n","2024-07-15 03:25:43 (13.1 MB/s) - ‘mesh_B_0603.zip’ saved [4217480891/4217480891]\n","\n"]}]},{"cell_type":"code","source":["!mkdir Dataset\n","!unzip -o track_B.zip -d Dataset/\n","!unzip -o train_data.zip -d Dataset/\n","!unzip -o track_A.zip -d Dataset/\n","!mkdir Dataset/train_track_B && unzip -o train_track_B.zip -d Dataset/train_track_B/\n","!unzip -o mesh_B_0603.zip -d Dataset/\n"],"metadata":{"id":"d0-rRJVfQ5Ge","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721014121176,"user_tz":-480,"elapsed":176716,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"9c54d5b5-30d2-48dc-a713-70d6ccdedfbc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  track_B.zip\n","  inflating: Dataset/track_B/area_1.npy  \n","  inflating: Dataset/track_B/area_10.npy  \n","  inflating: Dataset/track_B/area_11.npy  \n","  inflating: Dataset/track_B/area_12.npy  \n","  inflating: Dataset/track_B/area_13.npy  \n","  inflating: Dataset/track_B/area_14.npy  \n","  inflating: Dataset/track_B/area_15.npy  \n","  inflating: Dataset/track_B/area_16.npy  \n","  inflating: Dataset/track_B/area_17.npy  \n","  inflating: Dataset/track_B/area_18.npy  \n","  inflating: Dataset/track_B/area_19.npy  \n","  inflating: Dataset/track_B/area_2.npy  \n","  inflating: Dataset/track_B/area_20.npy  \n","  inflating: Dataset/track_B/area_21.npy  \n","  inflating: Dataset/track_B/area_22.npy  \n","  inflating: Dataset/track_B/area_23.npy  \n","  inflating: Dataset/track_B/area_24.npy  \n","  inflating: Dataset/track_B/area_25.npy  \n","  inflating: Dataset/track_B/area_26.npy  \n","  inflating: Dataset/track_B/area_27.npy  \n","  inflating: Dataset/track_B/area_28.npy  \n","  inflating: Dataset/track_B/area_29.npy  \n","  inflating: Dataset/track_B/area_3.npy  \n","  inflating: Dataset/track_B/area_30.npy  \n","  inflating: Dataset/track_B/area_31.npy  \n","  inflating: Dataset/track_B/area_32.npy  \n","  inflating: Dataset/track_B/area_33.npy  \n","  inflating: Dataset/track_B/area_34.npy  \n","  inflating: Dataset/track_B/area_35.npy  \n","  inflating: Dataset/track_B/area_36.npy  \n","  inflating: Dataset/track_B/area_37.npy  \n","  inflating: Dataset/track_B/area_38.npy  \n","  inflating: Dataset/track_B/area_39.npy  \n","  inflating: Dataset/track_B/area_4.npy  \n","  inflating: Dataset/track_B/area_40.npy  \n","  inflating: Dataset/track_B/area_41.npy  \n","  inflating: Dataset/track_B/area_42.npy  \n","  inflating: Dataset/track_B/area_43.npy  \n","  inflating: Dataset/track_B/area_44.npy  \n","  inflating: Dataset/track_B/area_45.npy  \n","  inflating: Dataset/track_B/area_46.npy  \n","  inflating: Dataset/track_B/area_47.npy  \n","  inflating: Dataset/track_B/area_48.npy  \n","  inflating: Dataset/track_B/area_49.npy  \n","  inflating: Dataset/track_B/area_5.npy  \n","  inflating: Dataset/track_B/area_50.npy  \n","  inflating: Dataset/track_B/area_6.npy  \n","  inflating: Dataset/track_B/area_7.npy  \n","  inflating: Dataset/track_B/area_8.npy  \n","  inflating: Dataset/track_B/area_9.npy  \n","  inflating: Dataset/track_B/area_bounds.txt  \n","  inflating: Dataset/track_B/centroid_1.npy  \n","  inflating: Dataset/track_B/centroid_10.npy  \n","  inflating: Dataset/track_B/centroid_11.npy  \n","  inflating: Dataset/track_B/centroid_12.npy  \n","  inflating: Dataset/track_B/centroid_13.npy  \n","  inflating: Dataset/track_B/centroid_14.npy  \n","  inflating: Dataset/track_B/centroid_15.npy  \n","  inflating: Dataset/track_B/centroid_16.npy  \n","  inflating: Dataset/track_B/centroid_17.npy  \n","  inflating: Dataset/track_B/centroid_18.npy  \n","  inflating: Dataset/track_B/centroid_19.npy  \n","  inflating: Dataset/track_B/centroid_2.npy  \n","  inflating: Dataset/track_B/centroid_20.npy  \n","  inflating: Dataset/track_B/centroid_21.npy  \n","  inflating: Dataset/track_B/centroid_22.npy  \n","  inflating: Dataset/track_B/centroid_23.npy  \n","  inflating: Dataset/track_B/centroid_24.npy  \n","  inflating: Dataset/track_B/centroid_25.npy  \n","  inflating: Dataset/track_B/centroid_26.npy  \n","  inflating: Dataset/track_B/centroid_27.npy  \n","  inflating: Dataset/track_B/centroid_28.npy  \n","  inflating: Dataset/track_B/centroid_29.npy  \n","  inflating: Dataset/track_B/centroid_3.npy  \n","  inflating: Dataset/track_B/centroid_30.npy  \n","  inflating: Dataset/track_B/centroid_31.npy  \n","  inflating: Dataset/track_B/centroid_32.npy  \n","  inflating: Dataset/track_B/centroid_33.npy  \n","  inflating: Dataset/track_B/centroid_34.npy  \n","  inflating: Dataset/track_B/centroid_35.npy  \n","  inflating: Dataset/track_B/centroid_36.npy  \n","  inflating: Dataset/track_B/centroid_37.npy  \n","  inflating: Dataset/track_B/centroid_38.npy  \n","  inflating: Dataset/track_B/centroid_39.npy  \n","  inflating: Dataset/track_B/centroid_4.npy  \n","  inflating: Dataset/track_B/centroid_40.npy  \n","  inflating: Dataset/track_B/centroid_41.npy  \n","  inflating: Dataset/track_B/centroid_42.npy  \n","  inflating: Dataset/track_B/centroid_43.npy  \n","  inflating: Dataset/track_B/centroid_44.npy  \n","  inflating: Dataset/track_B/centroid_45.npy  \n","  inflating: Dataset/track_B/centroid_46.npy  \n","  inflating: Dataset/track_B/centroid_47.npy  \n","  inflating: Dataset/track_B/centroid_48.npy  \n","  inflating: Dataset/track_B/centroid_49.npy  \n","  inflating: Dataset/track_B/centroid_5.npy  \n","  inflating: Dataset/track_B/centroid_50.npy  \n","  inflating: Dataset/track_B/centroid_6.npy  \n","  inflating: Dataset/track_B/centroid_7.npy  \n","  inflating: Dataset/track_B/centroid_8.npy  \n","  inflating: Dataset/track_B/centroid_9.npy  \n","  inflating: Dataset/track_B/global_bounds.txt  \n","  inflating: Dataset/track_B/info_1.npy  \n","  inflating: Dataset/track_B/info_10.npy  \n","  inflating: Dataset/track_B/info_11.npy  \n","  inflating: Dataset/track_B/info_12.npy  \n","  inflating: Dataset/track_B/info_13.npy  \n","  inflating: Dataset/track_B/info_14.npy  \n","  inflating: Dataset/track_B/info_15.npy  \n","  inflating: Dataset/track_B/info_16.npy  \n","  inflating: Dataset/track_B/info_17.npy  \n","  inflating: Dataset/track_B/info_18.npy  \n","  inflating: Dataset/track_B/info_19.npy  \n","  inflating: Dataset/track_B/info_2.npy  \n","  inflating: Dataset/track_B/info_20.npy  \n","  inflating: Dataset/track_B/info_21.npy  \n","  inflating: Dataset/track_B/info_22.npy  \n","  inflating: Dataset/track_B/info_23.npy  \n","  inflating: Dataset/track_B/info_24.npy  \n","  inflating: Dataset/track_B/info_25.npy  \n","  inflating: Dataset/track_B/info_26.npy  \n","  inflating: Dataset/track_B/info_27.npy  \n","  inflating: Dataset/track_B/info_28.npy  \n","  inflating: Dataset/track_B/info_29.npy  \n","  inflating: Dataset/track_B/info_3.npy  \n","  inflating: Dataset/track_B/info_30.npy  \n","  inflating: Dataset/track_B/info_31.npy  \n","  inflating: Dataset/track_B/info_32.npy  \n","  inflating: Dataset/track_B/info_33.npy  \n","  inflating: Dataset/track_B/info_34.npy  \n","  inflating: Dataset/track_B/info_35.npy  \n","  inflating: Dataset/track_B/info_36.npy  \n","  inflating: Dataset/track_B/info_37.npy  \n","  inflating: Dataset/track_B/info_38.npy  \n","  inflating: Dataset/track_B/info_39.npy  \n","  inflating: Dataset/track_B/info_4.npy  \n","  inflating: Dataset/track_B/info_40.npy  \n","  inflating: Dataset/track_B/info_41.npy  \n","  inflating: Dataset/track_B/info_42.npy  \n","  inflating: Dataset/track_B/info_43.npy  \n","  inflating: Dataset/track_B/info_44.npy  \n","  inflating: Dataset/track_B/info_45.npy  \n","  inflating: Dataset/track_B/info_46.npy  \n","  inflating: Dataset/track_B/info_47.npy  \n","  inflating: Dataset/track_B/info_48.npy  \n","  inflating: Dataset/track_B/info_49.npy  \n","  inflating: Dataset/track_B/info_5.npy  \n","  inflating: Dataset/track_B/info_50.npy  \n","  inflating: Dataset/track_B/info_6.npy  \n","  inflating: Dataset/track_B/info_7.npy  \n","  inflating: Dataset/track_B/info_8.npy  \n","  inflating: Dataset/track_B/info_9.npy  \n","  inflating: Dataset/track_B/info_bounds.txt  \n","  inflating: Dataset/track_B/mesh_1.ply  \n","  inflating: Dataset/track_B/mesh_10.ply  \n","  inflating: Dataset/track_B/mesh_11.ply  \n","  inflating: Dataset/track_B/mesh_12.ply  \n","  inflating: Dataset/track_B/mesh_13.ply  \n","  inflating: Dataset/track_B/mesh_14.ply  \n","  inflating: Dataset/track_B/mesh_15.ply  \n","  inflating: Dataset/track_B/mesh_16.ply  \n","  inflating: Dataset/track_B/mesh_17.ply  \n","  inflating: Dataset/track_B/mesh_18.ply  \n","  inflating: Dataset/track_B/mesh_19.ply  \n","  inflating: Dataset/track_B/mesh_2.ply  \n","  inflating: Dataset/track_B/mesh_20.ply  \n","  inflating: Dataset/track_B/mesh_21.ply  \n","  inflating: Dataset/track_B/mesh_22.ply  \n","  inflating: Dataset/track_B/mesh_23.ply  \n","  inflating: Dataset/track_B/mesh_24.ply  \n","  inflating: Dataset/track_B/mesh_25.ply  \n","  inflating: Dataset/track_B/mesh_26.ply  \n","  inflating: Dataset/track_B/mesh_27.ply  \n","  inflating: Dataset/track_B/mesh_28.ply  \n","  inflating: Dataset/track_B/mesh_29.ply  \n","  inflating: Dataset/track_B/mesh_3.ply  \n","  inflating: Dataset/track_B/mesh_30.ply  \n","  inflating: Dataset/track_B/mesh_31.ply  \n","  inflating: Dataset/track_B/mesh_32.ply  \n","  inflating: Dataset/track_B/mesh_33.ply  \n","  inflating: Dataset/track_B/mesh_34.ply  \n","  inflating: Dataset/track_B/mesh_35.ply  \n","  inflating: Dataset/track_B/mesh_36.ply  \n","  inflating: Dataset/track_B/mesh_37.ply  \n","  inflating: Dataset/track_B/mesh_38.ply  \n","  inflating: Dataset/track_B/mesh_39.ply  \n","  inflating: Dataset/track_B/mesh_4.ply  \n","  inflating: Dataset/track_B/mesh_40.ply  \n","  inflating: Dataset/track_B/mesh_41.ply  \n","  inflating: Dataset/track_B/mesh_42.ply  \n","  inflating: Dataset/track_B/mesh_43.ply  \n","  inflating: Dataset/track_B/mesh_44.ply  \n","  inflating: Dataset/track_B/mesh_45.ply  \n","  inflating: Dataset/track_B/mesh_46.ply  \n","  inflating: Dataset/track_B/mesh_47.ply  \n","  inflating: Dataset/track_B/mesh_48.ply  \n","  inflating: Dataset/track_B/mesh_49.ply  \n","  inflating: Dataset/track_B/mesh_5.ply  \n","  inflating: Dataset/track_B/mesh_50.ply  \n","  inflating: Dataset/track_B/mesh_6.ply  \n","  inflating: Dataset/track_B/mesh_7.ply  \n","  inflating: Dataset/track_B/mesh_8.ply  \n","  inflating: Dataset/track_B/mesh_9.ply  \n","  inflating: Dataset/track_B/normal_1.npy  \n","  inflating: Dataset/track_B/normal_10.npy  \n","  inflating: Dataset/track_B/normal_11.npy  \n","  inflating: Dataset/track_B/normal_12.npy  \n","  inflating: Dataset/track_B/normal_13.npy  \n","  inflating: Dataset/track_B/normal_14.npy  \n","  inflating: Dataset/track_B/normal_15.npy  \n","  inflating: Dataset/track_B/normal_16.npy  \n","  inflating: Dataset/track_B/normal_17.npy  \n","  inflating: Dataset/track_B/normal_18.npy  \n","  inflating: Dataset/track_B/normal_19.npy  \n","  inflating: Dataset/track_B/normal_2.npy  \n","  inflating: Dataset/track_B/normal_20.npy  \n","  inflating: Dataset/track_B/normal_21.npy  \n","  inflating: Dataset/track_B/normal_22.npy  \n","  inflating: Dataset/track_B/normal_23.npy  \n","  inflating: Dataset/track_B/normal_24.npy  \n","  inflating: Dataset/track_B/normal_25.npy  \n","  inflating: Dataset/track_B/normal_26.npy  \n","  inflating: Dataset/track_B/normal_27.npy  \n","  inflating: Dataset/track_B/normal_28.npy  \n","  inflating: Dataset/track_B/normal_29.npy  \n","  inflating: Dataset/track_B/normal_3.npy  \n","  inflating: Dataset/track_B/normal_30.npy  \n","  inflating: Dataset/track_B/normal_31.npy  \n","  inflating: Dataset/track_B/normal_32.npy  \n","  inflating: Dataset/track_B/normal_33.npy  \n","  inflating: Dataset/track_B/normal_34.npy  \n","  inflating: Dataset/track_B/normal_35.npy  \n","  inflating: Dataset/track_B/normal_36.npy  \n","  inflating: Dataset/track_B/normal_37.npy  \n","  inflating: Dataset/track_B/normal_38.npy  \n","  inflating: Dataset/track_B/normal_39.npy  \n","  inflating: Dataset/track_B/normal_4.npy  \n","  inflating: Dataset/track_B/normal_40.npy  \n","  inflating: Dataset/track_B/normal_41.npy  \n","  inflating: Dataset/track_B/normal_42.npy  \n","  inflating: Dataset/track_B/normal_43.npy  \n","  inflating: Dataset/track_B/normal_44.npy  \n","  inflating: Dataset/track_B/normal_45.npy  \n","  inflating: Dataset/track_B/normal_46.npy  \n","  inflating: Dataset/track_B/normal_47.npy  \n","  inflating: Dataset/track_B/normal_48.npy  \n","  inflating: Dataset/track_B/normal_49.npy  \n","  inflating: Dataset/track_B/normal_5.npy  \n","  inflating: Dataset/track_B/normal_50.npy  \n","  inflating: Dataset/track_B/normal_6.npy  \n","  inflating: Dataset/track_B/normal_7.npy  \n","  inflating: Dataset/track_B/normal_8.npy  \n","  inflating: Dataset/track_B/normal_9.npy  \n","  inflating: Dataset/track_B/train_pressure_mean_std.txt  \n","Archive:  train_data.zip\n","  inflating: Dataset/data/mesh_001.ply  \n","  inflating: Dataset/data/mesh_002.ply  \n","  inflating: Dataset/data/mesh_004.ply  \n","  inflating: Dataset/data/mesh_005.ply  \n","  inflating: Dataset/data/mesh_006.ply  \n","  inflating: Dataset/data/mesh_007.ply  \n","  inflating: Dataset/data/mesh_008.ply  \n","  inflating: Dataset/data/mesh_010.ply  \n","  inflating: Dataset/data/mesh_012.ply  \n","  inflating: Dataset/data/mesh_013.ply  \n","  inflating: Dataset/data/mesh_017.ply  \n","  inflating: Dataset/data/mesh_018.ply  \n","  inflating: Dataset/data/mesh_021.ply  \n","  inflating: Dataset/data/mesh_022.ply  \n","  inflating: Dataset/data/mesh_023.ply  \n","  inflating: Dataset/data/mesh_025.ply  \n","  inflating: Dataset/data/mesh_026.ply  \n","  inflating: Dataset/data/mesh_027.ply  \n","  inflating: Dataset/data/mesh_028.ply  \n","  inflating: Dataset/data/mesh_029.ply  \n","  inflating: Dataset/data/mesh_030.ply  \n","  inflating: Dataset/data/mesh_031.ply  \n","  inflating: Dataset/data/mesh_032.ply  \n","  inflating: Dataset/data/mesh_034.ply  \n","  inflating: Dataset/data/mesh_035.ply  \n","  inflating: Dataset/data/mesh_039.ply  \n","  inflating: Dataset/data/mesh_040.ply  \n","  inflating: Dataset/data/mesh_043.ply  \n","  inflating: Dataset/data/mesh_044.ply  \n","  inflating: Dataset/data/mesh_045.ply  \n","  inflating: Dataset/data/mesh_046.ply  \n","  inflating: Dataset/data/mesh_047.ply  \n","  inflating: Dataset/data/mesh_048.ply  \n","  inflating: Dataset/data/mesh_049.ply  \n","  inflating: Dataset/data/mesh_050.ply  \n","  inflating: Dataset/data/mesh_051.ply  \n","  inflating: Dataset/data/mesh_052.ply  \n","  inflating: Dataset/data/mesh_054.ply  \n","  inflating: Dataset/data/mesh_055.ply  \n","  inflating: Dataset/data/mesh_056.ply  \n","  inflating: Dataset/data/mesh_058.ply  \n","  inflating: Dataset/data/mesh_059.ply  \n","  inflating: Dataset/data/mesh_060.ply  \n","  inflating: Dataset/data/mesh_061.ply  \n","  inflating: Dataset/data/mesh_062.ply  \n","  inflating: Dataset/data/mesh_063.ply  \n","  inflating: Dataset/data/mesh_064.ply  \n","  inflating: Dataset/data/mesh_065.ply  \n","  inflating: Dataset/data/mesh_067.ply  \n","  inflating: Dataset/data/mesh_069.ply  \n","  inflating: Dataset/data/mesh_070.ply  \n","  inflating: Dataset/data/mesh_071.ply  \n","  inflating: Dataset/data/mesh_072.ply  \n","  inflating: Dataset/data/mesh_073.ply  \n","  inflating: Dataset/data/mesh_074.ply  \n","  inflating: Dataset/data/mesh_075.ply  \n","  inflating: Dataset/data/mesh_076.ply  \n","  inflating: Dataset/data/mesh_077.ply  \n","  inflating: Dataset/data/mesh_078.ply  \n","  inflating: Dataset/data/mesh_079.ply  \n","  inflating: Dataset/data/mesh_080.ply  \n","  inflating: Dataset/data/mesh_081.ply  \n","  inflating: Dataset/data/mesh_083.ply  \n","  inflating: Dataset/data/mesh_084.ply  \n","  inflating: Dataset/data/mesh_085.ply  \n","  inflating: Dataset/data/mesh_086.ply  \n","  inflating: Dataset/data/mesh_087.ply  \n","  inflating: Dataset/data/mesh_088.ply  \n","  inflating: Dataset/data/mesh_090.ply  \n","  inflating: Dataset/data/mesh_091.ply  \n","  inflating: Dataset/data/mesh_092.ply  \n","  inflating: Dataset/data/mesh_094.ply  \n","  inflating: Dataset/data/mesh_095.ply  \n","  inflating: Dataset/data/mesh_096.ply  \n","  inflating: Dataset/data/mesh_097.ply  \n","  inflating: Dataset/data/mesh_100.ply  \n","  inflating: Dataset/data/mesh_101.ply  \n","  inflating: Dataset/data/mesh_102.ply  \n","  inflating: Dataset/data/mesh_105.ply  \n","  inflating: Dataset/data/mesh_106.ply  \n","  inflating: Dataset/data/mesh_107.ply  \n","  inflating: Dataset/data/mesh_109.ply  \n","  inflating: Dataset/data/mesh_110.ply  \n","  inflating: Dataset/data/mesh_111.ply  \n","  inflating: Dataset/data/mesh_112.ply  \n","  inflating: Dataset/data/mesh_113.ply  \n","  inflating: Dataset/data/mesh_114.ply  \n","  inflating: Dataset/data/mesh_115.ply  \n","  inflating: Dataset/data/mesh_116.ply  \n","  inflating: Dataset/data/mesh_117.ply  \n","  inflating: Dataset/data/mesh_118.ply  \n","  inflating: Dataset/data/mesh_119.ply  \n","  inflating: Dataset/data/mesh_120.ply  \n","  inflating: Dataset/data/mesh_121.ply  \n","  inflating: Dataset/data/mesh_123.ply  \n","  inflating: Dataset/data/mesh_124.ply  \n","  inflating: Dataset/data/mesh_125.ply  \n","  inflating: Dataset/data/mesh_126.ply  \n","  inflating: Dataset/data/mesh_127.ply  \n","  inflating: Dataset/data/mesh_128.ply  \n","  inflating: Dataset/data/mesh_129.ply  \n","  inflating: Dataset/data/mesh_130.ply  \n","  inflating: Dataset/data/mesh_131.ply  \n","  inflating: Dataset/data/mesh_133.ply  \n","  inflating: Dataset/data/mesh_134.ply  \n","  inflating: Dataset/data/mesh_136.ply  \n","  inflating: Dataset/data/mesh_137.ply  \n","  inflating: Dataset/data/mesh_138.ply  \n","  inflating: Dataset/data/mesh_139.ply  \n","  inflating: Dataset/data/mesh_140.ply  \n","  inflating: Dataset/data/mesh_141.ply  \n","  inflating: Dataset/data/mesh_142.ply  \n","  inflating: Dataset/data/mesh_143.ply  \n","  inflating: Dataset/data/mesh_144.ply  \n","  inflating: Dataset/data/mesh_145.ply  \n","  inflating: Dataset/data/mesh_146.ply  \n","  inflating: Dataset/data/mesh_147.ply  \n","  inflating: Dataset/data/mesh_148.ply  \n","  inflating: Dataset/data/mesh_149.ply  \n","  inflating: Dataset/data/mesh_150.ply  \n","  inflating: Dataset/data/mesh_151.ply  \n","  inflating: Dataset/data/mesh_152.ply  \n","  inflating: Dataset/data/mesh_153.ply  \n","  inflating: Dataset/data/mesh_155.ply  \n","  inflating: Dataset/data/mesh_156.ply  \n","  inflating: Dataset/data/mesh_157.ply  \n","  inflating: Dataset/data/mesh_158.ply  \n","  inflating: Dataset/data/mesh_159.ply  \n","  inflating: Dataset/data/mesh_160.ply  \n","  inflating: Dataset/data/mesh_161.ply  \n","  inflating: Dataset/data/mesh_162.ply  \n","  inflating: Dataset/data/mesh_163.ply  \n","  inflating: Dataset/data/mesh_165.ply  \n","  inflating: Dataset/data/mesh_166.ply  \n","  inflating: Dataset/data/mesh_170.ply  \n","  inflating: Dataset/data/mesh_172.ply  \n","  inflating: Dataset/data/mesh_173.ply  \n","  inflating: Dataset/data/mesh_175.ply  \n","  inflating: Dataset/data/mesh_176.ply  \n","  inflating: Dataset/data/mesh_177.ply  \n","  inflating: Dataset/data/mesh_178.ply  \n","  inflating: Dataset/data/mesh_179.ply  \n","  inflating: Dataset/data/mesh_180.ply  \n","  inflating: Dataset/data/mesh_181.ply  \n","  inflating: Dataset/data/mesh_182.ply  \n","  inflating: Dataset/data/mesh_183.ply  \n","  inflating: Dataset/data/mesh_184.ply  \n","  inflating: Dataset/data/mesh_186.ply  \n","  inflating: Dataset/data/mesh_190.ply  \n","  inflating: Dataset/data/mesh_191.ply  \n","  inflating: Dataset/data/mesh_192.ply  \n","  inflating: Dataset/data/mesh_193.ply  \n","  inflating: Dataset/data/mesh_195.ply  \n","  inflating: Dataset/data/mesh_196.ply  \n","  inflating: Dataset/data/mesh_198.ply  \n","  inflating: Dataset/data/mesh_199.ply  \n","  inflating: Dataset/data/mesh_200.ply  \n","  inflating: Dataset/data/mesh_201.ply  \n","  inflating: Dataset/data/mesh_202.ply  \n","  inflating: Dataset/data/mesh_203.ply  \n","  inflating: Dataset/data/mesh_205.ply  \n","  inflating: Dataset/data/mesh_207.ply  \n","  inflating: Dataset/data/mesh_210.ply  \n","  inflating: Dataset/data/mesh_211.ply  \n","  inflating: Dataset/data/mesh_212.ply  \n","  inflating: Dataset/data/mesh_213.ply  \n","  inflating: Dataset/data/mesh_214.ply  \n","  inflating: Dataset/data/mesh_215.ply  \n","  inflating: Dataset/data/mesh_217.ply  \n","  inflating: Dataset/data/mesh_219.ply  \n","  inflating: Dataset/data/mesh_220.ply  \n","  inflating: Dataset/data/mesh_221.ply  \n","  inflating: Dataset/data/mesh_222.ply  \n","  inflating: Dataset/data/mesh_223.ply  \n","  inflating: Dataset/data/mesh_224.ply  \n","  inflating: Dataset/data/mesh_225.ply  \n","  inflating: Dataset/data/mesh_227.ply  \n","  inflating: Dataset/data/mesh_228.ply  \n","  inflating: Dataset/data/mesh_229.ply  \n","  inflating: Dataset/data/mesh_230.ply  \n","  inflating: Dataset/data/mesh_231.ply  \n","  inflating: Dataset/data/mesh_232.ply  \n","  inflating: Dataset/data/mesh_233.ply  \n","  inflating: Dataset/data/mesh_234.ply  \n","  inflating: Dataset/data/mesh_235.ply  \n","  inflating: Dataset/data/mesh_236.ply  \n","  inflating: Dataset/data/mesh_237.ply  \n","  inflating: Dataset/data/mesh_241.ply  \n","  inflating: Dataset/data/mesh_243.ply  \n","  inflating: Dataset/data/mesh_244.ply  \n","  inflating: Dataset/data/mesh_245.ply  \n","  inflating: Dataset/data/mesh_246.ply  \n","  inflating: Dataset/data/mesh_247.ply  \n","  inflating: Dataset/data/mesh_248.ply  \n","  inflating: Dataset/data/mesh_249.ply  \n","  inflating: Dataset/data/mesh_251.ply  \n","  inflating: Dataset/data/mesh_252.ply  \n","  inflating: Dataset/data/mesh_253.ply  \n","  inflating: Dataset/data/mesh_255.ply  \n","  inflating: Dataset/data/mesh_257.ply  \n","  inflating: Dataset/data/mesh_258.ply  \n","  inflating: Dataset/data/mesh_259.ply  \n","  inflating: Dataset/data/mesh_260.ply  \n","  inflating: Dataset/data/mesh_261.ply  \n","  inflating: Dataset/data/mesh_262.ply  \n","  inflating: Dataset/data/mesh_263.ply  \n","  inflating: Dataset/data/mesh_264.ply  \n","  inflating: Dataset/data/mesh_266.ply  \n","  inflating: Dataset/data/mesh_267.ply  \n","  inflating: Dataset/data/mesh_268.ply  \n","  inflating: Dataset/data/mesh_269.ply  \n","  inflating: Dataset/data/mesh_271.ply  \n","  inflating: Dataset/data/mesh_272.ply  \n","  inflating: Dataset/data/mesh_273.ply  \n","  inflating: Dataset/data/mesh_274.ply  \n","  inflating: Dataset/data/mesh_275.ply  \n","  inflating: Dataset/data/mesh_276.ply  \n","  inflating: Dataset/data/mesh_277.ply  \n","  inflating: Dataset/data/mesh_278.ply  \n","  inflating: Dataset/data/mesh_279.ply  \n","  inflating: Dataset/data/mesh_280.ply  \n","  inflating: Dataset/data/mesh_281.ply  \n","  inflating: Dataset/data/mesh_282.ply  \n","  inflating: Dataset/data/mesh_283.ply  \n","  inflating: Dataset/data/mesh_285.ply  \n","  inflating: Dataset/data/mesh_286.ply  \n","  inflating: Dataset/data/mesh_289.ply  \n","  inflating: Dataset/data/mesh_290.ply  \n","  inflating: Dataset/data/mesh_291.ply  \n","  inflating: Dataset/data/mesh_292.ply  \n","  inflating: Dataset/data/mesh_293.ply  \n","  inflating: Dataset/data/mesh_294.ply  \n","  inflating: Dataset/data/mesh_295.ply  \n","  inflating: Dataset/data/mesh_296.ply  \n","  inflating: Dataset/data/mesh_297.ply  \n","  inflating: Dataset/data/mesh_298.ply  \n","  inflating: Dataset/data/mesh_299.ply  \n","  inflating: Dataset/data/mesh_300.ply  \n","  inflating: Dataset/data/mesh_301.ply  \n","  inflating: Dataset/data/mesh_302.ply  \n","  inflating: Dataset/data/mesh_304.ply  \n","  inflating: Dataset/data/mesh_305.ply  \n","  inflating: Dataset/data/mesh_306.ply  \n","  inflating: Dataset/data/mesh_308.ply  \n","  inflating: Dataset/data/mesh_309.ply  \n","  inflating: Dataset/data/mesh_310.ply  \n","  inflating: Dataset/data/mesh_311.ply  \n","  inflating: Dataset/data/mesh_312.ply  \n","  inflating: Dataset/data/mesh_313.ply  \n","  inflating: Dataset/data/mesh_314.ply  \n","  inflating: Dataset/data/mesh_315.ply  \n","  inflating: Dataset/data/mesh_319.ply  \n","  inflating: Dataset/data/mesh_320.ply  \n","  inflating: Dataset/data/mesh_321.ply  \n","  inflating: Dataset/data/mesh_322.ply  \n","  inflating: Dataset/data/mesh_323.ply  \n","  inflating: Dataset/data/mesh_324.ply  \n","  inflating: Dataset/data/mesh_325.ply  \n","  inflating: Dataset/data/mesh_327.ply  \n","  inflating: Dataset/data/mesh_328.ply  \n","  inflating: Dataset/data/mesh_329.ply  \n","  inflating: Dataset/data/mesh_331.ply  \n","  inflating: Dataset/data/mesh_332.ply  \n","  inflating: Dataset/data/mesh_333.ply  \n","  inflating: Dataset/data/mesh_334.ply  \n","  inflating: Dataset/data/mesh_335.ply  \n","  inflating: Dataset/data/mesh_337.ply  \n","  inflating: Dataset/data/mesh_338.ply  \n","  inflating: Dataset/data/mesh_339.ply  \n","  inflating: Dataset/data/mesh_340.ply  \n","  inflating: Dataset/data/mesh_341.ply  \n","  inflating: Dataset/data/mesh_344.ply  \n","  inflating: Dataset/data/mesh_345.ply  \n","  inflating: Dataset/data/mesh_347.ply  \n","  inflating: Dataset/data/mesh_348.ply  \n","  inflating: Dataset/data/mesh_349.ply  \n","  inflating: Dataset/data/mesh_350.ply  \n","  inflating: Dataset/data/mesh_352.ply  \n","  inflating: Dataset/data/mesh_353.ply  \n","  inflating: Dataset/data/mesh_354.ply  \n","  inflating: Dataset/data/mesh_355.ply  \n","  inflating: Dataset/data/mesh_356.ply  \n","  inflating: Dataset/data/mesh_357.ply  \n","  inflating: Dataset/data/mesh_358.ply  \n","  inflating: Dataset/data/mesh_360.ply  \n","  inflating: Dataset/data/mesh_362.ply  \n","  inflating: Dataset/data/mesh_364.ply  \n","  inflating: Dataset/data/mesh_365.ply  \n","  inflating: Dataset/data/mesh_366.ply  \n","  inflating: Dataset/data/mesh_367.ply  \n","  inflating: Dataset/data/mesh_369.ply  \n","  inflating: Dataset/data/mesh_371.ply  \n","  inflating: Dataset/data/mesh_372.ply  \n","  inflating: Dataset/data/mesh_373.ply  \n","  inflating: Dataset/data/mesh_374.ply  \n","  inflating: Dataset/data/mesh_375.ply  \n","  inflating: Dataset/data/mesh_376.ply  \n","  inflating: Dataset/data/mesh_378.ply  \n","  inflating: Dataset/data/mesh_379.ply  \n","  inflating: Dataset/data/mesh_380.ply  \n","  inflating: Dataset/data/mesh_381.ply  \n","  inflating: Dataset/data/mesh_384.ply  \n","  inflating: Dataset/data/mesh_385.ply  \n","  inflating: Dataset/data/mesh_389.ply  \n","  inflating: Dataset/data/mesh_392.ply  \n","  inflating: Dataset/data/mesh_393.ply  \n","  inflating: Dataset/data/mesh_397.ply  \n","  inflating: Dataset/data/mesh_398.ply  \n","  inflating: Dataset/data/mesh_399.ply  \n","  inflating: Dataset/data/mesh_401.ply  \n","  inflating: Dataset/data/mesh_402.ply  \n","  inflating: Dataset/data/mesh_403.ply  \n","  inflating: Dataset/data/mesh_404.ply  \n","  inflating: Dataset/data/mesh_405.ply  \n","  inflating: Dataset/data/mesh_407.ply  \n","  inflating: Dataset/data/mesh_408.ply  \n","  inflating: Dataset/data/mesh_410.ply  \n","  inflating: Dataset/data/mesh_412.ply  \n","  inflating: Dataset/data/mesh_413.ply  \n","  inflating: Dataset/data/mesh_414.ply  \n","  inflating: Dataset/data/mesh_415.ply  \n","  inflating: Dataset/data/mesh_417.ply  \n","  inflating: Dataset/data/mesh_418.ply  \n","  inflating: Dataset/data/mesh_419.ply  \n","  inflating: Dataset/data/mesh_420.ply  \n","  inflating: Dataset/data/mesh_422.ply  \n","  inflating: Dataset/data/mesh_424.ply  \n","  inflating: Dataset/data/mesh_425.ply  \n","  inflating: Dataset/data/mesh_427.ply  \n","  inflating: Dataset/data/mesh_430.ply  \n","  inflating: Dataset/data/mesh_431.ply  \n","  inflating: Dataset/data/mesh_433.ply  \n","  inflating: Dataset/data/mesh_435.ply  \n","  inflating: Dataset/data/mesh_436.ply  \n","  inflating: Dataset/data/mesh_437.ply  \n","  inflating: Dataset/data/mesh_439.ply  \n","  inflating: Dataset/data/mesh_440.ply  \n","  inflating: Dataset/data/mesh_443.ply  \n","  inflating: Dataset/data/mesh_444.ply  \n","  inflating: Dataset/data/mesh_446.ply  \n","  inflating: Dataset/data/mesh_447.ply  \n","  inflating: Dataset/data/mesh_448.ply  \n","  inflating: Dataset/data/mesh_449.ply  \n","  inflating: Dataset/data/mesh_450.ply  \n","  inflating: Dataset/data/mesh_451.ply  \n","  inflating: Dataset/data/mesh_452.ply  \n","  inflating: Dataset/data/mesh_453.ply  \n","  inflating: Dataset/data/mesh_454.ply  \n","  inflating: Dataset/data/mesh_455.ply  \n","  inflating: Dataset/data/mesh_456.ply  \n","  inflating: Dataset/data/mesh_457.ply  \n","  inflating: Dataset/data/mesh_459.ply  \n","  inflating: Dataset/data/mesh_460.ply  \n","  inflating: Dataset/data/mesh_462.ply  \n","  inflating: Dataset/data/mesh_463.ply  \n","  inflating: Dataset/data/mesh_464.ply  \n","  inflating: Dataset/data/mesh_465.ply  \n","  inflating: Dataset/data/mesh_466.ply  \n","  inflating: Dataset/data/mesh_467.ply  \n","  inflating: Dataset/data/mesh_468.ply  \n","  inflating: Dataset/data/mesh_469.ply  \n","  inflating: Dataset/data/mesh_470.ply  \n","  inflating: Dataset/data/mesh_472.ply  \n","  inflating: Dataset/data/mesh_473.ply  \n","  inflating: Dataset/data/mesh_474.ply  \n","  inflating: Dataset/data/mesh_475.ply  \n","  inflating: Dataset/data/mesh_476.ply  \n","  inflating: Dataset/data/mesh_478.ply  \n","  inflating: Dataset/data/mesh_479.ply  \n","  inflating: Dataset/data/mesh_480.ply  \n","  inflating: Dataset/data/mesh_482.ply  \n","  inflating: Dataset/data/mesh_483.ply  \n","  inflating: Dataset/data/mesh_486.ply  \n","  inflating: Dataset/data/mesh_487.ply  \n","  inflating: Dataset/data/mesh_488.ply  \n","  inflating: Dataset/data/mesh_490.ply  \n","  inflating: Dataset/data/mesh_493.ply  \n","  inflating: Dataset/data/mesh_494.ply  \n","  inflating: Dataset/data/mesh_495.ply  \n","  inflating: Dataset/data/mesh_496.ply  \n","  inflating: Dataset/data/mesh_497.ply  \n","  inflating: Dataset/data/mesh_498.ply  \n","  inflating: Dataset/data/mesh_499.ply  \n","  inflating: Dataset/data/mesh_501.ply  \n","  inflating: Dataset/data/mesh_502.ply  \n","  inflating: Dataset/data/mesh_503.ply  \n","  inflating: Dataset/data/mesh_504.ply  \n","  inflating: Dataset/data/mesh_505.ply  \n","  inflating: Dataset/data/mesh_507.ply  \n","  inflating: Dataset/data/mesh_508.ply  \n","  inflating: Dataset/data/mesh_509.ply  \n","  inflating: Dataset/data/mesh_511.ply  \n","  inflating: Dataset/data/mesh_512.ply  \n","  inflating: Dataset/data/mesh_513.ply  \n","  inflating: Dataset/data/mesh_514.ply  \n","  inflating: Dataset/data/mesh_515.ply  \n","  inflating: Dataset/data/mesh_516.ply  \n","  inflating: Dataset/data/mesh_518.ply  \n","  inflating: Dataset/data/mesh_519.ply  \n","  inflating: Dataset/data/mesh_521.ply  \n","  inflating: Dataset/data/mesh_522.ply  \n","  inflating: Dataset/data/mesh_523.ply  \n","  inflating: Dataset/data/mesh_524.ply  \n","  inflating: Dataset/data/mesh_525.ply  \n","  inflating: Dataset/data/mesh_527.ply  \n","  inflating: Dataset/data/mesh_529.ply  \n","  inflating: Dataset/data/mesh_530.ply  \n","  inflating: Dataset/data/mesh_532.ply  \n","  inflating: Dataset/data/mesh_533.ply  \n","  inflating: Dataset/data/mesh_536.ply  \n","  inflating: Dataset/data/mesh_538.ply  \n","  inflating: Dataset/data/mesh_539.ply  \n","  inflating: Dataset/data/mesh_540.ply  \n","  inflating: Dataset/data/mesh_542.ply  \n","  inflating: Dataset/data/mesh_543.ply  \n","  inflating: Dataset/data/mesh_545.ply  \n","  inflating: Dataset/data/mesh_547.ply  \n","  inflating: Dataset/data/mesh_548.ply  \n","  inflating: Dataset/data/mesh_549.ply  \n","  inflating: Dataset/data/mesh_550.ply  \n","  inflating: Dataset/data/mesh_551.ply  \n","  inflating: Dataset/data/mesh_552.ply  \n","  inflating: Dataset/data/mesh_553.ply  \n","  inflating: Dataset/data/mesh_554.ply  \n","  inflating: Dataset/data/mesh_555.ply  \n","  inflating: Dataset/data/mesh_560.ply  \n","  inflating: Dataset/data/mesh_561.ply  \n","  inflating: Dataset/data/mesh_562.ply  \n","  inflating: Dataset/data/mesh_564.ply  \n","  inflating: Dataset/data/mesh_565.ply  \n","  inflating: Dataset/data/mesh_566.ply  \n","  inflating: Dataset/data/mesh_567.ply  \n","  inflating: Dataset/data/mesh_568.ply  \n","  inflating: Dataset/data/mesh_569.ply  \n","  inflating: Dataset/data/mesh_572.ply  \n","  inflating: Dataset/data/mesh_573.ply  \n","  inflating: Dataset/data/mesh_574.ply  \n","  inflating: Dataset/data/mesh_576.ply  \n","  inflating: Dataset/data/mesh_577.ply  \n","  inflating: Dataset/data/mesh_579.ply  \n","  inflating: Dataset/data/mesh_581.ply  \n","  inflating: Dataset/data/mesh_582.ply  \n","  inflating: Dataset/data/mesh_583.ply  \n","  inflating: Dataset/data/mesh_584.ply  \n","  inflating: Dataset/data/mesh_587.ply  \n","  inflating: Dataset/data/mesh_588.ply  \n","  inflating: Dataset/data/mesh_589.ply  \n","  inflating: Dataset/data/mesh_591.ply  \n","  inflating: Dataset/data/mesh_593.ply  \n","  inflating: Dataset/data/mesh_594.ply  \n","  inflating: Dataset/data/mesh_595.ply  \n","  inflating: Dataset/data/mesh_596.ply  \n","  inflating: Dataset/data/mesh_597.ply  \n","  inflating: Dataset/data/mesh_598.ply  \n","  inflating: Dataset/data/mesh_600.ply  \n","  inflating: Dataset/data/mesh_602.ply  \n","  inflating: Dataset/data/mesh_604.ply  \n","  inflating: Dataset/data/mesh_608.ply  \n","  inflating: Dataset/data/mesh_610.ply  \n","  inflating: Dataset/data/mesh_611.ply  \n","  inflating: Dataset/data/mesh_612.ply  \n","  inflating: Dataset/data/mesh_613.ply  \n","  inflating: Dataset/data/mesh_615.ply  \n","  inflating: Dataset/data/mesh_616.ply  \n","  inflating: Dataset/data/mesh_617.ply  \n","  inflating: Dataset/data/mesh_618.ply  \n","  inflating: Dataset/data/mesh_620.ply  \n","  inflating: Dataset/data/mesh_621.ply  \n","  inflating: Dataset/data/mesh_622.ply  \n","  inflating: Dataset/data/mesh_623.ply  \n","  inflating: Dataset/data/mesh_625.ply  \n","  inflating: Dataset/data/mesh_626.ply  \n","  inflating: Dataset/data/mesh_627.ply  \n","  inflating: Dataset/data/mesh_628.ply  \n","  inflating: Dataset/data/mesh_629.ply  \n","  inflating: Dataset/data/mesh_630.ply  \n","  inflating: Dataset/data/mesh_631.ply  \n","  inflating: Dataset/data/mesh_632.ply  \n","  inflating: Dataset/data/mesh_633.ply  \n","  inflating: Dataset/data/mesh_634.ply  \n","  inflating: Dataset/data/mesh_635.ply  \n","  inflating: Dataset/data/mesh_636.ply  \n","  inflating: Dataset/data/mesh_638.ply  \n","  inflating: Dataset/data/mesh_639.ply  \n","  inflating: Dataset/data/mesh_640.ply  \n","  inflating: Dataset/data/mesh_641.ply  \n","  inflating: Dataset/data/mesh_642.ply  \n","  inflating: Dataset/data/mesh_643.ply  \n","  inflating: Dataset/data/mesh_644.ply  \n","  inflating: Dataset/data/mesh_645.ply  \n","  inflating: Dataset/data/mesh_646.ply  \n","  inflating: Dataset/data/mesh_647.ply  \n","  inflating: Dataset/data/mesh_648.ply  \n","  inflating: Dataset/data/mesh_649.ply  \n","  inflating: Dataset/data/mesh_651.ply  \n","  inflating: Dataset/data/mesh_652.ply  \n","  inflating: Dataset/data/mesh_654.ply  \n","  inflating: Dataset/data/mesh_655.ply  \n","  inflating: Dataset/data/mesh_656.ply  \n","  inflating: Dataset/data/mesh_657.ply  \n","  inflating: Dataset/data/press_001.npy  \n","  inflating: Dataset/data/press_002.npy  \n","  inflating: Dataset/data/press_004.npy  \n","  inflating: Dataset/data/press_005.npy  \n","  inflating: Dataset/data/press_006.npy  \n","  inflating: Dataset/data/press_007.npy  \n","  inflating: Dataset/data/press_008.npy  \n","  inflating: Dataset/data/press_010.npy  \n","  inflating: Dataset/data/press_012.npy  \n","  inflating: Dataset/data/press_013.npy  \n","  inflating: Dataset/data/press_017.npy  \n","  inflating: Dataset/data/press_018.npy  \n","  inflating: Dataset/data/press_021.npy  \n","  inflating: Dataset/data/press_022.npy  \n","  inflating: Dataset/data/press_023.npy  \n","  inflating: Dataset/data/press_025.npy  \n","  inflating: Dataset/data/press_026.npy  \n","  inflating: Dataset/data/press_027.npy  \n","  inflating: Dataset/data/press_028.npy  \n","  inflating: Dataset/data/press_029.npy  \n","  inflating: Dataset/data/press_030.npy  \n","  inflating: Dataset/data/press_031.npy  \n","  inflating: Dataset/data/press_032.npy  \n","  inflating: Dataset/data/press_034.npy  \n","  inflating: Dataset/data/press_035.npy  \n","  inflating: Dataset/data/press_039.npy  \n","  inflating: Dataset/data/press_040.npy  \n","  inflating: Dataset/data/press_043.npy  \n","  inflating: Dataset/data/press_044.npy  \n","  inflating: Dataset/data/press_045.npy  \n","  inflating: Dataset/data/press_046.npy  \n","  inflating: Dataset/data/press_047.npy  \n","  inflating: Dataset/data/press_048.npy  \n","  inflating: Dataset/data/press_049.npy  \n","  inflating: Dataset/data/press_050.npy  \n","  inflating: Dataset/data/press_051.npy  \n","  inflating: Dataset/data/press_052.npy  \n","  inflating: Dataset/data/press_054.npy  \n","  inflating: Dataset/data/press_055.npy  \n","  inflating: Dataset/data/press_056.npy  \n","  inflating: Dataset/data/press_058.npy  \n","  inflating: Dataset/data/press_059.npy  \n","  inflating: Dataset/data/press_060.npy  \n","  inflating: Dataset/data/press_061.npy  \n","  inflating: Dataset/data/press_062.npy  \n","  inflating: Dataset/data/press_063.npy  \n","  inflating: Dataset/data/press_064.npy  \n","  inflating: Dataset/data/press_065.npy  \n","  inflating: Dataset/data/press_067.npy  \n","  inflating: Dataset/data/press_069.npy  \n","  inflating: Dataset/data/press_070.npy  \n","  inflating: Dataset/data/press_071.npy  \n","  inflating: Dataset/data/press_072.npy  \n","  inflating: Dataset/data/press_073.npy  \n","  inflating: Dataset/data/press_074.npy  \n","  inflating: Dataset/data/press_075.npy  \n","  inflating: Dataset/data/press_076.npy  \n","  inflating: Dataset/data/press_077.npy  \n","  inflating: Dataset/data/press_078.npy  \n","  inflating: Dataset/data/press_079.npy  \n","  inflating: Dataset/data/press_080.npy  \n","  inflating: Dataset/data/press_081.npy  \n","  inflating: Dataset/data/press_083.npy  \n","  inflating: Dataset/data/press_084.npy  \n","  inflating: Dataset/data/press_085.npy  \n","  inflating: Dataset/data/press_086.npy  \n","  inflating: Dataset/data/press_087.npy  \n","  inflating: Dataset/data/press_088.npy  \n","  inflating: Dataset/data/press_090.npy  \n","  inflating: Dataset/data/press_091.npy  \n","  inflating: Dataset/data/press_092.npy  \n","  inflating: Dataset/data/press_094.npy  \n","  inflating: Dataset/data/press_095.npy  \n","  inflating: Dataset/data/press_096.npy  \n","  inflating: Dataset/data/press_097.npy  \n","  inflating: Dataset/data/press_100.npy  \n","  inflating: Dataset/data/press_101.npy  \n","  inflating: Dataset/data/press_102.npy  \n","  inflating: Dataset/data/press_105.npy  \n","  inflating: Dataset/data/press_106.npy  \n","  inflating: Dataset/data/press_107.npy  \n","  inflating: Dataset/data/press_109.npy  \n","  inflating: Dataset/data/press_110.npy  \n","  inflating: Dataset/data/press_111.npy  \n","  inflating: Dataset/data/press_112.npy  \n","  inflating: Dataset/data/press_113.npy  \n","  inflating: Dataset/data/press_114.npy  \n","  inflating: Dataset/data/press_115.npy  \n","  inflating: Dataset/data/press_116.npy  \n","  inflating: Dataset/data/press_117.npy  \n","  inflating: Dataset/data/press_118.npy  \n","  inflating: Dataset/data/press_119.npy  \n","  inflating: Dataset/data/press_120.npy  \n","  inflating: Dataset/data/press_121.npy  \n","  inflating: Dataset/data/press_123.npy  \n","  inflating: Dataset/data/press_124.npy  \n","  inflating: Dataset/data/press_125.npy  \n","  inflating: Dataset/data/press_126.npy  \n","  inflating: Dataset/data/press_127.npy  \n","  inflating: Dataset/data/press_128.npy  \n","  inflating: Dataset/data/press_129.npy  \n","  inflating: Dataset/data/press_130.npy  \n","  inflating: Dataset/data/press_131.npy  \n","  inflating: Dataset/data/press_133.npy  \n","  inflating: Dataset/data/press_134.npy  \n","  inflating: Dataset/data/press_136.npy  \n","  inflating: Dataset/data/press_137.npy  \n","  inflating: Dataset/data/press_138.npy  \n","  inflating: Dataset/data/press_139.npy  \n","  inflating: Dataset/data/press_140.npy  \n","  inflating: Dataset/data/press_141.npy  \n","  inflating: Dataset/data/press_142.npy  \n","  inflating: Dataset/data/press_143.npy  \n","  inflating: Dataset/data/press_144.npy  \n","  inflating: Dataset/data/press_145.npy  \n","  inflating: Dataset/data/press_146.npy  \n","  inflating: Dataset/data/press_147.npy  \n","  inflating: Dataset/data/press_148.npy  \n","  inflating: Dataset/data/press_149.npy  \n","  inflating: Dataset/data/press_150.npy  \n","  inflating: Dataset/data/press_151.npy  \n","  inflating: Dataset/data/press_152.npy  \n","  inflating: Dataset/data/press_153.npy  \n","  inflating: Dataset/data/press_155.npy  \n","  inflating: Dataset/data/press_156.npy  \n","  inflating: Dataset/data/press_157.npy  \n","  inflating: Dataset/data/press_158.npy  \n","  inflating: Dataset/data/press_159.npy  \n","  inflating: Dataset/data/press_160.npy  \n","  inflating: Dataset/data/press_161.npy  \n","  inflating: Dataset/data/press_162.npy  \n","  inflating: Dataset/data/press_163.npy  \n","  inflating: Dataset/data/press_165.npy  \n","  inflating: Dataset/data/press_166.npy  \n","  inflating: Dataset/data/press_170.npy  \n","  inflating: Dataset/data/press_172.npy  \n","  inflating: Dataset/data/press_173.npy  \n","  inflating: Dataset/data/press_175.npy  \n","  inflating: Dataset/data/press_176.npy  \n","  inflating: Dataset/data/press_177.npy  \n","  inflating: Dataset/data/press_178.npy  \n","  inflating: Dataset/data/press_179.npy  \n","  inflating: Dataset/data/press_180.npy  \n","  inflating: Dataset/data/press_181.npy  \n","  inflating: Dataset/data/press_182.npy  \n","  inflating: Dataset/data/press_183.npy  \n","  inflating: Dataset/data/press_184.npy  \n","  inflating: Dataset/data/press_186.npy  \n","  inflating: Dataset/data/press_190.npy  \n","  inflating: Dataset/data/press_191.npy  \n","  inflating: Dataset/data/press_192.npy  \n","  inflating: Dataset/data/press_193.npy  \n","  inflating: Dataset/data/press_195.npy  \n","  inflating: Dataset/data/press_196.npy  \n","  inflating: Dataset/data/press_198.npy  \n","  inflating: Dataset/data/press_199.npy  \n","  inflating: Dataset/data/press_200.npy  \n","  inflating: Dataset/data/press_201.npy  \n","  inflating: Dataset/data/press_202.npy  \n","  inflating: Dataset/data/press_203.npy  \n","  inflating: Dataset/data/press_205.npy  \n","  inflating: Dataset/data/press_207.npy  \n","  inflating: Dataset/data/press_210.npy  \n","  inflating: Dataset/data/press_211.npy  \n","  inflating: Dataset/data/press_212.npy  \n","  inflating: Dataset/data/press_213.npy  \n","  inflating: Dataset/data/press_214.npy  \n","  inflating: Dataset/data/press_215.npy  \n","  inflating: Dataset/data/press_217.npy  \n","  inflating: Dataset/data/press_219.npy  \n","  inflating: Dataset/data/press_220.npy  \n","  inflating: Dataset/data/press_221.npy  \n","  inflating: Dataset/data/press_222.npy  \n","  inflating: Dataset/data/press_223.npy  \n","  inflating: Dataset/data/press_224.npy  \n","  inflating: Dataset/data/press_225.npy  \n","  inflating: Dataset/data/press_227.npy  \n","  inflating: Dataset/data/press_228.npy  \n","  inflating: Dataset/data/press_229.npy  \n","  inflating: Dataset/data/press_230.npy  \n","  inflating: Dataset/data/press_231.npy  \n","  inflating: Dataset/data/press_232.npy  \n","  inflating: Dataset/data/press_233.npy  \n","  inflating: Dataset/data/press_234.npy  \n","  inflating: Dataset/data/press_235.npy  \n","  inflating: Dataset/data/press_236.npy  \n","  inflating: Dataset/data/press_237.npy  \n","  inflating: Dataset/data/press_241.npy  \n","  inflating: Dataset/data/press_243.npy  \n","  inflating: Dataset/data/press_244.npy  \n","  inflating: Dataset/data/press_245.npy  \n","  inflating: Dataset/data/press_246.npy  \n","  inflating: Dataset/data/press_247.npy  \n","  inflating: Dataset/data/press_248.npy  \n","  inflating: Dataset/data/press_249.npy  \n","  inflating: Dataset/data/press_251.npy  \n","  inflating: Dataset/data/press_252.npy  \n","  inflating: Dataset/data/press_253.npy  \n","  inflating: Dataset/data/press_255.npy  \n","  inflating: Dataset/data/press_257.npy  \n","  inflating: Dataset/data/press_258.npy  \n","  inflating: Dataset/data/press_259.npy  \n","  inflating: Dataset/data/press_260.npy  \n","  inflating: Dataset/data/press_261.npy  \n","  inflating: Dataset/data/press_262.npy  \n","  inflating: Dataset/data/press_263.npy  \n","  inflating: Dataset/data/press_264.npy  \n","  inflating: Dataset/data/press_266.npy  \n","  inflating: Dataset/data/press_267.npy  \n","  inflating: Dataset/data/press_268.npy  \n","  inflating: Dataset/data/press_269.npy  \n","  inflating: Dataset/data/press_271.npy  \n","  inflating: Dataset/data/press_272.npy  \n","  inflating: Dataset/data/press_273.npy  \n","  inflating: Dataset/data/press_274.npy  \n","  inflating: Dataset/data/press_275.npy  \n","  inflating: Dataset/data/press_276.npy  \n","  inflating: Dataset/data/press_277.npy  \n","  inflating: Dataset/data/press_278.npy  \n","  inflating: Dataset/data/press_279.npy  \n","  inflating: Dataset/data/press_280.npy  \n","  inflating: Dataset/data/press_281.npy  \n","  inflating: Dataset/data/press_282.npy  \n","  inflating: Dataset/data/press_283.npy  \n","  inflating: Dataset/data/press_285.npy  \n","  inflating: Dataset/data/press_286.npy  \n","  inflating: Dataset/data/press_289.npy  \n","  inflating: Dataset/data/press_290.npy  \n","  inflating: Dataset/data/press_291.npy  \n","  inflating: Dataset/data/press_292.npy  \n","  inflating: Dataset/data/press_293.npy  \n","  inflating: Dataset/data/press_294.npy  \n","  inflating: Dataset/data/press_295.npy  \n","  inflating: Dataset/data/press_296.npy  \n","  inflating: Dataset/data/press_297.npy  \n","  inflating: Dataset/data/press_298.npy  \n","  inflating: Dataset/data/press_299.npy  \n","  inflating: Dataset/data/press_300.npy  \n","  inflating: Dataset/data/press_301.npy  \n","  inflating: Dataset/data/press_302.npy  \n","  inflating: Dataset/data/press_304.npy  \n","  inflating: Dataset/data/press_305.npy  \n","  inflating: Dataset/data/press_306.npy  \n","  inflating: Dataset/data/press_308.npy  \n","  inflating: Dataset/data/press_309.npy  \n","  inflating: Dataset/data/press_310.npy  \n","  inflating: Dataset/data/press_311.npy  \n","  inflating: Dataset/data/press_312.npy  \n","  inflating: Dataset/data/press_313.npy  \n","  inflating: Dataset/data/press_314.npy  \n","  inflating: Dataset/data/press_315.npy  \n","  inflating: Dataset/data/press_319.npy  \n","  inflating: Dataset/data/press_320.npy  \n","  inflating: Dataset/data/press_321.npy  \n","  inflating: Dataset/data/press_322.npy  \n","  inflating: Dataset/data/press_323.npy  \n","  inflating: Dataset/data/press_324.npy  \n","  inflating: Dataset/data/press_325.npy  \n","  inflating: Dataset/data/press_327.npy  \n","  inflating: Dataset/data/press_328.npy  \n","  inflating: Dataset/data/press_329.npy  \n","  inflating: Dataset/data/press_331.npy  \n","  inflating: Dataset/data/press_332.npy  \n","  inflating: Dataset/data/press_333.npy  \n","  inflating: Dataset/data/press_334.npy  \n","  inflating: Dataset/data/press_335.npy  \n","  inflating: Dataset/data/press_337.npy  \n","  inflating: Dataset/data/press_338.npy  \n","  inflating: Dataset/data/press_339.npy  \n","  inflating: Dataset/data/press_340.npy  \n","  inflating: Dataset/data/press_341.npy  \n","  inflating: Dataset/data/press_344.npy  \n","  inflating: Dataset/data/press_345.npy  \n","  inflating: Dataset/data/press_347.npy  \n","  inflating: Dataset/data/press_348.npy  \n","  inflating: Dataset/data/press_349.npy  \n","  inflating: Dataset/data/press_350.npy  \n","  inflating: Dataset/data/press_352.npy  \n","  inflating: Dataset/data/press_353.npy  \n","  inflating: Dataset/data/press_354.npy  \n","  inflating: Dataset/data/press_355.npy  \n","  inflating: Dataset/data/press_356.npy  \n","  inflating: Dataset/data/press_357.npy  \n","  inflating: Dataset/data/press_358.npy  \n","  inflating: Dataset/data/press_360.npy  \n","  inflating: Dataset/data/press_362.npy  \n","  inflating: Dataset/data/press_364.npy  \n","  inflating: Dataset/data/press_365.npy  \n","  inflating: Dataset/data/press_366.npy  \n","  inflating: Dataset/data/press_367.npy  \n","  inflating: Dataset/data/press_369.npy  \n","  inflating: Dataset/data/press_371.npy  \n","  inflating: Dataset/data/press_372.npy  \n","  inflating: Dataset/data/press_373.npy  \n","  inflating: Dataset/data/press_374.npy  \n","  inflating: Dataset/data/press_375.npy  \n","  inflating: Dataset/data/press_376.npy  \n","  inflating: Dataset/data/press_378.npy  \n","  inflating: Dataset/data/press_379.npy  \n","  inflating: Dataset/data/press_380.npy  \n","  inflating: Dataset/data/press_381.npy  \n","  inflating: Dataset/data/press_384.npy  \n","  inflating: Dataset/data/press_385.npy  \n","  inflating: Dataset/data/press_389.npy  \n","  inflating: Dataset/data/press_392.npy  \n","  inflating: Dataset/data/press_393.npy  \n","  inflating: Dataset/data/press_397.npy  \n","  inflating: Dataset/data/press_398.npy  \n","  inflating: Dataset/data/press_399.npy  \n","  inflating: Dataset/data/press_401.npy  \n","  inflating: Dataset/data/press_402.npy  \n","  inflating: Dataset/data/press_403.npy  \n","  inflating: Dataset/data/press_404.npy  \n","  inflating: Dataset/data/press_405.npy  \n","  inflating: Dataset/data/press_407.npy  \n","  inflating: Dataset/data/press_408.npy  \n","  inflating: Dataset/data/press_410.npy  \n","  inflating: Dataset/data/press_412.npy  \n","  inflating: Dataset/data/press_413.npy  \n","  inflating: Dataset/data/press_414.npy  \n","  inflating: Dataset/data/press_415.npy  \n","  inflating: Dataset/data/press_417.npy  \n","  inflating: Dataset/data/press_418.npy  \n","  inflating: Dataset/data/press_419.npy  \n","  inflating: Dataset/data/press_420.npy  \n","  inflating: Dataset/data/press_422.npy  \n","  inflating: Dataset/data/press_424.npy  \n","  inflating: Dataset/data/press_425.npy  \n","  inflating: Dataset/data/press_427.npy  \n","  inflating: Dataset/data/press_430.npy  \n","  inflating: Dataset/data/press_431.npy  \n","  inflating: Dataset/data/press_433.npy  \n","  inflating: Dataset/data/press_435.npy  \n","  inflating: Dataset/data/press_436.npy  \n","  inflating: Dataset/data/press_437.npy  \n","  inflating: Dataset/data/press_439.npy  \n","  inflating: Dataset/data/press_440.npy  \n","  inflating: Dataset/data/press_443.npy  \n","  inflating: Dataset/data/press_444.npy  \n","  inflating: Dataset/data/press_446.npy  \n","  inflating: Dataset/data/press_447.npy  \n","  inflating: Dataset/data/press_448.npy  \n","  inflating: Dataset/data/press_449.npy  \n","  inflating: Dataset/data/press_450.npy  \n","  inflating: Dataset/data/press_451.npy  \n","  inflating: Dataset/data/press_452.npy  \n","  inflating: Dataset/data/press_453.npy  \n","  inflating: Dataset/data/press_454.npy  \n","  inflating: Dataset/data/press_455.npy  \n","  inflating: Dataset/data/press_456.npy  \n","  inflating: Dataset/data/press_457.npy  \n","  inflating: Dataset/data/press_459.npy  \n","  inflating: Dataset/data/press_460.npy  \n","  inflating: Dataset/data/press_462.npy  \n","  inflating: Dataset/data/press_463.npy  \n","  inflating: Dataset/data/press_464.npy  \n","  inflating: Dataset/data/press_465.npy  \n","  inflating: Dataset/data/press_466.npy  \n","  inflating: Dataset/data/press_467.npy  \n","  inflating: Dataset/data/press_468.npy  \n","  inflating: Dataset/data/press_469.npy  \n","  inflating: Dataset/data/press_470.npy  \n","  inflating: Dataset/data/press_472.npy  \n","  inflating: Dataset/data/press_473.npy  \n","  inflating: Dataset/data/press_474.npy  \n","  inflating: Dataset/data/press_475.npy  \n","  inflating: Dataset/data/press_476.npy  \n","  inflating: Dataset/data/press_478.npy  \n","  inflating: Dataset/data/press_479.npy  \n","  inflating: Dataset/data/press_480.npy  \n","  inflating: Dataset/data/press_482.npy  \n","  inflating: Dataset/data/press_483.npy  \n","  inflating: Dataset/data/press_486.npy  \n","  inflating: Dataset/data/press_487.npy  \n","  inflating: Dataset/data/press_488.npy  \n","  inflating: Dataset/data/press_490.npy  \n","  inflating: Dataset/data/press_493.npy  \n","  inflating: Dataset/data/press_494.npy  \n","  inflating: Dataset/data/press_495.npy  \n","  inflating: Dataset/data/press_496.npy  \n","  inflating: Dataset/data/press_497.npy  \n","  inflating: Dataset/data/press_498.npy  \n","  inflating: Dataset/data/press_499.npy  \n","  inflating: Dataset/data/press_501.npy  \n","  inflating: Dataset/data/press_502.npy  \n","  inflating: Dataset/data/press_503.npy  \n","  inflating: Dataset/data/press_504.npy  \n","  inflating: Dataset/data/press_505.npy  \n","  inflating: Dataset/data/press_507.npy  \n","  inflating: Dataset/data/press_508.npy  \n","  inflating: Dataset/data/press_509.npy  \n","  inflating: Dataset/data/press_511.npy  \n","  inflating: Dataset/data/press_512.npy  \n","  inflating: Dataset/data/press_513.npy  \n","  inflating: Dataset/data/press_514.npy  \n","  inflating: Dataset/data/press_515.npy  \n","  inflating: Dataset/data/press_516.npy  \n","  inflating: Dataset/data/press_518.npy  \n","  inflating: Dataset/data/press_519.npy  \n","  inflating: Dataset/data/press_521.npy  \n","  inflating: Dataset/data/press_522.npy  \n","  inflating: Dataset/data/press_523.npy  \n","  inflating: Dataset/data/press_524.npy  \n","  inflating: Dataset/data/press_525.npy  \n","  inflating: Dataset/data/press_527.npy  \n","  inflating: Dataset/data/press_529.npy  \n","  inflating: Dataset/data/press_530.npy  \n","  inflating: Dataset/data/press_532.npy  \n","  inflating: Dataset/data/press_533.npy  \n","  inflating: Dataset/data/press_536.npy  \n","  inflating: Dataset/data/press_538.npy  \n","  inflating: Dataset/data/press_539.npy  \n","  inflating: Dataset/data/press_540.npy  \n","  inflating: Dataset/data/press_542.npy  \n","  inflating: Dataset/data/press_543.npy  \n","  inflating: Dataset/data/press_545.npy  \n","  inflating: Dataset/data/press_547.npy  \n","  inflating: Dataset/data/press_548.npy  \n","  inflating: Dataset/data/press_549.npy  \n","  inflating: Dataset/data/press_550.npy  \n","  inflating: Dataset/data/press_551.npy  \n","  inflating: Dataset/data/press_552.npy  \n","  inflating: Dataset/data/press_553.npy  \n","  inflating: Dataset/data/press_554.npy  \n","  inflating: Dataset/data/press_555.npy  \n","  inflating: Dataset/data/press_560.npy  \n","  inflating: Dataset/data/press_561.npy  \n","  inflating: Dataset/data/press_562.npy  \n","  inflating: Dataset/data/press_564.npy  \n","  inflating: Dataset/data/press_565.npy  \n","  inflating: Dataset/data/press_566.npy  \n","  inflating: Dataset/data/press_567.npy  \n","  inflating: Dataset/data/press_568.npy  \n","  inflating: Dataset/data/press_569.npy  \n","  inflating: Dataset/data/press_572.npy  \n","  inflating: Dataset/data/press_573.npy  \n","  inflating: Dataset/data/press_574.npy  \n","  inflating: Dataset/data/press_576.npy  \n","  inflating: Dataset/data/press_577.npy  \n","  inflating: Dataset/data/press_579.npy  \n","  inflating: Dataset/data/press_581.npy  \n","  inflating: Dataset/data/press_582.npy  \n","  inflating: Dataset/data/press_583.npy  \n","  inflating: Dataset/data/press_584.npy  \n","  inflating: Dataset/data/press_587.npy  \n","  inflating: Dataset/data/press_588.npy  \n","  inflating: Dataset/data/press_589.npy  \n","  inflating: Dataset/data/press_591.npy  \n","  inflating: Dataset/data/press_593.npy  \n","  inflating: Dataset/data/press_594.npy  \n","  inflating: Dataset/data/press_595.npy  \n","  inflating: Dataset/data/press_596.npy  \n","  inflating: Dataset/data/press_597.npy  \n","  inflating: Dataset/data/press_598.npy  \n","  inflating: Dataset/data/press_600.npy  \n","  inflating: Dataset/data/press_602.npy  \n","  inflating: Dataset/data/press_604.npy  \n","  inflating: Dataset/data/press_608.npy  \n","  inflating: Dataset/data/press_610.npy  \n","  inflating: Dataset/data/press_611.npy  \n","  inflating: Dataset/data/press_612.npy  \n","  inflating: Dataset/data/press_613.npy  \n","  inflating: Dataset/data/press_615.npy  \n","  inflating: Dataset/data/press_616.npy  \n","  inflating: Dataset/data/press_617.npy  \n","  inflating: Dataset/data/press_618.npy  \n","  inflating: Dataset/data/press_620.npy  \n","  inflating: Dataset/data/press_621.npy  \n","  inflating: Dataset/data/press_622.npy  \n","  inflating: Dataset/data/press_623.npy  \n","  inflating: Dataset/data/press_625.npy  \n","  inflating: Dataset/data/press_626.npy  \n","  inflating: Dataset/data/press_627.npy  \n","  inflating: Dataset/data/press_628.npy  \n","  inflating: Dataset/data/press_629.npy  \n","  inflating: Dataset/data/press_630.npy  \n","  inflating: Dataset/data/press_631.npy  \n","  inflating: Dataset/data/press_632.npy  \n","  inflating: Dataset/data/press_633.npy  \n","  inflating: Dataset/data/press_634.npy  \n","  inflating: Dataset/data/press_635.npy  \n","  inflating: Dataset/data/press_636.npy  \n","  inflating: Dataset/data/press_638.npy  \n","  inflating: Dataset/data/press_639.npy  \n","  inflating: Dataset/data/press_640.npy  \n","  inflating: Dataset/data/press_641.npy  \n","  inflating: Dataset/data/press_642.npy  \n","  inflating: Dataset/data/press_643.npy  \n","  inflating: Dataset/data/press_644.npy  \n","  inflating: Dataset/data/press_645.npy  \n","  inflating: Dataset/data/press_646.npy  \n","  inflating: Dataset/data/press_647.npy  \n","  inflating: Dataset/data/press_648.npy  \n","  inflating: Dataset/data/press_649.npy  \n","  inflating: Dataset/data/press_651.npy  \n","  inflating: Dataset/data/press_652.npy  \n","  inflating: Dataset/data/press_654.npy  \n","  inflating: Dataset/data/press_655.npy  \n","  inflating: Dataset/data/press_656.npy  \n","  inflating: Dataset/data/press_657.npy  \n"," extracting: Dataset/data/train_pressure_min_std.txt  \n","  inflating: Dataset/data/watertight_global_bounds.txt  \n","  inflating: Dataset/data/watertight_meshes.txt  \n","Archive:  track_A.zip\n","  inflating: Dataset/track_A/mesh_658.ply  \n","  inflating: Dataset/track_A/mesh_659.ply  \n","  inflating: Dataset/track_A/mesh_660.ply  \n","  inflating: Dataset/track_A/mesh_661.ply  \n","  inflating: Dataset/track_A/mesh_662.ply  \n","  inflating: Dataset/track_A/mesh_663.ply  \n","  inflating: Dataset/track_A/mesh_664.ply  \n","  inflating: Dataset/track_A/mesh_665.ply  \n","  inflating: Dataset/track_A/mesh_666.ply  \n","  inflating: Dataset/track_A/mesh_667.ply  \n","  inflating: Dataset/track_A/mesh_668.ply  \n","  inflating: Dataset/track_A/mesh_669.ply  \n","  inflating: Dataset/track_A/mesh_670.ply  \n","  inflating: Dataset/track_A/mesh_671.ply  \n","  inflating: Dataset/track_A/mesh_672.ply  \n","  inflating: Dataset/track_A/mesh_673.ply  \n","  inflating: Dataset/track_A/mesh_674.ply  \n","  inflating: Dataset/track_A/mesh_675.ply  \n","  inflating: Dataset/track_A/mesh_676.ply  \n","  inflating: Dataset/track_A/mesh_677.ply  \n","  inflating: Dataset/track_A/mesh_678.ply  \n","  inflating: Dataset/track_A/mesh_679.ply  \n","  inflating: Dataset/track_A/mesh_680.ply  \n","  inflating: Dataset/track_A/mesh_681.ply  \n","  inflating: Dataset/track_A/mesh_682.ply  \n","  inflating: Dataset/track_A/mesh_683.ply  \n","  inflating: Dataset/track_A/mesh_684.ply  \n","  inflating: Dataset/track_A/mesh_685.ply  \n","  inflating: Dataset/track_A/mesh_686.ply  \n","  inflating: Dataset/track_A/mesh_687.ply  \n","  inflating: Dataset/track_A/mesh_688.ply  \n","  inflating: Dataset/track_A/mesh_689.ply  \n","  inflating: Dataset/track_A/mesh_690.ply  \n","  inflating: Dataset/track_A/mesh_691.ply  \n","  inflating: Dataset/track_A/mesh_692.ply  \n","  inflating: Dataset/track_A/mesh_693.ply  \n","  inflating: Dataset/track_A/mesh_694.ply  \n","  inflating: Dataset/track_A/mesh_695.ply  \n","  inflating: Dataset/track_A/mesh_696.ply  \n","  inflating: Dataset/track_A/mesh_697.ply  \n","  inflating: Dataset/track_A/mesh_698.ply  \n","  inflating: Dataset/track_A/mesh_699.ply  \n","  inflating: Dataset/track_A/mesh_700.ply  \n","  inflating: Dataset/track_A/mesh_701.ply  \n","  inflating: Dataset/track_A/mesh_702.ply  \n","  inflating: Dataset/track_A/mesh_703.ply  \n","  inflating: Dataset/track_A/mesh_704.ply  \n","  inflating: Dataset/track_A/mesh_705.ply  \n","  inflating: Dataset/track_A/mesh_706.ply  \n","  inflating: Dataset/track_A/mesh_707.ply  \n","  inflating: Dataset/track_A/mesh_708.ply  \n","  inflating: Dataset/track_A/mesh_709.ply  \n","  inflating: Dataset/track_A/mesh_710.ply  \n","  inflating: Dataset/track_A/mesh_711.ply  \n","  inflating: Dataset/track_A/mesh_712.ply  \n","  inflating: Dataset/track_A/mesh_713.ply  \n","  inflating: Dataset/track_A/mesh_714.ply  \n","  inflating: Dataset/track_A/mesh_715.ply  \n","  inflating: Dataset/track_A/mesh_716.ply  \n","  inflating: Dataset/track_A/mesh_717.ply  \n","  inflating: Dataset/track_A/mesh_718.ply  \n","  inflating: Dataset/track_A/mesh_719.ply  \n","  inflating: Dataset/track_A/mesh_720.ply  \n","  inflating: Dataset/track_A/mesh_721.ply  \n","  inflating: Dataset/track_A/mesh_722.ply  \n","  inflating: Dataset/track_A/watertight_meshes.txt  \n","Archive:  train_track_B.zip\n","  inflating: Dataset/train_track_B/area_0002.npy  \n","  inflating: Dataset/train_track_B/area_0003.npy  \n","  inflating: Dataset/train_track_B/area_0004.npy  \n","  inflating: Dataset/train_track_B/area_0005.npy  \n","  inflating: Dataset/train_track_B/area_0006.npy  \n","  inflating: Dataset/train_track_B/area_0011.npy  \n","  inflating: Dataset/train_track_B/area_0012.npy  \n","  inflating: Dataset/train_track_B/area_0013.npy  \n","  inflating: Dataset/train_track_B/area_0015.npy  \n","  inflating: Dataset/train_track_B/area_0017.npy  \n","  inflating: Dataset/train_track_B/area_0018.npy  \n","  inflating: Dataset/train_track_B/area_0020.npy  \n","  inflating: Dataset/train_track_B/area_0021.npy  \n","  inflating: Dataset/train_track_B/area_0022.npy  \n","  inflating: Dataset/train_track_B/area_0023.npy  \n","  inflating: Dataset/train_track_B/area_0024.npy  \n","  inflating: Dataset/train_track_B/area_0026.npy  \n","  inflating: Dataset/train_track_B/area_0029.npy  \n","  inflating: Dataset/train_track_B/area_0030.npy  \n","  inflating: Dataset/train_track_B/area_0036.npy  \n","  inflating: Dataset/train_track_B/area_0037.npy  \n","  inflating: Dataset/train_track_B/area_0038.npy  \n","  inflating: Dataset/train_track_B/area_0039.npy  \n","  inflating: Dataset/train_track_B/area_0040.npy  \n","  inflating: Dataset/train_track_B/area_0041.npy  \n","  inflating: Dataset/train_track_B/area_0042.npy  \n","  inflating: Dataset/train_track_B/area_0043.npy  \n","  inflating: Dataset/train_track_B/area_0044.npy  \n","  inflating: Dataset/train_track_B/area_0048.npy  \n","  inflating: Dataset/train_track_B/area_0049.npy  \n","  inflating: Dataset/train_track_B/area_0051.npy  \n","  inflating: Dataset/train_track_B/area_0052.npy  \n","  inflating: Dataset/train_track_B/area_0055.npy  \n","  inflating: Dataset/train_track_B/area_0056.npy  \n","  inflating: Dataset/train_track_B/area_0057.npy  \n","  inflating: Dataset/train_track_B/area_0059.npy  \n","  inflating: Dataset/train_track_B/area_0062.npy  \n","  inflating: Dataset/train_track_B/area_0064.npy  \n","  inflating: Dataset/train_track_B/area_0066.npy  \n","  inflating: Dataset/train_track_B/area_0067.npy  \n","  inflating: Dataset/train_track_B/area_0068.npy  \n","  inflating: Dataset/train_track_B/area_0071.npy  \n","  inflating: Dataset/train_track_B/area_0074.npy  \n","  inflating: Dataset/train_track_B/area_0075.npy  \n","  inflating: Dataset/train_track_B/area_0077.npy  \n","  inflating: Dataset/train_track_B/area_0078.npy  \n","  inflating: Dataset/train_track_B/area_0080.npy  \n","  inflating: Dataset/train_track_B/area_0081.npy  \n","  inflating: Dataset/train_track_B/area_0082.npy  \n","  inflating: Dataset/train_track_B/area_0084.npy  \n","  inflating: Dataset/train_track_B/area_0085.npy  \n","  inflating: Dataset/train_track_B/area_0086.npy  \n","  inflating: Dataset/train_track_B/area_0087.npy  \n","  inflating: Dataset/train_track_B/area_0088.npy  \n","  inflating: Dataset/train_track_B/area_0089.npy  \n","  inflating: Dataset/train_track_B/area_0090.npy  \n","  inflating: Dataset/train_track_B/area_0092.npy  \n","  inflating: Dataset/train_track_B/area_0093.npy  \n","  inflating: Dataset/train_track_B/area_0094.npy  \n","  inflating: Dataset/train_track_B/area_0095.npy  \n","  inflating: Dataset/train_track_B/area_0097.npy  \n","  inflating: Dataset/train_track_B/area_0098.npy  \n","  inflating: Dataset/train_track_B/area_0100.npy  \n","  inflating: Dataset/train_track_B/area_0101.npy  \n","  inflating: Dataset/train_track_B/area_0102.npy  \n","  inflating: Dataset/train_track_B/area_0103.npy  \n","  inflating: Dataset/train_track_B/area_0104.npy  \n","  inflating: Dataset/train_track_B/area_0106.npy  \n","  inflating: Dataset/train_track_B/area_0107.npy  \n","  inflating: Dataset/train_track_B/area_0108.npy  \n","  inflating: Dataset/train_track_B/area_0109.npy  \n","  inflating: Dataset/train_track_B/area_0110.npy  \n","  inflating: Dataset/train_track_B/area_0113.npy  \n","  inflating: Dataset/train_track_B/area_0114.npy  \n","  inflating: Dataset/train_track_B/area_0115.npy  \n","  inflating: Dataset/train_track_B/area_0116.npy  \n","  inflating: Dataset/train_track_B/area_0117.npy  \n","  inflating: Dataset/train_track_B/area_0118.npy  \n","  inflating: Dataset/train_track_B/area_0119.npy  \n","  inflating: Dataset/train_track_B/area_0120.npy  \n","  inflating: Dataset/train_track_B/area_0121.npy  \n","  inflating: Dataset/train_track_B/area_0122.npy  \n","  inflating: Dataset/train_track_B/area_0124.npy  \n","  inflating: Dataset/train_track_B/area_0125.npy  \n","  inflating: Dataset/train_track_B/area_0126.npy  \n","  inflating: Dataset/train_track_B/area_0128.npy  \n","  inflating: Dataset/train_track_B/area_0129.npy  \n","  inflating: Dataset/train_track_B/area_0130.npy  \n","  inflating: Dataset/train_track_B/area_0131.npy  \n","  inflating: Dataset/train_track_B/area_0132.npy  \n","  inflating: Dataset/train_track_B/area_0133.npy  \n","  inflating: Dataset/train_track_B/area_0134.npy  \n","  inflating: Dataset/train_track_B/area_0135.npy  \n","  inflating: Dataset/train_track_B/area_0136.npy  \n","  inflating: Dataset/train_track_B/area_0138.npy  \n","  inflating: Dataset/train_track_B/area_0139.npy  \n","  inflating: Dataset/train_track_B/area_0140.npy  \n","  inflating: Dataset/train_track_B/area_0141.npy  \n","  inflating: Dataset/train_track_B/area_0143.npy  \n","  inflating: Dataset/train_track_B/area_0145.npy  \n","  inflating: Dataset/train_track_B/area_0146.npy  \n","  inflating: Dataset/train_track_B/area_0148.npy  \n","  inflating: Dataset/train_track_B/area_0149.npy  \n","  inflating: Dataset/train_track_B/area_0150.npy  \n","  inflating: Dataset/train_track_B/area_0151.npy  \n","  inflating: Dataset/train_track_B/area_0153.npy  \n","  inflating: Dataset/train_track_B/area_0154.npy  \n","  inflating: Dataset/train_track_B/area_0156.npy  \n","  inflating: Dataset/train_track_B/area_0157.npy  \n","  inflating: Dataset/train_track_B/area_0158.npy  \n","  inflating: Dataset/train_track_B/area_0161.npy  \n","  inflating: Dataset/train_track_B/area_0162.npy  \n","  inflating: Dataset/train_track_B/area_0163.npy  \n","  inflating: Dataset/train_track_B/area_0164.npy  \n","  inflating: Dataset/train_track_B/area_0166.npy  \n","  inflating: Dataset/train_track_B/area_0167.npy  \n","  inflating: Dataset/train_track_B/area_0168.npy  \n","  inflating: Dataset/train_track_B/area_0170.npy  \n","  inflating: Dataset/train_track_B/area_0171.npy  \n","  inflating: Dataset/train_track_B/area_0172.npy  \n","  inflating: Dataset/train_track_B/area_0174.npy  \n","  inflating: Dataset/train_track_B/area_0175.npy  \n","  inflating: Dataset/train_track_B/area_0183.npy  \n","  inflating: Dataset/train_track_B/area_0184.npy  \n","  inflating: Dataset/train_track_B/area_0185.npy  \n","  inflating: Dataset/train_track_B/area_0189.npy  \n","  inflating: Dataset/train_track_B/area_0190.npy  \n","  inflating: Dataset/train_track_B/area_0193.npy  \n","  inflating: Dataset/train_track_B/area_0194.npy  \n","  inflating: Dataset/train_track_B/area_0195.npy  \n","  inflating: Dataset/train_track_B/area_0197.npy  \n","  inflating: Dataset/train_track_B/area_0201.npy  \n","  inflating: Dataset/train_track_B/area_0203.npy  \n","  inflating: Dataset/train_track_B/area_0204.npy  \n","  inflating: Dataset/train_track_B/area_0205.npy  \n","  inflating: Dataset/train_track_B/area_0206.npy  \n","  inflating: Dataset/train_track_B/area_0208.npy  \n","  inflating: Dataset/train_track_B/area_0210.npy  \n","  inflating: Dataset/train_track_B/area_0211.npy  \n","  inflating: Dataset/train_track_B/area_0216.npy  \n","  inflating: Dataset/train_track_B/area_0217.npy  \n","  inflating: Dataset/train_track_B/area_0219.npy  \n","  inflating: Dataset/train_track_B/area_0220.npy  \n","  inflating: Dataset/train_track_B/area_0227.npy  \n","  inflating: Dataset/train_track_B/area_0228.npy  \n","  inflating: Dataset/train_track_B/area_0229.npy  \n","  inflating: Dataset/train_track_B/area_0232.npy  \n","  inflating: Dataset/train_track_B/area_0234.npy  \n","  inflating: Dataset/train_track_B/area_0235.npy  \n","  inflating: Dataset/train_track_B/area_0236.npy  \n","  inflating: Dataset/train_track_B/area_0238.npy  \n","  inflating: Dataset/train_track_B/area_0239.npy  \n","  inflating: Dataset/train_track_B/area_0240.npy  \n","  inflating: Dataset/train_track_B/area_0241.npy  \n","  inflating: Dataset/train_track_B/area_0245.npy  \n","  inflating: Dataset/train_track_B/area_0246.npy  \n","  inflating: Dataset/train_track_B/area_0247.npy  \n","  inflating: Dataset/train_track_B/area_0248.npy  \n","  inflating: Dataset/train_track_B/area_0249.npy  \n","  inflating: Dataset/train_track_B/area_0252.npy  \n","  inflating: Dataset/train_track_B/area_0253.npy  \n","  inflating: Dataset/train_track_B/area_0254.npy  \n","  inflating: Dataset/train_track_B/area_0256.npy  \n","  inflating: Dataset/train_track_B/area_0257.npy  \n","  inflating: Dataset/train_track_B/area_0259.npy  \n","  inflating: Dataset/train_track_B/area_0264.npy  \n","  inflating: Dataset/train_track_B/area_0265.npy  \n","  inflating: Dataset/train_track_B/area_0266.npy  \n","  inflating: Dataset/train_track_B/area_0268.npy  \n","  inflating: Dataset/train_track_B/area_0269.npy  \n","  inflating: Dataset/train_track_B/area_0271.npy  \n","  inflating: Dataset/train_track_B/area_0272.npy  \n","  inflating: Dataset/train_track_B/area_0273.npy  \n","  inflating: Dataset/train_track_B/area_0275.npy  \n","  inflating: Dataset/train_track_B/area_0276.npy  \n","  inflating: Dataset/train_track_B/area_0277.npy  \n","  inflating: Dataset/train_track_B/area_0279.npy  \n","  inflating: Dataset/train_track_B/area_0280.npy  \n","  inflating: Dataset/train_track_B/area_0281.npy  \n","  inflating: Dataset/train_track_B/area_0284.npy  \n","  inflating: Dataset/train_track_B/area_0285.npy  \n","  inflating: Dataset/train_track_B/area_0286.npy  \n","  inflating: Dataset/train_track_B/area_0288.npy  \n","  inflating: Dataset/train_track_B/area_0289.npy  \n","  inflating: Dataset/train_track_B/area_0290.npy  \n","  inflating: Dataset/train_track_B/area_0291.npy  \n","  inflating: Dataset/train_track_B/area_0294.npy  \n","  inflating: Dataset/train_track_B/area_0296.npy  \n","  inflating: Dataset/train_track_B/area_0297.npy  \n","  inflating: Dataset/train_track_B/area_0298.npy  \n","  inflating: Dataset/train_track_B/area_0301.npy  \n","  inflating: Dataset/train_track_B/area_0304.npy  \n","  inflating: Dataset/train_track_B/area_0305.npy  \n","  inflating: Dataset/train_track_B/area_0306.npy  \n","  inflating: Dataset/train_track_B/area_0307.npy  \n","  inflating: Dataset/train_track_B/area_0308.npy  \n","  inflating: Dataset/train_track_B/area_0310.npy  \n","  inflating: Dataset/train_track_B/area_0311.npy  \n","  inflating: Dataset/train_track_B/area_0314.npy  \n","  inflating: Dataset/train_track_B/area_0315.npy  \n","  inflating: Dataset/train_track_B/area_0316.npy  \n","  inflating: Dataset/train_track_B/area_0320.npy  \n","  inflating: Dataset/train_track_B/area_0321.npy  \n","  inflating: Dataset/train_track_B/area_0323.npy  \n","  inflating: Dataset/train_track_B/area_0324.npy  \n","  inflating: Dataset/train_track_B/area_0327.npy  \n","  inflating: Dataset/train_track_B/area_0330.npy  \n","  inflating: Dataset/train_track_B/area_0331.npy  \n","  inflating: Dataset/train_track_B/area_0332.npy  \n","  inflating: Dataset/train_track_B/area_0333.npy  \n","  inflating: Dataset/train_track_B/area_0334.npy  \n","  inflating: Dataset/train_track_B/area_0337.npy  \n","  inflating: Dataset/train_track_B/area_0338.npy  \n","  inflating: Dataset/train_track_B/area_0339.npy  \n","  inflating: Dataset/train_track_B/area_0340.npy  \n","  inflating: Dataset/train_track_B/area_0341.npy  \n","  inflating: Dataset/train_track_B/area_0342.npy  \n","  inflating: Dataset/train_track_B/area_0343.npy  \n","  inflating: Dataset/train_track_B/area_0344.npy  \n","  inflating: Dataset/train_track_B/area_0345.npy  \n","  inflating: Dataset/train_track_B/area_0346.npy  \n","  inflating: Dataset/train_track_B/area_0348.npy  \n","  inflating: Dataset/train_track_B/area_0349.npy  \n","  inflating: Dataset/train_track_B/area_0351.npy  \n","  inflating: Dataset/train_track_B/area_0352.npy  \n","  inflating: Dataset/train_track_B/area_0353.npy  \n","  inflating: Dataset/train_track_B/area_0354.npy  \n","  inflating: Dataset/train_track_B/area_0356.npy  \n","  inflating: Dataset/train_track_B/area_0357.npy  \n","  inflating: Dataset/train_track_B/area_0359.npy  \n","  inflating: Dataset/train_track_B/area_0360.npy  \n","  inflating: Dataset/train_track_B/area_0361.npy  \n","  inflating: Dataset/train_track_B/area_0363.npy  \n","  inflating: Dataset/train_track_B/area_0364.npy  \n","  inflating: Dataset/train_track_B/area_0365.npy  \n","  inflating: Dataset/train_track_B/area_0366.npy  \n","  inflating: Dataset/train_track_B/area_0367.npy  \n","  inflating: Dataset/train_track_B/area_0368.npy  \n","  inflating: Dataset/train_track_B/area_0369.npy  \n","  inflating: Dataset/train_track_B/area_0371.npy  \n","  inflating: Dataset/train_track_B/area_0373.npy  \n","  inflating: Dataset/train_track_B/area_0376.npy  \n","  inflating: Dataset/train_track_B/area_0377.npy  \n","  inflating: Dataset/train_track_B/area_0378.npy  \n","  inflating: Dataset/train_track_B/area_0379.npy  \n","  inflating: Dataset/train_track_B/area_0381.npy  \n","  inflating: Dataset/train_track_B/area_0382.npy  \n","  inflating: Dataset/train_track_B/area_0383.npy  \n","  inflating: Dataset/train_track_B/area_0384.npy  \n","  inflating: Dataset/train_track_B/area_0385.npy  \n","  inflating: Dataset/train_track_B/area_0387.npy  \n","  inflating: Dataset/train_track_B/area_0388.npy  \n","  inflating: Dataset/train_track_B/area_0389.npy  \n","  inflating: Dataset/train_track_B/area_0392.npy  \n","  inflating: Dataset/train_track_B/area_0393.npy  \n","  inflating: Dataset/train_track_B/area_0394.npy  \n","  inflating: Dataset/train_track_B/area_0395.npy  \n","  inflating: Dataset/train_track_B/area_0396.npy  \n","  inflating: Dataset/train_track_B/area_0398.npy  \n","  inflating: Dataset/train_track_B/area_0399.npy  \n","  inflating: Dataset/train_track_B/area_0400.npy  \n","  inflating: Dataset/train_track_B/area_0401.npy  \n","  inflating: Dataset/train_track_B/area_0402.npy  \n","  inflating: Dataset/train_track_B/area_0403.npy  \n","  inflating: Dataset/train_track_B/area_0404.npy  \n","  inflating: Dataset/train_track_B/area_0405.npy  \n","  inflating: Dataset/train_track_B/area_0407.npy  \n","  inflating: Dataset/train_track_B/area_0408.npy  \n","  inflating: Dataset/train_track_B/area_0409.npy  \n","  inflating: Dataset/train_track_B/area_0410.npy  \n","  inflating: Dataset/train_track_B/area_0411.npy  \n","  inflating: Dataset/train_track_B/area_0413.npy  \n","  inflating: Dataset/train_track_B/area_0416.npy  \n","  inflating: Dataset/train_track_B/area_0417.npy  \n","  inflating: Dataset/train_track_B/area_0421.npy  \n","  inflating: Dataset/train_track_B/area_0422.npy  \n","  inflating: Dataset/train_track_B/area_0423.npy  \n","  inflating: Dataset/train_track_B/area_0424.npy  \n","  inflating: Dataset/train_track_B/area_0425.npy  \n","  inflating: Dataset/train_track_B/area_0428.npy  \n","  inflating: Dataset/train_track_B/area_0429.npy  \n","  inflating: Dataset/train_track_B/area_0430.npy  \n","  inflating: Dataset/train_track_B/area_0431.npy  \n","  inflating: Dataset/train_track_B/area_0432.npy  \n","  inflating: Dataset/train_track_B/area_0435.npy  \n","  inflating: Dataset/train_track_B/area_0438.npy  \n","  inflating: Dataset/train_track_B/area_0439.npy  \n","  inflating: Dataset/train_track_B/area_0441.npy  \n","  inflating: Dataset/train_track_B/area_0444.npy  \n","  inflating: Dataset/train_track_B/area_0445.npy  \n","  inflating: Dataset/train_track_B/area_0449.npy  \n","  inflating: Dataset/train_track_B/area_0450.npy  \n","  inflating: Dataset/train_track_B/area_0451.npy  \n","  inflating: Dataset/train_track_B/area_0452.npy  \n","  inflating: Dataset/train_track_B/area_0453.npy  \n","  inflating: Dataset/train_track_B/area_0456.npy  \n","  inflating: Dataset/train_track_B/area_0457.npy  \n","  inflating: Dataset/train_track_B/area_0458.npy  \n","  inflating: Dataset/train_track_B/area_0459.npy  \n","  inflating: Dataset/train_track_B/area_0460.npy  \n","  inflating: Dataset/train_track_B/area_0461.npy  \n","  inflating: Dataset/train_track_B/area_0463.npy  \n","  inflating: Dataset/train_track_B/area_0464.npy  \n","  inflating: Dataset/train_track_B/area_0465.npy  \n","  inflating: Dataset/train_track_B/area_0467.npy  \n","  inflating: Dataset/train_track_B/area_0469.npy  \n","  inflating: Dataset/train_track_B/area_0471.npy  \n","  inflating: Dataset/train_track_B/area_0472.npy  \n","  inflating: Dataset/train_track_B/area_0474.npy  \n","  inflating: Dataset/train_track_B/area_0475.npy  \n","  inflating: Dataset/train_track_B/area_0477.npy  \n","  inflating: Dataset/train_track_B/area_0478.npy  \n","  inflating: Dataset/train_track_B/area_0479.npy  \n","  inflating: Dataset/train_track_B/area_0480.npy  \n","  inflating: Dataset/train_track_B/area_0481.npy  \n","  inflating: Dataset/train_track_B/area_0482.npy  \n","  inflating: Dataset/train_track_B/area_0485.npy  \n","  inflating: Dataset/train_track_B/area_0486.npy  \n","  inflating: Dataset/train_track_B/area_0487.npy  \n","  inflating: Dataset/train_track_B/area_0488.npy  \n","  inflating: Dataset/train_track_B/area_0489.npy  \n","  inflating: Dataset/train_track_B/area_0492.npy  \n","  inflating: Dataset/train_track_B/area_0493.npy  \n","  inflating: Dataset/train_track_B/area_0494.npy  \n","  inflating: Dataset/train_track_B/area_0497.npy  \n","  inflating: Dataset/train_track_B/area_0498.npy  \n","  inflating: Dataset/train_track_B/area_0499.npy  \n","  inflating: Dataset/train_track_B/area_0501.npy  \n","  inflating: Dataset/train_track_B/area_0502.npy  \n","  inflating: Dataset/train_track_B/area_0503.npy  \n","  inflating: Dataset/train_track_B/area_0504.npy  \n","  inflating: Dataset/train_track_B/area_0507.npy  \n","  inflating: Dataset/train_track_B/area_0508.npy  \n","  inflating: Dataset/train_track_B/area_0509.npy  \n","  inflating: Dataset/train_track_B/area_0513.npy  \n","  inflating: Dataset/train_track_B/area_0514.npy  \n","  inflating: Dataset/train_track_B/area_0515.npy  \n","  inflating: Dataset/train_track_B/area_0517.npy  \n","  inflating: Dataset/train_track_B/area_0518.npy  \n","  inflating: Dataset/train_track_B/area_0519.npy  \n","  inflating: Dataset/train_track_B/area_0520.npy  \n","  inflating: Dataset/train_track_B/area_0521.npy  \n","  inflating: Dataset/train_track_B/area_0522.npy  \n","  inflating: Dataset/train_track_B/area_0523.npy  \n","  inflating: Dataset/train_track_B/area_0524.npy  \n","  inflating: Dataset/train_track_B/area_0525.npy  \n","  inflating: Dataset/train_track_B/area_0526.npy  \n","  inflating: Dataset/train_track_B/area_0527.npy  \n","  inflating: Dataset/train_track_B/area_0528.npy  \n","  inflating: Dataset/train_track_B/area_0529.npy  \n","  inflating: Dataset/train_track_B/area_0530.npy  \n","  inflating: Dataset/train_track_B/area_0531.npy  \n","  inflating: Dataset/train_track_B/area_0534.npy  \n","  inflating: Dataset/train_track_B/area_0535.npy  \n","  inflating: Dataset/train_track_B/area_0536.npy  \n","  inflating: Dataset/train_track_B/area_0538.npy  \n","  inflating: Dataset/train_track_B/area_0541.npy  \n","  inflating: Dataset/train_track_B/area_0542.npy  \n","  inflating: Dataset/train_track_B/area_0544.npy  \n","  inflating: Dataset/train_track_B/area_0545.npy  \n","  inflating: Dataset/train_track_B/area_0546.npy  \n","  inflating: Dataset/train_track_B/area_0547.npy  \n","  inflating: Dataset/train_track_B/area_0550.npy  \n","  inflating: Dataset/train_track_B/area_0551.npy  \n","  inflating: Dataset/train_track_B/area_0553.npy  \n","  inflating: Dataset/train_track_B/area_0555.npy  \n","  inflating: Dataset/train_track_B/area_0557.npy  \n","  inflating: Dataset/train_track_B/area_0558.npy  \n","  inflating: Dataset/train_track_B/area_0561.npy  \n","  inflating: Dataset/train_track_B/area_0563.npy  \n","  inflating: Dataset/train_track_B/area_0564.npy  \n","  inflating: Dataset/train_track_B/area_0565.npy  \n","  inflating: Dataset/train_track_B/area_0567.npy  \n","  inflating: Dataset/train_track_B/area_0568.npy  \n","  inflating: Dataset/train_track_B/area_0571.npy  \n","  inflating: Dataset/train_track_B/area_0574.npy  \n","  inflating: Dataset/train_track_B/area_0576.npy  \n","  inflating: Dataset/train_track_B/area_0579.npy  \n","  inflating: Dataset/train_track_B/area_0580.npy  \n","  inflating: Dataset/train_track_B/area_0582.npy  \n","  inflating: Dataset/train_track_B/area_0584.npy  \n","  inflating: Dataset/train_track_B/area_0585.npy  \n","  inflating: Dataset/train_track_B/area_0588.npy  \n","  inflating: Dataset/train_track_B/area_0589.npy  \n","  inflating: Dataset/train_track_B/area_0590.npy  \n","  inflating: Dataset/train_track_B/area_0591.npy  \n","  inflating: Dataset/train_track_B/area_0592.npy  \n","  inflating: Dataset/train_track_B/area_0593.npy  \n","  inflating: Dataset/train_track_B/area_0594.npy  \n","  inflating: Dataset/train_track_B/area_0595.npy  \n","  inflating: Dataset/train_track_B/area_0596.npy  \n","  inflating: Dataset/train_track_B/area_0597.npy  \n","  inflating: Dataset/train_track_B/area_0598.npy  \n","  inflating: Dataset/train_track_B/area_0600.npy  \n","  inflating: Dataset/train_track_B/area_0602.npy  \n","  inflating: Dataset/train_track_B/area_0605.npy  \n","  inflating: Dataset/train_track_B/area_0608.npy  \n","  inflating: Dataset/train_track_B/area_0609.npy  \n","  inflating: Dataset/train_track_B/area_0611.npy  \n","  inflating: Dataset/train_track_B/area_0612.npy  \n","  inflating: Dataset/train_track_B/area_0613.npy  \n","  inflating: Dataset/train_track_B/area_0614.npy  \n","  inflating: Dataset/train_track_B/area_0618.npy  \n","  inflating: Dataset/train_track_B/area_0619.npy  \n","  inflating: Dataset/train_track_B/area_0620.npy  \n","  inflating: Dataset/train_track_B/area_0621.npy  \n","  inflating: Dataset/train_track_B/area_0622.npy  \n","  inflating: Dataset/train_track_B/area_0623.npy  \n","  inflating: Dataset/train_track_B/area_0624.npy  \n","  inflating: Dataset/train_track_B/area_0625.npy  \n","  inflating: Dataset/train_track_B/area_0627.npy  \n","  inflating: Dataset/train_track_B/area_0628.npy  \n","  inflating: Dataset/train_track_B/area_0629.npy  \n","  inflating: Dataset/train_track_B/area_0630.npy  \n","  inflating: Dataset/train_track_B/area_0631.npy  \n","  inflating: Dataset/train_track_B/area_0632.npy  \n","  inflating: Dataset/train_track_B/area_0633.npy  \n","  inflating: Dataset/train_track_B/area_0634.npy  \n","  inflating: Dataset/train_track_B/area_0635.npy  \n","  inflating: Dataset/train_track_B/area_0637.npy  \n","  inflating: Dataset/train_track_B/area_0638.npy  \n","  inflating: Dataset/train_track_B/area_0639.npy  \n","  inflating: Dataset/train_track_B/area_0640.npy  \n","  inflating: Dataset/train_track_B/area_0641.npy  \n","  inflating: Dataset/train_track_B/area_0643.npy  \n","  inflating: Dataset/train_track_B/area_0644.npy  \n","  inflating: Dataset/train_track_B/area_0645.npy  \n","  inflating: Dataset/train_track_B/area_0646.npy  \n","  inflating: Dataset/train_track_B/area_0648.npy  \n","  inflating: Dataset/train_track_B/area_0650.npy  \n","  inflating: Dataset/train_track_B/area_0651.npy  \n","  inflating: Dataset/train_track_B/area_0652.npy  \n","  inflating: Dataset/train_track_B/area_0653.npy  \n","  inflating: Dataset/train_track_B/area_0654.npy  \n","  inflating: Dataset/train_track_B/area_0656.npy  \n","  inflating: Dataset/train_track_B/area_0657.npy  \n","  inflating: Dataset/train_track_B/area_0658.npy  \n","  inflating: Dataset/train_track_B/area_0661.npy  \n","  inflating: Dataset/train_track_B/area_0663.npy  \n","  inflating: Dataset/train_track_B/area_0664.npy  \n","  inflating: Dataset/train_track_B/area_0665.npy  \n","  inflating: Dataset/train_track_B/area_0666.npy  \n","  inflating: Dataset/train_track_B/area_0667.npy  \n","  inflating: Dataset/train_track_B/area_0668.npy  \n","  inflating: Dataset/train_track_B/area_0669.npy  \n","  inflating: Dataset/train_track_B/area_0671.npy  \n","  inflating: Dataset/train_track_B/area_0672.npy  \n","  inflating: Dataset/train_track_B/area_0673.npy  \n","  inflating: Dataset/train_track_B/area_0674.npy  \n","  inflating: Dataset/train_track_B/area_0676.npy  \n","  inflating: Dataset/train_track_B/area_0677.npy  \n","  inflating: Dataset/train_track_B/area_0678.npy  \n","  inflating: Dataset/train_track_B/area_0679.npy  \n","  inflating: Dataset/train_track_B/area_0680.npy  \n","  inflating: Dataset/train_track_B/area_0682.npy  \n","  inflating: Dataset/train_track_B/area_0686.npy  \n","  inflating: Dataset/train_track_B/area_0688.npy  \n","  inflating: Dataset/train_track_B/area_0689.npy  \n","  inflating: Dataset/train_track_B/area_0690.npy  \n","  inflating: Dataset/train_track_B/area_0691.npy  \n","  inflating: Dataset/train_track_B/area_0692.npy  \n","  inflating: Dataset/train_track_B/area_0693.npy  \n","  inflating: Dataset/train_track_B/area_0694.npy  \n","  inflating: Dataset/train_track_B/area_0695.npy  \n","  inflating: Dataset/train_track_B/area_0697.npy  \n","  inflating: Dataset/train_track_B/area_0699.npy  \n","  inflating: Dataset/train_track_B/area_0700.npy  \n","  inflating: Dataset/train_track_B/area_0701.npy  \n","  inflating: Dataset/train_track_B/area_0703.npy  \n","  inflating: Dataset/train_track_B/area_0704.npy  \n","  inflating: Dataset/train_track_B/area_0706.npy  \n","  inflating: Dataset/train_track_B/area_0707.npy  \n","  inflating: Dataset/train_track_B/area_0708.npy  \n","  inflating: Dataset/train_track_B/area_0709.npy  \n","  inflating: Dataset/train_track_B/area_0711.npy  \n","  inflating: Dataset/train_track_B/area_0712.npy  \n","  inflating: Dataset/train_track_B/area_0713.npy  \n","  inflating: Dataset/train_track_B/area_0714.npy  \n","  inflating: Dataset/train_track_B/area_0715.npy  \n","  inflating: Dataset/train_track_B/area_0716.npy  \n","  inflating: Dataset/train_track_B/area_0718.npy  \n","  inflating: Dataset/train_track_B/area_0719.npy  \n","  inflating: Dataset/train_track_B/area_0720.npy  \n","  inflating: Dataset/train_track_B/area_0721.npy  \n","  inflating: Dataset/train_track_B/area_0722.npy  \n","  inflating: Dataset/train_track_B/area_0724.npy  \n","  inflating: Dataset/train_track_B/area_0727.npy  \n","  inflating: Dataset/train_track_B/area_0728.npy  \n","  inflating: Dataset/train_track_B/area_0729.npy  \n","  inflating: Dataset/train_track_B/area_0730.npy  \n","  inflating: Dataset/train_track_B/area_0731.npy  \n","  inflating: Dataset/train_track_B/area_0733.npy  \n","  inflating: Dataset/train_track_B/area_0735.npy  \n","  inflating: Dataset/train_track_B/area_0736.npy  \n","  inflating: Dataset/train_track_B/area_0737.npy  \n","  inflating: Dataset/train_track_B/area_0740.npy  \n","  inflating: Dataset/train_track_B/area_0742.npy  \n","  inflating: Dataset/train_track_B/area_0743.npy  \n","  inflating: Dataset/train_track_B/area_0744.npy  \n","  inflating: Dataset/train_track_B/area_0745.npy  \n","  inflating: Dataset/train_track_B/centroid_0002.npy  \n","  inflating: Dataset/train_track_B/centroid_0003.npy  \n","  inflating: Dataset/train_track_B/centroid_0004.npy  \n","  inflating: Dataset/train_track_B/centroid_0005.npy  \n","  inflating: Dataset/train_track_B/centroid_0006.npy  \n","  inflating: Dataset/train_track_B/centroid_0011.npy  \n","  inflating: Dataset/train_track_B/centroid_0012.npy  \n","  inflating: Dataset/train_track_B/centroid_0013.npy  \n","  inflating: Dataset/train_track_B/centroid_0015.npy  \n","  inflating: Dataset/train_track_B/centroid_0017.npy  \n","  inflating: Dataset/train_track_B/centroid_0018.npy  \n","  inflating: Dataset/train_track_B/centroid_0020.npy  \n","  inflating: Dataset/train_track_B/centroid_0021.npy  \n","  inflating: Dataset/train_track_B/centroid_0022.npy  \n","  inflating: Dataset/train_track_B/centroid_0023.npy  \n","  inflating: Dataset/train_track_B/centroid_0024.npy  \n","  inflating: Dataset/train_track_B/centroid_0026.npy  \n","  inflating: Dataset/train_track_B/centroid_0029.npy  \n","  inflating: Dataset/train_track_B/centroid_0030.npy  \n","  inflating: Dataset/train_track_B/centroid_0036.npy  \n","  inflating: Dataset/train_track_B/centroid_0037.npy  \n","  inflating: Dataset/train_track_B/centroid_0038.npy  \n","  inflating: Dataset/train_track_B/centroid_0039.npy  \n","  inflating: Dataset/train_track_B/centroid_0040.npy  \n","  inflating: Dataset/train_track_B/centroid_0041.npy  \n","  inflating: Dataset/train_track_B/centroid_0042.npy  \n","  inflating: Dataset/train_track_B/centroid_0043.npy  \n","  inflating: Dataset/train_track_B/centroid_0044.npy  \n","  inflating: Dataset/train_track_B/centroid_0048.npy  \n","  inflating: Dataset/train_track_B/centroid_0049.npy  \n","  inflating: Dataset/train_track_B/centroid_0051.npy  \n","  inflating: Dataset/train_track_B/centroid_0052.npy  \n","  inflating: Dataset/train_track_B/centroid_0055.npy  \n","  inflating: Dataset/train_track_B/centroid_0056.npy  \n","  inflating: Dataset/train_track_B/centroid_0057.npy  \n","  inflating: Dataset/train_track_B/centroid_0059.npy  \n","  inflating: Dataset/train_track_B/centroid_0062.npy  \n","  inflating: Dataset/train_track_B/centroid_0064.npy  \n","  inflating: Dataset/train_track_B/centroid_0066.npy  \n","  inflating: Dataset/train_track_B/centroid_0067.npy  \n","  inflating: Dataset/train_track_B/centroid_0068.npy  \n","  inflating: Dataset/train_track_B/centroid_0071.npy  \n","  inflating: Dataset/train_track_B/centroid_0074.npy  \n","  inflating: Dataset/train_track_B/centroid_0075.npy  \n","  inflating: Dataset/train_track_B/centroid_0077.npy  \n","  inflating: Dataset/train_track_B/centroid_0078.npy  \n","  inflating: Dataset/train_track_B/centroid_0080.npy  \n","  inflating: Dataset/train_track_B/centroid_0081.npy  \n","  inflating: Dataset/train_track_B/centroid_0082.npy  \n","  inflating: Dataset/train_track_B/centroid_0084.npy  \n","  inflating: Dataset/train_track_B/centroid_0085.npy  \n","  inflating: Dataset/train_track_B/centroid_0086.npy  \n","  inflating: Dataset/train_track_B/centroid_0087.npy  \n","  inflating: Dataset/train_track_B/centroid_0088.npy  \n","  inflating: Dataset/train_track_B/centroid_0089.npy  \n","  inflating: Dataset/train_track_B/centroid_0090.npy  \n","  inflating: Dataset/train_track_B/centroid_0092.npy  \n","  inflating: Dataset/train_track_B/centroid_0093.npy  \n","  inflating: Dataset/train_track_B/centroid_0094.npy  \n","  inflating: Dataset/train_track_B/centroid_0095.npy  \n","  inflating: Dataset/train_track_B/centroid_0097.npy  \n","  inflating: Dataset/train_track_B/centroid_0098.npy  \n","  inflating: Dataset/train_track_B/centroid_0100.npy  \n","  inflating: Dataset/train_track_B/centroid_0101.npy  \n","  inflating: Dataset/train_track_B/centroid_0102.npy  \n","  inflating: Dataset/train_track_B/centroid_0103.npy  \n","  inflating: Dataset/train_track_B/centroid_0104.npy  \n","  inflating: Dataset/train_track_B/centroid_0106.npy  \n","  inflating: Dataset/train_track_B/centroid_0107.npy  \n","  inflating: Dataset/train_track_B/centroid_0108.npy  \n","  inflating: Dataset/train_track_B/centroid_0109.npy  \n","  inflating: Dataset/train_track_B/centroid_0110.npy  \n","  inflating: Dataset/train_track_B/centroid_0113.npy  \n","  inflating: Dataset/train_track_B/centroid_0114.npy  \n","  inflating: Dataset/train_track_B/centroid_0115.npy  \n","  inflating: Dataset/train_track_B/centroid_0116.npy  \n","  inflating: Dataset/train_track_B/centroid_0117.npy  \n","  inflating: Dataset/train_track_B/centroid_0118.npy  \n","  inflating: Dataset/train_track_B/centroid_0119.npy  \n","  inflating: Dataset/train_track_B/centroid_0120.npy  \n","  inflating: Dataset/train_track_B/centroid_0121.npy  \n","  inflating: Dataset/train_track_B/centroid_0122.npy  \n","  inflating: Dataset/train_track_B/centroid_0124.npy  \n","  inflating: Dataset/train_track_B/centroid_0125.npy  \n","  inflating: Dataset/train_track_B/centroid_0126.npy  \n","  inflating: Dataset/train_track_B/centroid_0128.npy  \n","  inflating: Dataset/train_track_B/centroid_0129.npy  \n","  inflating: Dataset/train_track_B/centroid_0130.npy  \n","  inflating: Dataset/train_track_B/centroid_0131.npy  \n","  inflating: Dataset/train_track_B/centroid_0132.npy  \n","  inflating: Dataset/train_track_B/centroid_0133.npy  \n","  inflating: Dataset/train_track_B/centroid_0134.npy  \n","  inflating: Dataset/train_track_B/centroid_0135.npy  \n","  inflating: Dataset/train_track_B/centroid_0136.npy  \n","  inflating: Dataset/train_track_B/centroid_0138.npy  \n","  inflating: Dataset/train_track_B/centroid_0139.npy  \n","  inflating: Dataset/train_track_B/centroid_0140.npy  \n","  inflating: Dataset/train_track_B/centroid_0141.npy  \n","  inflating: Dataset/train_track_B/centroid_0143.npy  \n","  inflating: Dataset/train_track_B/centroid_0145.npy  \n","  inflating: Dataset/train_track_B/centroid_0146.npy  \n","  inflating: Dataset/train_track_B/centroid_0148.npy  \n","  inflating: Dataset/train_track_B/centroid_0149.npy  \n","  inflating: Dataset/train_track_B/centroid_0150.npy  \n","  inflating: Dataset/train_track_B/centroid_0151.npy  \n","  inflating: Dataset/train_track_B/centroid_0153.npy  \n","  inflating: Dataset/train_track_B/centroid_0154.npy  \n","  inflating: Dataset/train_track_B/centroid_0156.npy  \n","  inflating: Dataset/train_track_B/centroid_0157.npy  \n","  inflating: Dataset/train_track_B/centroid_0158.npy  \n","  inflating: Dataset/train_track_B/centroid_0161.npy  \n","  inflating: Dataset/train_track_B/centroid_0162.npy  \n","  inflating: Dataset/train_track_B/centroid_0163.npy  \n","  inflating: Dataset/train_track_B/centroid_0164.npy  \n","  inflating: Dataset/train_track_B/centroid_0166.npy  \n","  inflating: Dataset/train_track_B/centroid_0167.npy  \n","  inflating: Dataset/train_track_B/centroid_0168.npy  \n","  inflating: Dataset/train_track_B/centroid_0170.npy  \n","  inflating: Dataset/train_track_B/centroid_0171.npy  \n","  inflating: Dataset/train_track_B/centroid_0172.npy  \n","  inflating: Dataset/train_track_B/centroid_0174.npy  \n","  inflating: Dataset/train_track_B/centroid_0175.npy  \n","  inflating: Dataset/train_track_B/centroid_0183.npy  \n","  inflating: Dataset/train_track_B/centroid_0184.npy  \n","  inflating: Dataset/train_track_B/centroid_0185.npy  \n","  inflating: Dataset/train_track_B/centroid_0189.npy  \n","  inflating: Dataset/train_track_B/centroid_0190.npy  \n","  inflating: Dataset/train_track_B/centroid_0193.npy  \n","  inflating: Dataset/train_track_B/centroid_0194.npy  \n","  inflating: Dataset/train_track_B/centroid_0195.npy  \n","  inflating: Dataset/train_track_B/centroid_0197.npy  \n","  inflating: Dataset/train_track_B/centroid_0201.npy  \n","  inflating: Dataset/train_track_B/centroid_0203.npy  \n","  inflating: Dataset/train_track_B/centroid_0204.npy  \n","  inflating: Dataset/train_track_B/centroid_0205.npy  \n","  inflating: Dataset/train_track_B/centroid_0206.npy  \n","  inflating: Dataset/train_track_B/centroid_0208.npy  \n","  inflating: Dataset/train_track_B/centroid_0210.npy  \n","  inflating: Dataset/train_track_B/centroid_0211.npy  \n","  inflating: Dataset/train_track_B/centroid_0216.npy  \n","  inflating: Dataset/train_track_B/centroid_0217.npy  \n","  inflating: Dataset/train_track_B/centroid_0219.npy  \n","  inflating: Dataset/train_track_B/centroid_0220.npy  \n","  inflating: Dataset/train_track_B/centroid_0227.npy  \n","  inflating: Dataset/train_track_B/centroid_0228.npy  \n","  inflating: Dataset/train_track_B/centroid_0229.npy  \n","  inflating: Dataset/train_track_B/centroid_0232.npy  \n","  inflating: Dataset/train_track_B/centroid_0234.npy  \n","  inflating: Dataset/train_track_B/centroid_0235.npy  \n","  inflating: Dataset/train_track_B/centroid_0236.npy  \n","  inflating: Dataset/train_track_B/centroid_0238.npy  \n","  inflating: Dataset/train_track_B/centroid_0239.npy  \n","  inflating: Dataset/train_track_B/centroid_0240.npy  \n","  inflating: Dataset/train_track_B/centroid_0241.npy  \n","  inflating: Dataset/train_track_B/centroid_0245.npy  \n","  inflating: Dataset/train_track_B/centroid_0246.npy  \n","  inflating: Dataset/train_track_B/centroid_0247.npy  \n","  inflating: Dataset/train_track_B/centroid_0248.npy  \n","  inflating: Dataset/train_track_B/centroid_0249.npy  \n","  inflating: Dataset/train_track_B/centroid_0252.npy  \n","  inflating: Dataset/train_track_B/centroid_0253.npy  \n","  inflating: Dataset/train_track_B/centroid_0254.npy  \n","  inflating: Dataset/train_track_B/centroid_0256.npy  \n","  inflating: Dataset/train_track_B/centroid_0257.npy  \n","  inflating: Dataset/train_track_B/centroid_0259.npy  \n","  inflating: Dataset/train_track_B/centroid_0264.npy  \n","  inflating: Dataset/train_track_B/centroid_0265.npy  \n","  inflating: Dataset/train_track_B/centroid_0266.npy  \n","  inflating: Dataset/train_track_B/centroid_0268.npy  \n","  inflating: Dataset/train_track_B/centroid_0269.npy  \n","  inflating: Dataset/train_track_B/centroid_0271.npy  \n","  inflating: Dataset/train_track_B/centroid_0272.npy  \n","  inflating: Dataset/train_track_B/centroid_0273.npy  \n","  inflating: Dataset/train_track_B/centroid_0275.npy  \n","  inflating: Dataset/train_track_B/centroid_0276.npy  \n","  inflating: Dataset/train_track_B/centroid_0277.npy  \n","  inflating: Dataset/train_track_B/centroid_0279.npy  \n","  inflating: Dataset/train_track_B/centroid_0280.npy  \n","  inflating: Dataset/train_track_B/centroid_0281.npy  \n","  inflating: Dataset/train_track_B/centroid_0284.npy  \n","  inflating: Dataset/train_track_B/centroid_0285.npy  \n","  inflating: Dataset/train_track_B/centroid_0286.npy  \n","  inflating: Dataset/train_track_B/centroid_0288.npy  \n","  inflating: Dataset/train_track_B/centroid_0289.npy  \n","  inflating: Dataset/train_track_B/centroid_0290.npy  \n","  inflating: Dataset/train_track_B/centroid_0291.npy  \n","  inflating: Dataset/train_track_B/centroid_0294.npy  \n","  inflating: Dataset/train_track_B/centroid_0296.npy  \n","  inflating: Dataset/train_track_B/centroid_0297.npy  \n","  inflating: Dataset/train_track_B/centroid_0298.npy  \n","  inflating: Dataset/train_track_B/centroid_0301.npy  \n","  inflating: Dataset/train_track_B/centroid_0304.npy  \n","  inflating: Dataset/train_track_B/centroid_0305.npy  \n","  inflating: Dataset/train_track_B/centroid_0306.npy  \n","  inflating: Dataset/train_track_B/centroid_0307.npy  \n","  inflating: Dataset/train_track_B/centroid_0308.npy  \n","  inflating: Dataset/train_track_B/centroid_0310.npy  \n","  inflating: Dataset/train_track_B/centroid_0311.npy  \n","  inflating: Dataset/train_track_B/centroid_0314.npy  \n","  inflating: Dataset/train_track_B/centroid_0315.npy  \n","  inflating: Dataset/train_track_B/centroid_0316.npy  \n","  inflating: Dataset/train_track_B/centroid_0320.npy  \n","  inflating: Dataset/train_track_B/centroid_0321.npy  \n","  inflating: Dataset/train_track_B/centroid_0323.npy  \n","  inflating: Dataset/train_track_B/centroid_0324.npy  \n","  inflating: Dataset/train_track_B/centroid_0327.npy  \n","  inflating: Dataset/train_track_B/centroid_0330.npy  \n","  inflating: Dataset/train_track_B/centroid_0331.npy  \n","  inflating: Dataset/train_track_B/centroid_0332.npy  \n","  inflating: Dataset/train_track_B/centroid_0333.npy  \n","  inflating: Dataset/train_track_B/centroid_0334.npy  \n","  inflating: Dataset/train_track_B/centroid_0337.npy  \n","  inflating: Dataset/train_track_B/centroid_0338.npy  \n","  inflating: Dataset/train_track_B/centroid_0339.npy  \n","  inflating: Dataset/train_track_B/centroid_0340.npy  \n","  inflating: Dataset/train_track_B/centroid_0341.npy  \n","  inflating: Dataset/train_track_B/centroid_0342.npy  \n","  inflating: Dataset/train_track_B/centroid_0343.npy  \n","  inflating: Dataset/train_track_B/centroid_0344.npy  \n","  inflating: Dataset/train_track_B/centroid_0345.npy  \n","  inflating: Dataset/train_track_B/centroid_0346.npy  \n","  inflating: Dataset/train_track_B/centroid_0348.npy  \n","  inflating: Dataset/train_track_B/centroid_0349.npy  \n","  inflating: Dataset/train_track_B/centroid_0351.npy  \n","  inflating: Dataset/train_track_B/centroid_0352.npy  \n","  inflating: Dataset/train_track_B/centroid_0353.npy  \n","  inflating: Dataset/train_track_B/centroid_0354.npy  \n","  inflating: Dataset/train_track_B/centroid_0356.npy  \n","  inflating: Dataset/train_track_B/centroid_0357.npy  \n","  inflating: Dataset/train_track_B/centroid_0359.npy  \n","  inflating: Dataset/train_track_B/centroid_0360.npy  \n","  inflating: Dataset/train_track_B/centroid_0361.npy  \n","  inflating: Dataset/train_track_B/centroid_0363.npy  \n","  inflating: Dataset/train_track_B/centroid_0364.npy  \n","  inflating: Dataset/train_track_B/centroid_0365.npy  \n","  inflating: Dataset/train_track_B/centroid_0366.npy  \n","  inflating: Dataset/train_track_B/centroid_0367.npy  \n","  inflating: Dataset/train_track_B/centroid_0368.npy  \n","  inflating: Dataset/train_track_B/centroid_0369.npy  \n","  inflating: Dataset/train_track_B/centroid_0371.npy  \n","  inflating: Dataset/train_track_B/centroid_0373.npy  \n","  inflating: Dataset/train_track_B/centroid_0376.npy  \n","  inflating: Dataset/train_track_B/centroid_0377.npy  \n","  inflating: Dataset/train_track_B/centroid_0378.npy  \n","  inflating: Dataset/train_track_B/centroid_0379.npy  \n","  inflating: Dataset/train_track_B/centroid_0381.npy  \n","  inflating: Dataset/train_track_B/centroid_0382.npy  \n","  inflating: Dataset/train_track_B/centroid_0383.npy  \n","  inflating: Dataset/train_track_B/centroid_0384.npy  \n","  inflating: Dataset/train_track_B/centroid_0385.npy  \n","  inflating: Dataset/train_track_B/centroid_0387.npy  \n","  inflating: Dataset/train_track_B/centroid_0388.npy  \n","  inflating: Dataset/train_track_B/centroid_0389.npy  \n","  inflating: Dataset/train_track_B/centroid_0392.npy  \n","  inflating: Dataset/train_track_B/centroid_0393.npy  \n","  inflating: Dataset/train_track_B/centroid_0394.npy  \n","  inflating: Dataset/train_track_B/centroid_0395.npy  \n","  inflating: Dataset/train_track_B/centroid_0396.npy  \n","  inflating: Dataset/train_track_B/centroid_0398.npy  \n","  inflating: Dataset/train_track_B/centroid_0399.npy  \n","  inflating: Dataset/train_track_B/centroid_0400.npy  \n","  inflating: Dataset/train_track_B/centroid_0401.npy  \n","  inflating: Dataset/train_track_B/centroid_0402.npy  \n","  inflating: Dataset/train_track_B/centroid_0403.npy  \n","  inflating: Dataset/train_track_B/centroid_0404.npy  \n","  inflating: Dataset/train_track_B/centroid_0405.npy  \n","  inflating: Dataset/train_track_B/centroid_0407.npy  \n","  inflating: Dataset/train_track_B/centroid_0408.npy  \n","  inflating: Dataset/train_track_B/centroid_0409.npy  \n","  inflating: Dataset/train_track_B/centroid_0410.npy  \n","  inflating: Dataset/train_track_B/centroid_0411.npy  \n","  inflating: Dataset/train_track_B/centroid_0413.npy  \n","  inflating: Dataset/train_track_B/centroid_0416.npy  \n","  inflating: Dataset/train_track_B/centroid_0417.npy  \n","  inflating: Dataset/train_track_B/centroid_0421.npy  \n","  inflating: Dataset/train_track_B/centroid_0422.npy  \n","  inflating: Dataset/train_track_B/centroid_0423.npy  \n","  inflating: Dataset/train_track_B/centroid_0424.npy  \n","  inflating: Dataset/train_track_B/centroid_0425.npy  \n","  inflating: Dataset/train_track_B/centroid_0428.npy  \n","  inflating: Dataset/train_track_B/centroid_0429.npy  \n","  inflating: Dataset/train_track_B/centroid_0430.npy  \n","  inflating: Dataset/train_track_B/centroid_0431.npy  \n","  inflating: Dataset/train_track_B/centroid_0432.npy  \n","  inflating: Dataset/train_track_B/centroid_0435.npy  \n","  inflating: Dataset/train_track_B/centroid_0438.npy  \n","  inflating: Dataset/train_track_B/centroid_0439.npy  \n","  inflating: Dataset/train_track_B/centroid_0441.npy  \n","  inflating: Dataset/train_track_B/centroid_0444.npy  \n","  inflating: Dataset/train_track_B/centroid_0445.npy  \n","  inflating: Dataset/train_track_B/centroid_0449.npy  \n","  inflating: Dataset/train_track_B/centroid_0450.npy  \n","  inflating: Dataset/train_track_B/centroid_0451.npy  \n","  inflating: Dataset/train_track_B/centroid_0452.npy  \n","  inflating: Dataset/train_track_B/centroid_0453.npy  \n","  inflating: Dataset/train_track_B/centroid_0456.npy  \n","  inflating: Dataset/train_track_B/centroid_0457.npy  \n","  inflating: Dataset/train_track_B/centroid_0458.npy  \n","  inflating: Dataset/train_track_B/centroid_0459.npy  \n","  inflating: Dataset/train_track_B/centroid_0460.npy  \n","  inflating: Dataset/train_track_B/centroid_0461.npy  \n","  inflating: Dataset/train_track_B/centroid_0463.npy  \n","  inflating: Dataset/train_track_B/centroid_0464.npy  \n","  inflating: Dataset/train_track_B/centroid_0465.npy  \n","  inflating: Dataset/train_track_B/centroid_0467.npy  \n","  inflating: Dataset/train_track_B/centroid_0469.npy  \n","  inflating: Dataset/train_track_B/centroid_0471.npy  \n","  inflating: Dataset/train_track_B/centroid_0472.npy  \n","  inflating: Dataset/train_track_B/centroid_0474.npy  \n","  inflating: Dataset/train_track_B/centroid_0475.npy  \n","  inflating: Dataset/train_track_B/centroid_0477.npy  \n","  inflating: Dataset/train_track_B/centroid_0478.npy  \n","  inflating: Dataset/train_track_B/centroid_0479.npy  \n","  inflating: Dataset/train_track_B/centroid_0480.npy  \n","  inflating: Dataset/train_track_B/centroid_0481.npy  \n","  inflating: Dataset/train_track_B/centroid_0482.npy  \n","  inflating: Dataset/train_track_B/centroid_0485.npy  \n","  inflating: Dataset/train_track_B/centroid_0486.npy  \n","  inflating: Dataset/train_track_B/centroid_0487.npy  \n","  inflating: Dataset/train_track_B/centroid_0488.npy  \n","  inflating: Dataset/train_track_B/centroid_0489.npy  \n","  inflating: Dataset/train_track_B/centroid_0492.npy  \n","  inflating: Dataset/train_track_B/centroid_0493.npy  \n","  inflating: Dataset/train_track_B/centroid_0494.npy  \n","  inflating: Dataset/train_track_B/centroid_0497.npy  \n","  inflating: Dataset/train_track_B/centroid_0498.npy  \n","  inflating: Dataset/train_track_B/centroid_0499.npy  \n","  inflating: Dataset/train_track_B/centroid_0501.npy  \n","  inflating: Dataset/train_track_B/centroid_0502.npy  \n","  inflating: Dataset/train_track_B/centroid_0503.npy  \n","  inflating: Dataset/train_track_B/centroid_0504.npy  \n","  inflating: Dataset/train_track_B/centroid_0507.npy  \n","  inflating: Dataset/train_track_B/centroid_0508.npy  \n","  inflating: Dataset/train_track_B/centroid_0509.npy  \n","  inflating: Dataset/train_track_B/centroid_0513.npy  \n","  inflating: Dataset/train_track_B/centroid_0514.npy  \n","  inflating: Dataset/train_track_B/centroid_0515.npy  \n","  inflating: Dataset/train_track_B/centroid_0517.npy  \n","  inflating: Dataset/train_track_B/centroid_0518.npy  \n","  inflating: Dataset/train_track_B/centroid_0519.npy  \n","  inflating: Dataset/train_track_B/centroid_0520.npy  \n","  inflating: Dataset/train_track_B/centroid_0521.npy  \n","  inflating: Dataset/train_track_B/centroid_0522.npy  \n","  inflating: Dataset/train_track_B/centroid_0523.npy  \n","  inflating: Dataset/train_track_B/centroid_0524.npy  \n","  inflating: Dataset/train_track_B/centroid_0525.npy  \n","  inflating: Dataset/train_track_B/centroid_0526.npy  \n","  inflating: Dataset/train_track_B/centroid_0527.npy  \n","  inflating: Dataset/train_track_B/centroid_0528.npy  \n","  inflating: Dataset/train_track_B/centroid_0529.npy  \n","  inflating: Dataset/train_track_B/centroid_0530.npy  \n","  inflating: Dataset/train_track_B/centroid_0531.npy  \n","  inflating: Dataset/train_track_B/centroid_0534.npy  \n","  inflating: Dataset/train_track_B/centroid_0535.npy  \n","  inflating: Dataset/train_track_B/centroid_0536.npy  \n","  inflating: Dataset/train_track_B/centroid_0538.npy  \n","  inflating: Dataset/train_track_B/centroid_0541.npy  \n","  inflating: Dataset/train_track_B/centroid_0542.npy  \n","  inflating: Dataset/train_track_B/centroid_0544.npy  \n","  inflating: Dataset/train_track_B/centroid_0545.npy  \n","  inflating: Dataset/train_track_B/centroid_0546.npy  \n","  inflating: Dataset/train_track_B/centroid_0547.npy  \n","  inflating: Dataset/train_track_B/centroid_0550.npy  \n","  inflating: Dataset/train_track_B/centroid_0551.npy  \n","  inflating: Dataset/train_track_B/centroid_0553.npy  \n","  inflating: Dataset/train_track_B/centroid_0555.npy  \n","  inflating: Dataset/train_track_B/centroid_0557.npy  \n","  inflating: Dataset/train_track_B/centroid_0558.npy  \n","  inflating: Dataset/train_track_B/centroid_0561.npy  \n","  inflating: Dataset/train_track_B/centroid_0563.npy  \n","  inflating: Dataset/train_track_B/centroid_0564.npy  \n","  inflating: Dataset/train_track_B/centroid_0565.npy  \n","  inflating: Dataset/train_track_B/centroid_0567.npy  \n","  inflating: Dataset/train_track_B/centroid_0568.npy  \n","  inflating: Dataset/train_track_B/centroid_0571.npy  \n","  inflating: Dataset/train_track_B/centroid_0574.npy  \n","  inflating: Dataset/train_track_B/centroid_0576.npy  \n","  inflating: Dataset/train_track_B/centroid_0579.npy  \n","  inflating: Dataset/train_track_B/centroid_0580.npy  \n","  inflating: Dataset/train_track_B/centroid_0582.npy  \n","  inflating: Dataset/train_track_B/centroid_0584.npy  \n","  inflating: Dataset/train_track_B/centroid_0585.npy  \n","  inflating: Dataset/train_track_B/centroid_0588.npy  \n","  inflating: Dataset/train_track_B/centroid_0589.npy  \n","  inflating: Dataset/train_track_B/centroid_0590.npy  \n","  inflating: Dataset/train_track_B/centroid_0591.npy  \n","  inflating: Dataset/train_track_B/centroid_0592.npy  \n","  inflating: Dataset/train_track_B/centroid_0593.npy  \n","  inflating: Dataset/train_track_B/centroid_0594.npy  \n","  inflating: Dataset/train_track_B/centroid_0595.npy  \n","  inflating: Dataset/train_track_B/centroid_0596.npy  \n","  inflating: Dataset/train_track_B/centroid_0597.npy  \n","  inflating: Dataset/train_track_B/centroid_0598.npy  \n","  inflating: Dataset/train_track_B/centroid_0600.npy  \n","  inflating: Dataset/train_track_B/centroid_0602.npy  \n","  inflating: Dataset/train_track_B/centroid_0605.npy  \n","  inflating: Dataset/train_track_B/centroid_0608.npy  \n","  inflating: Dataset/train_track_B/centroid_0609.npy  \n","  inflating: Dataset/train_track_B/centroid_0611.npy  \n","  inflating: Dataset/train_track_B/centroid_0612.npy  \n","  inflating: Dataset/train_track_B/centroid_0613.npy  \n","  inflating: Dataset/train_track_B/centroid_0614.npy  \n","  inflating: Dataset/train_track_B/centroid_0618.npy  \n","  inflating: Dataset/train_track_B/centroid_0619.npy  \n","  inflating: Dataset/train_track_B/centroid_0620.npy  \n","  inflating: Dataset/train_track_B/centroid_0621.npy  \n","  inflating: Dataset/train_track_B/centroid_0622.npy  \n","  inflating: Dataset/train_track_B/centroid_0623.npy  \n","  inflating: Dataset/train_track_B/centroid_0624.npy  \n","  inflating: Dataset/train_track_B/centroid_0625.npy  \n","  inflating: Dataset/train_track_B/centroid_0627.npy  \n","  inflating: Dataset/train_track_B/centroid_0628.npy  \n","  inflating: Dataset/train_track_B/centroid_0629.npy  \n","  inflating: Dataset/train_track_B/centroid_0630.npy  \n","  inflating: Dataset/train_track_B/centroid_0631.npy  \n","  inflating: Dataset/train_track_B/centroid_0632.npy  \n","  inflating: Dataset/train_track_B/centroid_0633.npy  \n","  inflating: Dataset/train_track_B/centroid_0634.npy  \n","  inflating: Dataset/train_track_B/centroid_0635.npy  \n","  inflating: Dataset/train_track_B/centroid_0637.npy  \n","  inflating: Dataset/train_track_B/centroid_0638.npy  \n","  inflating: Dataset/train_track_B/centroid_0639.npy  \n","  inflating: Dataset/train_track_B/centroid_0640.npy  \n","  inflating: Dataset/train_track_B/centroid_0641.npy  \n","  inflating: Dataset/train_track_B/centroid_0643.npy  \n","  inflating: Dataset/train_track_B/centroid_0644.npy  \n","  inflating: Dataset/train_track_B/centroid_0645.npy  \n","  inflating: Dataset/train_track_B/centroid_0646.npy  \n","  inflating: Dataset/train_track_B/centroid_0648.npy  \n","  inflating: Dataset/train_track_B/centroid_0650.npy  \n","  inflating: Dataset/train_track_B/centroid_0651.npy  \n","  inflating: Dataset/train_track_B/centroid_0652.npy  \n","  inflating: Dataset/train_track_B/centroid_0653.npy  \n","  inflating: Dataset/train_track_B/centroid_0654.npy  \n","  inflating: Dataset/train_track_B/centroid_0656.npy  \n","  inflating: Dataset/train_track_B/centroid_0657.npy  \n","  inflating: Dataset/train_track_B/centroid_0658.npy  \n","  inflating: Dataset/train_track_B/centroid_0661.npy  \n","  inflating: Dataset/train_track_B/centroid_0663.npy  \n","  inflating: Dataset/train_track_B/centroid_0664.npy  \n","  inflating: Dataset/train_track_B/centroid_0665.npy  \n","  inflating: Dataset/train_track_B/centroid_0666.npy  \n","  inflating: Dataset/train_track_B/centroid_0667.npy  \n","  inflating: Dataset/train_track_B/centroid_0668.npy  \n","  inflating: Dataset/train_track_B/centroid_0669.npy  \n","  inflating: Dataset/train_track_B/centroid_0671.npy  \n","  inflating: Dataset/train_track_B/centroid_0672.npy  \n","  inflating: Dataset/train_track_B/centroid_0673.npy  \n","  inflating: Dataset/train_track_B/centroid_0674.npy  \n","  inflating: Dataset/train_track_B/centroid_0676.npy  \n","  inflating: Dataset/train_track_B/centroid_0677.npy  \n","  inflating: Dataset/train_track_B/centroid_0678.npy  \n","  inflating: Dataset/train_track_B/centroid_0679.npy  \n","  inflating: Dataset/train_track_B/centroid_0680.npy  \n","  inflating: Dataset/train_track_B/centroid_0682.npy  \n","  inflating: Dataset/train_track_B/centroid_0686.npy  \n","  inflating: Dataset/train_track_B/centroid_0688.npy  \n","  inflating: Dataset/train_track_B/centroid_0689.npy  \n","  inflating: Dataset/train_track_B/centroid_0690.npy  \n","  inflating: Dataset/train_track_B/centroid_0691.npy  \n","  inflating: Dataset/train_track_B/centroid_0692.npy  \n","  inflating: Dataset/train_track_B/centroid_0693.npy  \n","  inflating: Dataset/train_track_B/centroid_0694.npy  \n","  inflating: Dataset/train_track_B/centroid_0695.npy  \n","  inflating: Dataset/train_track_B/centroid_0697.npy  \n","  inflating: Dataset/train_track_B/centroid_0699.npy  \n","  inflating: Dataset/train_track_B/centroid_0700.npy  \n","  inflating: Dataset/train_track_B/centroid_0701.npy  \n","  inflating: Dataset/train_track_B/centroid_0703.npy  \n","  inflating: Dataset/train_track_B/centroid_0704.npy  \n","  inflating: Dataset/train_track_B/centroid_0706.npy  \n","  inflating: Dataset/train_track_B/centroid_0707.npy  \n","  inflating: Dataset/train_track_B/centroid_0708.npy  \n","  inflating: Dataset/train_track_B/centroid_0709.npy  \n","  inflating: Dataset/train_track_B/centroid_0711.npy  \n","  inflating: Dataset/train_track_B/centroid_0712.npy  \n","  inflating: Dataset/train_track_B/centroid_0713.npy  \n","  inflating: Dataset/train_track_B/centroid_0714.npy  \n","  inflating: Dataset/train_track_B/centroid_0715.npy  \n","  inflating: Dataset/train_track_B/centroid_0716.npy  \n","  inflating: Dataset/train_track_B/centroid_0718.npy  \n","  inflating: Dataset/train_track_B/centroid_0719.npy  \n","  inflating: Dataset/train_track_B/centroid_0720.npy  \n","  inflating: Dataset/train_track_B/centroid_0721.npy  \n","  inflating: Dataset/train_track_B/centroid_0722.npy  \n","  inflating: Dataset/train_track_B/centroid_0724.npy  \n","  inflating: Dataset/train_track_B/centroid_0727.npy  \n","  inflating: Dataset/train_track_B/centroid_0728.npy  \n","  inflating: Dataset/train_track_B/centroid_0729.npy  \n","  inflating: Dataset/train_track_B/centroid_0730.npy  \n","  inflating: Dataset/train_track_B/centroid_0731.npy  \n","  inflating: Dataset/train_track_B/centroid_0733.npy  \n","  inflating: Dataset/train_track_B/centroid_0735.npy  \n","  inflating: Dataset/train_track_B/centroid_0736.npy  \n","  inflating: Dataset/train_track_B/centroid_0737.npy  \n","  inflating: Dataset/train_track_B/centroid_0740.npy  \n","  inflating: Dataset/train_track_B/centroid_0742.npy  \n","  inflating: Dataset/train_track_B/centroid_0743.npy  \n","  inflating: Dataset/train_track_B/centroid_0744.npy  \n","  inflating: Dataset/train_track_B/centroid_0745.npy  \n","  inflating: Dataset/train_track_B/press_0002.npy  \n","  inflating: Dataset/train_track_B/press_0003.npy  \n","  inflating: Dataset/train_track_B/press_0004.npy  \n","  inflating: Dataset/train_track_B/press_0005.npy  \n","  inflating: Dataset/train_track_B/press_0006.npy  \n","  inflating: Dataset/train_track_B/press_0011.npy  \n","  inflating: Dataset/train_track_B/press_0012.npy  \n","  inflating: Dataset/train_track_B/press_0013.npy  \n","  inflating: Dataset/train_track_B/press_0015.npy  \n","  inflating: Dataset/train_track_B/press_0017.npy  \n","  inflating: Dataset/train_track_B/press_0018.npy  \n","  inflating: Dataset/train_track_B/press_0020.npy  \n","  inflating: Dataset/train_track_B/press_0021.npy  \n","  inflating: Dataset/train_track_B/press_0022.npy  \n","  inflating: Dataset/train_track_B/press_0023.npy  \n","  inflating: Dataset/train_track_B/press_0024.npy  \n","  inflating: Dataset/train_track_B/press_0026.npy  \n","  inflating: Dataset/train_track_B/press_0029.npy  \n","  inflating: Dataset/train_track_B/press_0030.npy  \n","  inflating: Dataset/train_track_B/press_0036.npy  \n","  inflating: Dataset/train_track_B/press_0037.npy  \n","  inflating: Dataset/train_track_B/press_0038.npy  \n","  inflating: Dataset/train_track_B/press_0039.npy  \n","  inflating: Dataset/train_track_B/press_0040.npy  \n","  inflating: Dataset/train_track_B/press_0041.npy  \n","  inflating: Dataset/train_track_B/press_0042.npy  \n","  inflating: Dataset/train_track_B/press_0043.npy  \n","  inflating: Dataset/train_track_B/press_0044.npy  \n","  inflating: Dataset/train_track_B/press_0048.npy  \n","  inflating: Dataset/train_track_B/press_0049.npy  \n","  inflating: Dataset/train_track_B/press_0051.npy  \n","  inflating: Dataset/train_track_B/press_0052.npy  \n","  inflating: Dataset/train_track_B/press_0055.npy  \n","  inflating: Dataset/train_track_B/press_0056.npy  \n","  inflating: Dataset/train_track_B/press_0057.npy  \n","  inflating: Dataset/train_track_B/press_0059.npy  \n","  inflating: Dataset/train_track_B/press_0062.npy  \n","  inflating: Dataset/train_track_B/press_0064.npy  \n","  inflating: Dataset/train_track_B/press_0066.npy  \n","  inflating: Dataset/train_track_B/press_0067.npy  \n","  inflating: Dataset/train_track_B/press_0068.npy  \n","  inflating: Dataset/train_track_B/press_0071.npy  \n","  inflating: Dataset/train_track_B/press_0074.npy  \n","  inflating: Dataset/train_track_B/press_0075.npy  \n","  inflating: Dataset/train_track_B/press_0077.npy  \n","  inflating: Dataset/train_track_B/press_0078.npy  \n","  inflating: Dataset/train_track_B/press_0080.npy  \n","  inflating: Dataset/train_track_B/press_0081.npy  \n","  inflating: Dataset/train_track_B/press_0082.npy  \n","  inflating: Dataset/train_track_B/press_0084.npy  \n","  inflating: Dataset/train_track_B/press_0085.npy  \n","  inflating: Dataset/train_track_B/press_0086.npy  \n","  inflating: Dataset/train_track_B/press_0087.npy  \n","  inflating: Dataset/train_track_B/press_0088.npy  \n","  inflating: Dataset/train_track_B/press_0089.npy  \n","  inflating: Dataset/train_track_B/press_0090.npy  \n","  inflating: Dataset/train_track_B/press_0092.npy  \n","  inflating: Dataset/train_track_B/press_0093.npy  \n","  inflating: Dataset/train_track_B/press_0094.npy  \n","  inflating: Dataset/train_track_B/press_0095.npy  \n","  inflating: Dataset/train_track_B/press_0097.npy  \n","  inflating: Dataset/train_track_B/press_0098.npy  \n","  inflating: Dataset/train_track_B/press_0100.npy  \n","  inflating: Dataset/train_track_B/press_0101.npy  \n","  inflating: Dataset/train_track_B/press_0102.npy  \n","  inflating: Dataset/train_track_B/press_0103.npy  \n","  inflating: Dataset/train_track_B/press_0104.npy  \n","  inflating: Dataset/train_track_B/press_0106.npy  \n","  inflating: Dataset/train_track_B/press_0107.npy  \n","  inflating: Dataset/train_track_B/press_0108.npy  \n","  inflating: Dataset/train_track_B/press_0109.npy  \n","  inflating: Dataset/train_track_B/press_0110.npy  \n","  inflating: Dataset/train_track_B/press_0113.npy  \n","  inflating: Dataset/train_track_B/press_0114.npy  \n","  inflating: Dataset/train_track_B/press_0115.npy  \n","  inflating: Dataset/train_track_B/press_0116.npy  \n","  inflating: Dataset/train_track_B/press_0117.npy  \n","  inflating: Dataset/train_track_B/press_0118.npy  \n","  inflating: Dataset/train_track_B/press_0119.npy  \n","  inflating: Dataset/train_track_B/press_0120.npy  \n","  inflating: Dataset/train_track_B/press_0121.npy  \n","  inflating: Dataset/train_track_B/press_0122.npy  \n","  inflating: Dataset/train_track_B/press_0124.npy  \n","  inflating: Dataset/train_track_B/press_0125.npy  \n","  inflating: Dataset/train_track_B/press_0126.npy  \n","  inflating: Dataset/train_track_B/press_0128.npy  \n","  inflating: Dataset/train_track_B/press_0129.npy  \n","  inflating: Dataset/train_track_B/press_0130.npy  \n","  inflating: Dataset/train_track_B/press_0131.npy  \n","  inflating: Dataset/train_track_B/press_0132.npy  \n","  inflating: Dataset/train_track_B/press_0133.npy  \n","  inflating: Dataset/train_track_B/press_0134.npy  \n","  inflating: Dataset/train_track_B/press_0135.npy  \n","  inflating: Dataset/train_track_B/press_0136.npy  \n","  inflating: Dataset/train_track_B/press_0138.npy  \n","  inflating: Dataset/train_track_B/press_0139.npy  \n","  inflating: Dataset/train_track_B/press_0140.npy  \n","  inflating: Dataset/train_track_B/press_0141.npy  \n","  inflating: Dataset/train_track_B/press_0143.npy  \n","  inflating: Dataset/train_track_B/press_0145.npy  \n","  inflating: Dataset/train_track_B/press_0146.npy  \n","  inflating: Dataset/train_track_B/press_0148.npy  \n","  inflating: Dataset/train_track_B/press_0149.npy  \n","  inflating: Dataset/train_track_B/press_0150.npy  \n","  inflating: Dataset/train_track_B/press_0151.npy  \n","  inflating: Dataset/train_track_B/press_0153.npy  \n","  inflating: Dataset/train_track_B/press_0154.npy  \n","  inflating: Dataset/train_track_B/press_0156.npy  \n","  inflating: Dataset/train_track_B/press_0157.npy  \n","  inflating: Dataset/train_track_B/press_0158.npy  \n","  inflating: Dataset/train_track_B/press_0161.npy  \n","  inflating: Dataset/train_track_B/press_0162.npy  \n","  inflating: Dataset/train_track_B/press_0163.npy  \n","  inflating: Dataset/train_track_B/press_0164.npy  \n","  inflating: Dataset/train_track_B/press_0166.npy  \n","  inflating: Dataset/train_track_B/press_0167.npy  \n","  inflating: Dataset/train_track_B/press_0168.npy  \n","  inflating: Dataset/train_track_B/press_0170.npy  \n","  inflating: Dataset/train_track_B/press_0171.npy  \n","  inflating: Dataset/train_track_B/press_0172.npy  \n","  inflating: Dataset/train_track_B/press_0174.npy  \n","  inflating: Dataset/train_track_B/press_0175.npy  \n","  inflating: Dataset/train_track_B/press_0183.npy  \n","  inflating: Dataset/train_track_B/press_0184.npy  \n","  inflating: Dataset/train_track_B/press_0185.npy  \n","  inflating: Dataset/train_track_B/press_0189.npy  \n","  inflating: Dataset/train_track_B/press_0190.npy  \n","  inflating: Dataset/train_track_B/press_0193.npy  \n","  inflating: Dataset/train_track_B/press_0194.npy  \n","  inflating: Dataset/train_track_B/press_0195.npy  \n","  inflating: Dataset/train_track_B/press_0197.npy  \n","  inflating: Dataset/train_track_B/press_0201.npy  \n","  inflating: Dataset/train_track_B/press_0203.npy  \n","  inflating: Dataset/train_track_B/press_0204.npy  \n","  inflating: Dataset/train_track_B/press_0205.npy  \n","  inflating: Dataset/train_track_B/press_0206.npy  \n","  inflating: Dataset/train_track_B/press_0208.npy  \n","  inflating: Dataset/train_track_B/press_0210.npy  \n","  inflating: Dataset/train_track_B/press_0211.npy  \n","  inflating: Dataset/train_track_B/press_0216.npy  \n","  inflating: Dataset/train_track_B/press_0217.npy  \n","  inflating: Dataset/train_track_B/press_0219.npy  \n","  inflating: Dataset/train_track_B/press_0220.npy  \n","  inflating: Dataset/train_track_B/press_0227.npy  \n","  inflating: Dataset/train_track_B/press_0228.npy  \n","  inflating: Dataset/train_track_B/press_0229.npy  \n","  inflating: Dataset/train_track_B/press_0232.npy  \n","  inflating: Dataset/train_track_B/press_0234.npy  \n","  inflating: Dataset/train_track_B/press_0235.npy  \n","  inflating: Dataset/train_track_B/press_0236.npy  \n","  inflating: Dataset/train_track_B/press_0238.npy  \n","  inflating: Dataset/train_track_B/press_0239.npy  \n","  inflating: Dataset/train_track_B/press_0240.npy  \n","  inflating: Dataset/train_track_B/press_0241.npy  \n","  inflating: Dataset/train_track_B/press_0245.npy  \n","  inflating: Dataset/train_track_B/press_0246.npy  \n","  inflating: Dataset/train_track_B/press_0247.npy  \n","  inflating: Dataset/train_track_B/press_0248.npy  \n","  inflating: Dataset/train_track_B/press_0249.npy  \n","  inflating: Dataset/train_track_B/press_0252.npy  \n","  inflating: Dataset/train_track_B/press_0253.npy  \n","  inflating: Dataset/train_track_B/press_0254.npy  \n","  inflating: Dataset/train_track_B/press_0256.npy  \n","  inflating: Dataset/train_track_B/press_0257.npy  \n","  inflating: Dataset/train_track_B/press_0259.npy  \n","  inflating: Dataset/train_track_B/press_0264.npy  \n","  inflating: Dataset/train_track_B/press_0265.npy  \n","  inflating: Dataset/train_track_B/press_0266.npy  \n","  inflating: Dataset/train_track_B/press_0268.npy  \n","  inflating: Dataset/train_track_B/press_0269.npy  \n","  inflating: Dataset/train_track_B/press_0271.npy  \n","  inflating: Dataset/train_track_B/press_0272.npy  \n","  inflating: Dataset/train_track_B/press_0273.npy  \n","  inflating: Dataset/train_track_B/press_0275.npy  \n","  inflating: Dataset/train_track_B/press_0276.npy  \n","  inflating: Dataset/train_track_B/press_0277.npy  \n","  inflating: Dataset/train_track_B/press_0279.npy  \n","  inflating: Dataset/train_track_B/press_0280.npy  \n","  inflating: Dataset/train_track_B/press_0281.npy  \n","  inflating: Dataset/train_track_B/press_0284.npy  \n","  inflating: Dataset/train_track_B/press_0285.npy  \n","  inflating: Dataset/train_track_B/press_0286.npy  \n","  inflating: Dataset/train_track_B/press_0288.npy  \n","  inflating: Dataset/train_track_B/press_0289.npy  \n","  inflating: Dataset/train_track_B/press_0290.npy  \n","  inflating: Dataset/train_track_B/press_0291.npy  \n","  inflating: Dataset/train_track_B/press_0294.npy  \n","  inflating: Dataset/train_track_B/press_0296.npy  \n","  inflating: Dataset/train_track_B/press_0297.npy  \n","  inflating: Dataset/train_track_B/press_0298.npy  \n","  inflating: Dataset/train_track_B/press_0301.npy  \n","  inflating: Dataset/train_track_B/press_0304.npy  \n","  inflating: Dataset/train_track_B/press_0305.npy  \n","  inflating: Dataset/train_track_B/press_0306.npy  \n","  inflating: Dataset/train_track_B/press_0307.npy  \n","  inflating: Dataset/train_track_B/press_0308.npy  \n","  inflating: Dataset/train_track_B/press_0310.npy  \n","  inflating: Dataset/train_track_B/press_0311.npy  \n","  inflating: Dataset/train_track_B/press_0314.npy  \n","  inflating: Dataset/train_track_B/press_0315.npy  \n","  inflating: Dataset/train_track_B/press_0316.npy  \n","  inflating: Dataset/train_track_B/press_0320.npy  \n","  inflating: Dataset/train_track_B/press_0321.npy  \n","  inflating: Dataset/train_track_B/press_0323.npy  \n","  inflating: Dataset/train_track_B/press_0324.npy  \n","  inflating: Dataset/train_track_B/press_0327.npy  \n","  inflating: Dataset/train_track_B/press_0330.npy  \n","  inflating: Dataset/train_track_B/press_0331.npy  \n","  inflating: Dataset/train_track_B/press_0332.npy  \n","  inflating: Dataset/train_track_B/press_0333.npy  \n","  inflating: Dataset/train_track_B/press_0334.npy  \n","  inflating: Dataset/train_track_B/press_0337.npy  \n","  inflating: Dataset/train_track_B/press_0338.npy  \n","  inflating: Dataset/train_track_B/press_0339.npy  \n","  inflating: Dataset/train_track_B/press_0340.npy  \n","  inflating: Dataset/train_track_B/press_0341.npy  \n","  inflating: Dataset/train_track_B/press_0342.npy  \n","  inflating: Dataset/train_track_B/press_0343.npy  \n","  inflating: Dataset/train_track_B/press_0344.npy  \n","  inflating: Dataset/train_track_B/press_0345.npy  \n","  inflating: Dataset/train_track_B/press_0346.npy  \n","  inflating: Dataset/train_track_B/press_0348.npy  \n","  inflating: Dataset/train_track_B/press_0349.npy  \n","  inflating: Dataset/train_track_B/press_0351.npy  \n","  inflating: Dataset/train_track_B/press_0352.npy  \n","  inflating: Dataset/train_track_B/press_0353.npy  \n","  inflating: Dataset/train_track_B/press_0354.npy  \n","  inflating: Dataset/train_track_B/press_0356.npy  \n","  inflating: Dataset/train_track_B/press_0357.npy  \n","  inflating: Dataset/train_track_B/press_0359.npy  \n","  inflating: Dataset/train_track_B/press_0360.npy  \n","  inflating: Dataset/train_track_B/press_0361.npy  \n","  inflating: Dataset/train_track_B/press_0363.npy  \n","  inflating: Dataset/train_track_B/press_0364.npy  \n","  inflating: Dataset/train_track_B/press_0365.npy  \n","  inflating: Dataset/train_track_B/press_0366.npy  \n","  inflating: Dataset/train_track_B/press_0367.npy  \n","  inflating: Dataset/train_track_B/press_0368.npy  \n","  inflating: Dataset/train_track_B/press_0369.npy  \n","  inflating: Dataset/train_track_B/press_0371.npy  \n","  inflating: Dataset/train_track_B/press_0373.npy  \n","  inflating: Dataset/train_track_B/press_0376.npy  \n","  inflating: Dataset/train_track_B/press_0377.npy  \n","  inflating: Dataset/train_track_B/press_0378.npy  \n","  inflating: Dataset/train_track_B/press_0379.npy  \n","  inflating: Dataset/train_track_B/press_0381.npy  \n","  inflating: Dataset/train_track_B/press_0382.npy  \n","  inflating: Dataset/train_track_B/press_0383.npy  \n","  inflating: Dataset/train_track_B/press_0384.npy  \n","  inflating: Dataset/train_track_B/press_0385.npy  \n","  inflating: Dataset/train_track_B/press_0387.npy  \n","  inflating: Dataset/train_track_B/press_0388.npy  \n","  inflating: Dataset/train_track_B/press_0389.npy  \n","  inflating: Dataset/train_track_B/press_0392.npy  \n","  inflating: Dataset/train_track_B/press_0393.npy  \n","  inflating: Dataset/train_track_B/press_0394.npy  \n","  inflating: Dataset/train_track_B/press_0395.npy  \n","  inflating: Dataset/train_track_B/press_0396.npy  \n","  inflating: Dataset/train_track_B/press_0398.npy  \n","  inflating: Dataset/train_track_B/press_0399.npy  \n","  inflating: Dataset/train_track_B/press_0400.npy  \n","  inflating: Dataset/train_track_B/press_0401.npy  \n","  inflating: Dataset/train_track_B/press_0402.npy  \n","  inflating: Dataset/train_track_B/press_0403.npy  \n","  inflating: Dataset/train_track_B/press_0404.npy  \n","  inflating: Dataset/train_track_B/press_0405.npy  \n","  inflating: Dataset/train_track_B/press_0407.npy  \n","  inflating: Dataset/train_track_B/press_0408.npy  \n","  inflating: Dataset/train_track_B/press_0409.npy  \n","  inflating: Dataset/train_track_B/press_0410.npy  \n","  inflating: Dataset/train_track_B/press_0411.npy  \n","  inflating: Dataset/train_track_B/press_0413.npy  \n","  inflating: Dataset/train_track_B/press_0416.npy  \n","  inflating: Dataset/train_track_B/press_0417.npy  \n","  inflating: Dataset/train_track_B/press_0421.npy  \n","  inflating: Dataset/train_track_B/press_0422.npy  \n","  inflating: Dataset/train_track_B/press_0423.npy  \n","  inflating: Dataset/train_track_B/press_0424.npy  \n","  inflating: Dataset/train_track_B/press_0425.npy  \n","  inflating: Dataset/train_track_B/press_0428.npy  \n","  inflating: Dataset/train_track_B/press_0429.npy  \n","  inflating: Dataset/train_track_B/press_0430.npy  \n","  inflating: Dataset/train_track_B/press_0431.npy  \n","  inflating: Dataset/train_track_B/press_0432.npy  \n","  inflating: Dataset/train_track_B/press_0435.npy  \n","  inflating: Dataset/train_track_B/press_0438.npy  \n","  inflating: Dataset/train_track_B/press_0439.npy  \n","  inflating: Dataset/train_track_B/press_0441.npy  \n","  inflating: Dataset/train_track_B/press_0444.npy  \n","  inflating: Dataset/train_track_B/press_0445.npy  \n","  inflating: Dataset/train_track_B/press_0449.npy  \n","  inflating: Dataset/train_track_B/press_0450.npy  \n","  inflating: Dataset/train_track_B/press_0451.npy  \n","  inflating: Dataset/train_track_B/press_0452.npy  \n","  inflating: Dataset/train_track_B/press_0453.npy  \n","  inflating: Dataset/train_track_B/press_0456.npy  \n","  inflating: Dataset/train_track_B/press_0457.npy  \n","  inflating: Dataset/train_track_B/press_0458.npy  \n","  inflating: Dataset/train_track_B/press_0459.npy  \n","  inflating: Dataset/train_track_B/press_0460.npy  \n","  inflating: Dataset/train_track_B/press_0461.npy  \n","  inflating: Dataset/train_track_B/press_0463.npy  \n","  inflating: Dataset/train_track_B/press_0464.npy  \n","  inflating: Dataset/train_track_B/press_0465.npy  \n","  inflating: Dataset/train_track_B/press_0467.npy  \n","  inflating: Dataset/train_track_B/press_0469.npy  \n","  inflating: Dataset/train_track_B/press_0471.npy  \n","  inflating: Dataset/train_track_B/press_0472.npy  \n","  inflating: Dataset/train_track_B/press_0474.npy  \n","  inflating: Dataset/train_track_B/press_0475.npy  \n","  inflating: Dataset/train_track_B/press_0477.npy  \n","  inflating: Dataset/train_track_B/press_0478.npy  \n","  inflating: Dataset/train_track_B/press_0479.npy  \n","  inflating: Dataset/train_track_B/press_0480.npy  \n","  inflating: Dataset/train_track_B/press_0481.npy  \n","  inflating: Dataset/train_track_B/press_0482.npy  \n","  inflating: Dataset/train_track_B/press_0485.npy  \n","  inflating: Dataset/train_track_B/press_0486.npy  \n","  inflating: Dataset/train_track_B/press_0487.npy  \n","  inflating: Dataset/train_track_B/press_0488.npy  \n","  inflating: Dataset/train_track_B/press_0489.npy  \n","  inflating: Dataset/train_track_B/press_0492.npy  \n","  inflating: Dataset/train_track_B/press_0493.npy  \n","  inflating: Dataset/train_track_B/press_0494.npy  \n","  inflating: Dataset/train_track_B/press_0497.npy  \n","  inflating: Dataset/train_track_B/press_0498.npy  \n","  inflating: Dataset/train_track_B/press_0499.npy  \n","  inflating: Dataset/train_track_B/press_0501.npy  \n","  inflating: Dataset/train_track_B/press_0502.npy  \n","  inflating: Dataset/train_track_B/press_0503.npy  \n","  inflating: Dataset/train_track_B/press_0504.npy  \n","  inflating: Dataset/train_track_B/press_0507.npy  \n","  inflating: Dataset/train_track_B/press_0508.npy  \n","  inflating: Dataset/train_track_B/press_0509.npy  \n","  inflating: Dataset/train_track_B/press_0513.npy  \n","  inflating: Dataset/train_track_B/press_0514.npy  \n","  inflating: Dataset/train_track_B/press_0515.npy  \n","  inflating: Dataset/train_track_B/press_0517.npy  \n","  inflating: Dataset/train_track_B/press_0518.npy  \n","  inflating: Dataset/train_track_B/press_0519.npy  \n","  inflating: Dataset/train_track_B/press_0520.npy  \n","  inflating: Dataset/train_track_B/press_0521.npy  \n","  inflating: Dataset/train_track_B/press_0522.npy  \n","  inflating: Dataset/train_track_B/press_0523.npy  \n","  inflating: Dataset/train_track_B/press_0524.npy  \n","  inflating: Dataset/train_track_B/press_0525.npy  \n","  inflating: Dataset/train_track_B/press_0526.npy  \n","  inflating: Dataset/train_track_B/press_0527.npy  \n","  inflating: Dataset/train_track_B/press_0528.npy  \n","  inflating: Dataset/train_track_B/press_0529.npy  \n","  inflating: Dataset/train_track_B/press_0530.npy  \n","  inflating: Dataset/train_track_B/press_0531.npy  \n","  inflating: Dataset/train_track_B/press_0534.npy  \n","  inflating: Dataset/train_track_B/press_0535.npy  \n","  inflating: Dataset/train_track_B/press_0536.npy  \n","  inflating: Dataset/train_track_B/press_0538.npy  \n","  inflating: Dataset/train_track_B/press_0541.npy  \n","  inflating: Dataset/train_track_B/press_0542.npy  \n","  inflating: Dataset/train_track_B/press_0544.npy  \n","  inflating: Dataset/train_track_B/press_0545.npy  \n","  inflating: Dataset/train_track_B/press_0546.npy  \n","  inflating: Dataset/train_track_B/press_0547.npy  \n","  inflating: Dataset/train_track_B/press_0550.npy  \n","  inflating: Dataset/train_track_B/press_0551.npy  \n","  inflating: Dataset/train_track_B/press_0553.npy  \n","  inflating: Dataset/train_track_B/press_0555.npy  \n","  inflating: Dataset/train_track_B/press_0557.npy  \n","  inflating: Dataset/train_track_B/press_0558.npy  \n","  inflating: Dataset/train_track_B/press_0561.npy  \n","  inflating: Dataset/train_track_B/press_0563.npy  \n","  inflating: Dataset/train_track_B/press_0564.npy  \n","  inflating: Dataset/train_track_B/press_0565.npy  \n","  inflating: Dataset/train_track_B/press_0567.npy  \n","  inflating: Dataset/train_track_B/press_0568.npy  \n","  inflating: Dataset/train_track_B/press_0571.npy  \n","  inflating: Dataset/train_track_B/press_0574.npy  \n","  inflating: Dataset/train_track_B/press_0576.npy  \n","  inflating: Dataset/train_track_B/press_0579.npy  \n","  inflating: Dataset/train_track_B/press_0580.npy  \n","  inflating: Dataset/train_track_B/press_0582.npy  \n","  inflating: Dataset/train_track_B/press_0584.npy  \n","  inflating: Dataset/train_track_B/press_0585.npy  \n","  inflating: Dataset/train_track_B/press_0588.npy  \n","  inflating: Dataset/train_track_B/press_0589.npy  \n","  inflating: Dataset/train_track_B/press_0590.npy  \n","  inflating: Dataset/train_track_B/press_0591.npy  \n","  inflating: Dataset/train_track_B/press_0592.npy  \n","  inflating: Dataset/train_track_B/press_0593.npy  \n","  inflating: Dataset/train_track_B/press_0594.npy  \n","  inflating: Dataset/train_track_B/press_0595.npy  \n","  inflating: Dataset/train_track_B/press_0596.npy  \n","  inflating: Dataset/train_track_B/press_0597.npy  \n","  inflating: Dataset/train_track_B/press_0598.npy  \n","  inflating: Dataset/train_track_B/press_0600.npy  \n","  inflating: Dataset/train_track_B/press_0602.npy  \n","  inflating: Dataset/train_track_B/press_0605.npy  \n","  inflating: Dataset/train_track_B/press_0608.npy  \n","  inflating: Dataset/train_track_B/press_0609.npy  \n","  inflating: Dataset/train_track_B/press_0611.npy  \n","  inflating: Dataset/train_track_B/press_0612.npy  \n","  inflating: Dataset/train_track_B/press_0613.npy  \n","  inflating: Dataset/train_track_B/press_0614.npy  \n","  inflating: Dataset/train_track_B/press_0618.npy  \n","  inflating: Dataset/train_track_B/press_0619.npy  \n","  inflating: Dataset/train_track_B/press_0620.npy  \n","  inflating: Dataset/train_track_B/press_0621.npy  \n","  inflating: Dataset/train_track_B/press_0622.npy  \n","  inflating: Dataset/train_track_B/press_0623.npy  \n","  inflating: Dataset/train_track_B/press_0624.npy  \n","  inflating: Dataset/train_track_B/press_0625.npy  \n","  inflating: Dataset/train_track_B/press_0627.npy  \n","  inflating: Dataset/train_track_B/press_0628.npy  \n","  inflating: Dataset/train_track_B/press_0629.npy  \n","  inflating: Dataset/train_track_B/press_0630.npy  \n","  inflating: Dataset/train_track_B/press_0631.npy  \n","  inflating: Dataset/train_track_B/press_0632.npy  \n","  inflating: Dataset/train_track_B/press_0633.npy  \n","  inflating: Dataset/train_track_B/press_0634.npy  \n","  inflating: Dataset/train_track_B/press_0635.npy  \n","  inflating: Dataset/train_track_B/press_0637.npy  \n","  inflating: Dataset/train_track_B/press_0638.npy  \n","  inflating: Dataset/train_track_B/press_0639.npy  \n","  inflating: Dataset/train_track_B/press_0640.npy  \n","  inflating: Dataset/train_track_B/press_0641.npy  \n","  inflating: Dataset/train_track_B/press_0643.npy  \n","  inflating: Dataset/train_track_B/press_0644.npy  \n","  inflating: Dataset/train_track_B/press_0645.npy  \n","  inflating: Dataset/train_track_B/press_0646.npy  \n","  inflating: Dataset/train_track_B/press_0648.npy  \n","  inflating: Dataset/train_track_B/press_0650.npy  \n","  inflating: Dataset/train_track_B/press_0651.npy  \n","  inflating: Dataset/train_track_B/press_0652.npy  \n","  inflating: Dataset/train_track_B/press_0653.npy  \n","  inflating: Dataset/train_track_B/press_0654.npy  \n","  inflating: Dataset/train_track_B/press_0656.npy  \n","  inflating: Dataset/train_track_B/press_0657.npy  \n","  inflating: Dataset/train_track_B/press_0658.npy  \n","  inflating: Dataset/train_track_B/press_0661.npy  \n","  inflating: Dataset/train_track_B/press_0663.npy  \n","  inflating: Dataset/train_track_B/press_0664.npy  \n","  inflating: Dataset/train_track_B/press_0665.npy  \n","  inflating: Dataset/train_track_B/press_0666.npy  \n","  inflating: Dataset/train_track_B/press_0667.npy  \n","  inflating: Dataset/train_track_B/press_0668.npy  \n","  inflating: Dataset/train_track_B/press_0669.npy  \n","  inflating: Dataset/train_track_B/press_0671.npy  \n","  inflating: Dataset/train_track_B/press_0672.npy  \n","  inflating: Dataset/train_track_B/press_0673.npy  \n","  inflating: Dataset/train_track_B/press_0674.npy  \n","  inflating: Dataset/train_track_B/press_0676.npy  \n","  inflating: Dataset/train_track_B/press_0677.npy  \n","  inflating: Dataset/train_track_B/press_0678.npy  \n","  inflating: Dataset/train_track_B/press_0679.npy  \n","  inflating: Dataset/train_track_B/press_0680.npy  \n","  inflating: Dataset/train_track_B/press_0682.npy  \n","  inflating: Dataset/train_track_B/press_0686.npy  \n","  inflating: Dataset/train_track_B/press_0688.npy  \n","  inflating: Dataset/train_track_B/press_0689.npy  \n","  inflating: Dataset/train_track_B/press_0690.npy  \n","  inflating: Dataset/train_track_B/press_0691.npy  \n","  inflating: Dataset/train_track_B/press_0692.npy  \n","  inflating: Dataset/train_track_B/press_0693.npy  \n","  inflating: Dataset/train_track_B/press_0694.npy  \n","  inflating: Dataset/train_track_B/press_0695.npy  \n","  inflating: Dataset/train_track_B/press_0697.npy  \n","  inflating: Dataset/train_track_B/press_0699.npy  \n","  inflating: Dataset/train_track_B/press_0700.npy  \n","  inflating: Dataset/train_track_B/press_0701.npy  \n","  inflating: Dataset/train_track_B/press_0703.npy  \n","  inflating: Dataset/train_track_B/press_0704.npy  \n","  inflating: Dataset/train_track_B/press_0706.npy  \n","  inflating: Dataset/train_track_B/press_0707.npy  \n","  inflating: Dataset/train_track_B/press_0708.npy  \n","  inflating: Dataset/train_track_B/press_0709.npy  \n","  inflating: Dataset/train_track_B/press_0711.npy  \n","  inflating: Dataset/train_track_B/press_0712.npy  \n","  inflating: Dataset/train_track_B/press_0713.npy  \n","  inflating: Dataset/train_track_B/press_0714.npy  \n","  inflating: Dataset/train_track_B/press_0715.npy  \n","  inflating: Dataset/train_track_B/press_0716.npy  \n","  inflating: Dataset/train_track_B/press_0718.npy  \n","  inflating: Dataset/train_track_B/press_0719.npy  \n","  inflating: Dataset/train_track_B/press_0720.npy  \n","  inflating: Dataset/train_track_B/press_0721.npy  \n","  inflating: Dataset/train_track_B/press_0722.npy  \n","  inflating: Dataset/train_track_B/press_0724.npy  \n","  inflating: Dataset/train_track_B/press_0727.npy  \n","  inflating: Dataset/train_track_B/press_0728.npy  \n","  inflating: Dataset/train_track_B/press_0729.npy  \n","  inflating: Dataset/train_track_B/press_0730.npy  \n","  inflating: Dataset/train_track_B/press_0731.npy  \n","  inflating: Dataset/train_track_B/press_0733.npy  \n","  inflating: Dataset/train_track_B/press_0735.npy  \n","  inflating: Dataset/train_track_B/press_0736.npy  \n","  inflating: Dataset/train_track_B/press_0737.npy  \n","  inflating: Dataset/train_track_B/press_0740.npy  \n","  inflating: Dataset/train_track_B/press_0742.npy  \n","  inflating: Dataset/train_track_B/press_0743.npy  \n","  inflating: Dataset/train_track_B/press_0744.npy  \n","  inflating: Dataset/train_track_B/press_0745.npy  \n","Archive:  mesh_B_0603.zip\n","   creating: Dataset/train_mesh_0603/\n","  inflating: Dataset/train_mesh_0603/mesh_0396.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0364.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0044.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0052.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0059.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0403.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0383.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0340.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0342.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0489.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0022.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0513.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0439.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0408.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0170.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0398.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0026.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0522.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0138.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0429.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0706.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0048.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0018.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0431.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0122.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0294.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0729.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0394.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0672.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0667.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0449.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0401.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0564.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0458.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0074.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0228.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0630.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0100.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0451.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0692.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0671.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0388.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0149.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0690.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0121.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0657.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0120.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0353.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0232.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0229.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0082.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0344.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0089.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0656.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0592.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0288.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0239.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0204.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0663.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0711.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0092.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0049.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0013.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0720.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0499.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0206.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0679.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0266.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0183.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0051.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0078.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0550.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0131.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0611.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0584.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0039.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0341.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0469.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0713.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0139.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0497.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0256.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0542.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0425.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0392.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0308.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0502.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0526.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0555.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0346.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0281.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0002.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0665.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0334.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0354.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0087.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0402.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0515.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0493.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0301.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0719.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0259.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0541.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0273.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0146.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0645.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0509.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0648.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0310.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0553.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0535.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0422.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0534.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0653.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0707.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0457.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0700.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0432.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0254.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0369.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0423.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0737.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0463.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0609.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0580.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0043.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0682.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0728.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0201.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0269.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0620.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0597.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0037.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0385.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0472.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0103.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0143.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0004.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0093.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0345.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0071.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0518.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0389.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0558.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0088.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0277.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0085.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0699.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0348.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0379.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0438.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0595.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0531.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0529.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0384.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0716.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0689.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0238.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0320.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0730.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0298.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0357.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0175.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0066.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0413.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0343.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0140.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0530.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0693.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0029.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0625.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0525.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0331.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0567.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0321.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0416.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0585.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0618.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0527.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0227.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0546.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0377.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0316.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0709.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0168.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0421.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0613.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0588.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0257.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0715.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0704.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0445.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0156.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0589.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0523.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0382.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0594.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0203.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0478.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0631.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0163.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0366.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0174.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0640.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0284.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0118.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0695.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0135.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0167.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0461.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0743.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0162.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0477.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0080.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0435.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0639.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0023.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0635.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0077.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0524.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0132.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0676.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0666.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0086.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0520.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0015.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0234.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0646.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0582.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0551.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0708.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0378.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0464.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0459.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0731.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0571.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0285.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0220.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0678.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0568.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0460.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0736.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0627.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0688.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0109.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0075.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0306.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0367.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0669.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0673.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0517.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0536.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0055.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0395.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0545.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0264.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0057.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0133.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0482.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0733.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0467.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0359.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0030.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0650.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0417.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0041.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0349.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0474.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0638.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0114.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0637.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0104.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0727.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0108.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0686.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0136.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0744.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0351.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0124.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0247.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0411.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0017.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0504.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0128.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0632.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0119.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0465.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0171.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0368.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0164.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0424.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0211.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0216.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0272.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0311.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0399.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0062.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0718.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0241.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0190.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0456.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0428.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0253.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0621.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0479.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0590.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0486.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0081.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0579.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0184.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0145.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0352.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0116.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0407.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0694.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0481.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0323.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0376.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0452.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0745.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0501.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0265.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0021.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0110.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0208.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0338.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0006.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0488.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0185.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0115.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0148.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0680.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0404.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0508.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0661.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0356.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0290.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0547.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0430.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0305.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0219.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0117.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0154.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0151.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0271.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0405.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0040.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0608.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0712.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0703.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0596.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0722.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0098.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0668.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0721.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0528.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0307.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0565.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0280.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0130.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0235.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0651.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0172.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0614.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0279.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0102.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0249.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0612.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0691.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0126.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0450.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0134.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0563.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0652.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0005.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0097.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0360.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0514.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0697.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0064.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0521.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0286.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0210.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0042.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0056.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0538.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0158.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0141.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0480.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0393.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0409.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0268.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0485.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0561.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0654.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0444.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0291.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0498.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0036.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0658.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0544.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0240.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0644.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0593.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0296.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0619.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0574.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0252.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0641.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0106.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0629.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0701.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0400.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0314.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0373.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0598.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0677.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0492.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0276.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0628.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0150.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0217.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0153.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0624.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0503.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0107.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0441.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0024.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0248.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0101.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0600.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0113.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0591.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0339.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0453.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0157.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0245.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0410.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0332.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0094.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0084.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0714.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0304.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0297.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0161.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0605.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0195.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0068.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0020.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0011.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0166.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0602.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0576.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0129.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0674.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0622.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0289.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0125.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0487.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0205.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0742.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0194.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0381.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0507.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0095.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0315.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0090.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0494.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0067.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0643.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0724.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0337.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0327.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0193.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0735.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0634.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0012.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0189.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0475.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0557.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0519.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0623.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0664.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0236.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0363.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0387.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0633.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0246.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0003.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0197.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0361.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0038.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0471.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0333.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0324.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0330.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0740.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0275.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0371.ply  \n","  inflating: Dataset/train_mesh_0603/mesh_0365.ply  \n"]}]},{"cell_type":"markdown","source":["# 数据处理"],"metadata":{"id":"-Vj68rbBEKES"}},{"cell_type":"markdown","source":["以下代码是将数据处理成官方文件夹的形式，单纯文件移动操作，不引入新的数据"],"metadata":{"id":"Z9z9-NNU3F46"}},{"cell_type":"code","source":["import os\n","import shutil\n","\n","# 指定 train_data 目录路径\n","path = 'Dataset/data'\n","\n","# 新建 Feature 和 Label 文件夹\n","Feature_path = os.path.join(path, 'Feature')\n","Label_path = os.path.join(path, 'Label')\n","os.makedirs(Feature_path, exist_ok=True)\n","os.makedirs(Label_path, exist_ok=True)\n","\n","# 遍历目录中的文件\n","for filename in os.listdir(path):\n","    # 跳过目录，只处理文件\n","    if os.path.isfile(os.path.join(path, filename)):\n","        # 获取文件名前缀\n","        prefix = filename.split('_')[0]\n","        # 检查文件名前缀是否是 area, info, normal 或 centroid\n","        if prefix in ['mesh']:\n","            # 构建源文件和目标文件路径\n","            src_file = os.path.join(path, filename)\n","            dst_file = os.path.join(Feature_path, filename)\n","            # 移动文件\n","            shutil.move(src_file, dst_file)\n","            # print(f'Moved {filename} to {Feature_path}')\n","        elif prefix in ['press']:\n","            # 构建源文件和目标文件路径\n","            src_file = os.path.join(path, filename)\n","            dst_file = os.path.join(Label_path, filename)\n","            # 移动文件\n","            shutil.move(src_file, dst_file)\n","            # print(f'Moved {filename} to {Label_path}')"],"metadata":{"id":"PCb-JOgEWC-u","executionInfo":{"status":"ok","timestamp":1721014121176,"user_tz":-480,"elapsed":8,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","# 指定 track_a 目录路径\n","track_a_path = 'Dataset/track_A'\n","\n","# 新建Inference 文件夹\n","\n","inference_path = os.path.join(track_a_path, 'Inference')\n","\n","os.makedirs(inference_path, exist_ok=True)\n","\n","# 遍历 track_b 目录中的文件\n","for filename in os.listdir(track_a_path):\n","    # 跳过目录，只处理文件\n","    if os.path.isfile(os.path.join(track_a_path, filename)):\n","        # 获取文件名前缀\n","        prefix = filename.split('_')[0]\n","        # 检查文件名前缀是否是 area, info, normal 或 centroid\n","        if prefix in ['mesh']:\n","            # 构建源文件和目标文件路径\n","            src_file = os.path.join(track_a_path, filename)\n","            dst_file = os.path.join(inference_path, filename)\n","            # 移动文件\n","            shutil.move(src_file, dst_file)\n","            # print(f'Moved {filename} to {inference_path}')\n"],"metadata":{"id":"coE0Qu-YU8pX","executionInfo":{"status":"ok","timestamp":1721014121176,"user_tz":-480,"elapsed":5,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","\n","# 指定 track_b 目录路径\n","track_b_path = 'Dataset/track_B'\n","\n","# 新建 Auxiliary 和 Inference 文件夹\n","auxiliary_path = os.path.join(track_b_path, 'Auxiliary')\n","inference_path = os.path.join(track_b_path, 'Inference')\n","os.makedirs(auxiliary_path, exist_ok=True)\n","os.makedirs(inference_path, exist_ok=True)\n","\n","# 遍历 track_b 目录中的文件\n","for filename in os.listdir(track_b_path):\n","    # 跳过目录，只处理文件\n","    if os.path.isfile(os.path.join(track_b_path, filename)):\n","        # 获取文件名前缀\n","        prefix = filename.split('_')[0]\n","        # 检查文件名前缀是否是 area, info, normal 或 centroid\n","        if prefix in ['area', 'info', 'normal']:\n","            # 构建源文件和目标文件路径\n","            src_file = os.path.join(track_b_path, filename)\n","            dst_file = os.path.join(auxiliary_path, filename)\n","            # 移动文件\n","            shutil.move(src_file, dst_file)\n","            # print(f'Moved {filename} to {auxiliary_path}')\n","        elif prefix in ['centroid','mesh']:\n","            # 构建源文件和目标文件路径\n","            src_file = os.path.join(track_b_path, filename)\n","            dst_file = os.path.join(inference_path, filename)\n","            # 移动文件\n","            shutil.move(src_file, dst_file)\n","            # print(f'Moved {filename} to {inference_path}')\n","\n"],"metadata":{"id":"UhavukX0TFcQ","executionInfo":{"status":"ok","timestamp":1721014121176,"user_tz":-480,"elapsed":5,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"VV4alT_R22se"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","# -*- coding:utf-8 _*-\n","import math\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","# import dgl\n","from einops import repeat, rearrange\n","from torch.nn import functional as F\n","from torch.nn import GELU, ReLU, Tanh, Sigmoid\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# from utils import MultipleTensors\n","\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","ACTIVATION = {'gelu':nn.GELU(),'tanh':nn.Tanh(),'sigmoid':nn.Sigmoid(),'relu':nn.ReLU(),'leaky_relu':nn.LeakyReLU(0.1),'softplus':nn.Softplus(),'ELU':nn.ELU()}\n","\n","'''\n","    A simple MLP class, includes at least 2 layers and n hidden layers\n","'''\n","class MLP(nn.Module):\n","    def __init__(self, n_input, n_hidden, n_output, n_layers=1, act='gelu'):\n","        super(MLP, self).__init__()\n","\n","        if act in ACTIVATION.keys():\n","            self.act = ACTIVATION[act]\n","        else:\n","            raise NotImplementedError\n","        self.n_input = n_input\n","        self.n_hidden = n_hidden\n","        self.n_output = n_output\n","        self.n_layers = n_layers\n","        self.linear_pre = nn.Linear(n_input, n_hidden)\n","        self.linear_post = nn.Linear(n_hidden, n_output)\n","        self.linears = nn.ModuleList([nn.Linear(n_hidden, n_hidden) for _ in range(n_layers)])\n","\n","        # self.bns = nn.ModuleList([nn.BatchNorm1d(n_hidden) for _ in range(n_layers)])\n","\n","\n","\n","    def forward(self, x):\n","        x = self.act(self.linear_pre(x))\n","        for i in range(self.n_layers):\n","            x = self.act(self.linears[i](x)) + x\n","            # x = self.act(self.bns[i](self.linears[i](x))) + x\n","\n","        x = self.linear_post(x)\n","        return x\n","\n","\n","\n","class MultipleTensors():\n","    def __init__(self, x):\n","        self.x = x\n","\n","    def to(self, device):\n","        self.x = [x_.to(device) for x_ in self.x]\n","        return self\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, item):\n","        return self.x[item]\n","\n","\n","class MoEGPTConfig():\n","    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n","\n","    def __init__(self, attn_type='linear', embd_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0, n_embd=128, n_head=1,\n","                 n_layer=3, block_size=128, n_inner=4, act='gelu', n_experts=2, space_dim=1, branch_sizes=None,\n","                 n_inputs=1):\n","        self.attn_type = attn_type\n","        self.embd_pdrop = embd_pdrop\n","        self.resid_pdrop = resid_pdrop\n","        self.attn_pdrop = attn_pdrop\n","        self.n_embd = n_embd  # 64\n","        self.n_head = n_head\n","        self.n_layer = n_layer\n","        self.block_size = block_size\n","        self.n_inner = n_inner * self.n_embd\n","        self.act = act\n","        self.n_experts = n_experts\n","        self.space_dim = space_dim\n","        self.branch_sizes = branch_sizes\n","        self.n_inputs = n_inputs\n","\n","\n","class LinearAttention(nn.Module):\n","    \"\"\"\n","    A vanilla multi-head masked self-attention layer with a projection at the end.\n","    It is possible to use torch.nn.MultiheadAttention here but I am including an\n","    explicit implementation here to show that there is nothing too scary here.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(LinearAttention, self).__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads\n","        self.key = nn.Linear(config.n_embd, config.n_embd)\n","        self.query = nn.Linear(config.n_embd, config.n_embd)\n","        self.value = nn.Linear(config.n_embd, config.n_embd)\n","        # regularization\n","        self.attn_drop = nn.Dropout(config.attn_pdrop)\n","        # output projection\n","        self.proj = nn.Linear(config.n_embd, config.n_embd)\n","\n","        self.n_head = config.n_head\n","\n","        self.attn_type = 'l1'\n","\n","    '''\n","        Linear Attention and Linear Cross Attention (if y is provided)\n","    '''\n","\n","    def forward(self, x, y=None, layer_past=None):\n","        y = x if y is None else y\n","        B, T1, C = x.size()\n","        _, T2, _ = y.size()\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        k = self.key(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        v = self.value(y).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","\n","        if self.attn_type == 'l1':\n","            q = q.softmax(dim=-1)\n","            k = k.softmax(dim=-1)  #\n","            k_cumsum = k.sum(dim=-2, keepdim=True)\n","            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized\n","        elif self.attn_type == \"galerkin\":\n","            q = q.softmax(dim=-1)\n","            k = k.softmax(dim=-1)  #\n","            D_inv = 1. / T2  # galerkin\n","        elif self.attn_type == \"l2\":  # still use l1 normalization\n","            q = q / q.norm(dim=-1, keepdim=True, p=1)\n","            k = k / k.norm(dim=-1, keepdim=True, p=1)\n","            k_cumsum = k.sum(dim=-2, keepdim=True)\n","            D_inv = 1. / (q * k_cumsum).abs().sum(dim=-1, keepdim=True)  # 这里有没有做sum都可以，按照论文公式是没有sum  # normalized\n","        else:\n","            raise NotImplementedError\n","\n","        context = k.transpose(-2, -1) @ v\n","        y = self.attn_drop((q @ context) * D_inv + q)\n","\n","        # output projection\n","        y = rearrange(y, 'b h n d -> b n (h d)')\n","        y = self.proj(y)\n","        return y\n","\n","\n","class LinearCrossAttention(nn.Module):\n","    \"\"\"\n","    A vanilla multi-head masked self-attention layer with a projection at the end.\n","    It is possible to use torch.nn.MultiheadAttention here but I am including an\n","    explicit implementation here to show that there is nothing too scary here.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super(LinearCrossAttention, self).__init__()\n","        assert config.n_embd % config.n_head == 0\n","        # key, query, value projections for all heads\n","        self.query = nn.Linear(config.n_embd, config.n_embd)\n","        self.keys = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n","        self.values = nn.ModuleList([nn.Linear(config.n_embd, config.n_embd) for _ in range(config.n_inputs)])\n","        # regularization\n","        self.attn_drop = nn.Dropout(config.attn_pdrop)\n","        # output projection\n","        self.proj = nn.Linear(config.n_embd, config.n_embd)\n","\n","        self.n_head = config.n_head\n","        self.n_inputs = config.n_inputs\n","\n","        self.attn_type = 'l1'\n","\n","    '''\n","        Linear Attention and Linear Cross Attention (if y is provided)\n","    '''\n","\n","    def forward(self, x, y=None, layer_past=None):\n","        y = x if y is None else y\n","        B, T1, C = x.size()\n","        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n","        q = self.query(x).view(B, T1, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        q = q.softmax(dim=-1)\n","        out = q\n","        for i in range(self.n_inputs):\n","            _, T2, _ = y[i].size()\n","            k = self.keys[i](y[i]).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","            v = self.values[i](y[i]).view(B, T2, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","            k = k.softmax(dim=-1)  #\n","            k_cumsum = k.sum(dim=-2, keepdim=True)\n","            D_inv = 1. / (q * k_cumsum).sum(dim=-1, keepdim=True)  # normalized\n","            out = out + 1 * (q @ (k.transpose(-2, -1) @ v)) * D_inv\n","\n","        # output projection\n","        out = rearrange(out, 'b h n d -> b n (h d)')\n","        out = self.proj(out)\n","        return out\n","\n","\n","'''\n","    X: N*T*C --> N*(4*n + 3)*C\n","'''\n","\n","\n","def horizontal_fourier_embedding(X, n=3):\n","    freqs = 2 ** torch.linspace(-n, n, 2 * n + 1).to(X.device)\n","    freqs = freqs[None, None, None, ...]\n","    X_ = X.unsqueeze(-1).repeat([1, 1, 1, 2 * n + 1])\n","    X_cos = torch.cos(freqs * X_)\n","    X_sin = torch.sin(freqs * X_)\n","    X = torch.cat([X.unsqueeze(-1), X_cos, X_sin], dim=-1).view(X.shape[0], X.shape[1], -1)\n","    return X\n","\n","\n","'''\n","    Self and Cross Attention block for CGPT, contains  a cross attention block and a self attention block\n","'''\n","\n","\n","class MIOECrossAttentionBlock(nn.Module):\n","    def __init__(self, config):\n","        super(MIOECrossAttentionBlock, self).__init__()\n","        self.ln1 = nn.LayerNorm(config.n_embd)\n","        self.ln2_branch = nn.ModuleList([nn.LayerNorm(config.n_embd) for _ in range(config.n_inputs)])\n","        self.ln3 = nn.LayerNorm(config.n_embd)\n","        self.ln4 = nn.LayerNorm(config.n_embd)\n","        self.ln5 = nn.LayerNorm(config.n_embd)\n","        if config.attn_type == 'linear':\n","            print('Using Linear Attention')\n","            self.selfattn = LinearAttention(config)\n","            self.crossattn = LinearCrossAttention(config)\n","        else:\n","            raise NotImplementedError\n","\n","        if config.act == 'gelu':\n","            self.act = GELU\n","        elif config.act == \"tanh\":\n","            self.act = Tanh\n","        elif config.act == 'relu':\n","            self.act = ReLU\n","        elif config.act == 'sigmoid':\n","            self.act = Sigmoid\n","\n","        self.resid_drop1 = nn.Dropout(config.resid_pdrop)\n","        self.resid_drop2 = nn.Dropout(config.resid_pdrop)\n","\n","        self.n_experts = config.n_experts\n","        self.n_inputs = config.n_inputs\n","\n","        self.moe_mlp1 = nn.ModuleList([nn.Sequential(\n","            nn.Linear(config.n_embd, config.n_inner),\n","            self.act(),\n","            nn.Linear(config.n_inner, config.n_embd),\n","        ) for _ in range(self.n_experts)])\n","\n","        self.moe_mlp2 = nn.ModuleList([nn.Sequential(\n","            nn.Linear(config.n_embd, config.n_inner),\n","            self.act(),\n","            nn.Linear(config.n_inner, config.n_embd),\n","        ) for _ in range(self.n_experts)])\n","\n","        self.gatenet = nn.Sequential(\n","            nn.Linear(config.space_dim, config.n_inner),\n","            self.act(),\n","            nn.Linear(config.n_inner, config.n_inner),\n","            self.act(),\n","            nn.Linear(config.n_inner, self.n_experts)\n","        )\n","\n","    def ln_branchs(self, y):\n","        return MultipleTensors([self.ln2_branch[i](y[i]) for i in range(self.n_inputs)])\n","\n","    '''\n","        x: [B, T1, C], y:[B, T2, C], pos:[B, T1, n]\n","    '''\n","\n","    def forward(self, x, y, pos):\n","        gate_score = F.softmax(self.gatenet(pos), dim=-1).unsqueeze(2)  # B, T1, 1, m\n","        x = x + self.resid_drop1(self.crossattn(self.ln1(x), self.ln_branchs(y)))\n","        x_moe1 = torch.stack([self.moe_mlp1[i](x) for i in range(self.n_experts)], dim=-1)  # B, T1, C, m\n","        x_moe1 = (gate_score * x_moe1).sum(dim=-1, keepdim=False)\n","        x = x + self.ln3(x_moe1)\n","        x = x + self.resid_drop2(self.selfattn(self.ln4(x)))\n","        x_moe2 = torch.stack([self.moe_mlp2[i](x) for i in range(self.n_experts)], dim=-1)  # B, T1, C, m\n","        x_moe2 = (gate_score * x_moe2).sum(dim=-1, keepdim=False)\n","        x = x + self.ln5(x_moe2)\n","        return x\n","\n","    #### No layernorm\n","    # def forward(self, x, y):\n","    #     # y = self.selfattn_branch(self.ln5(y))\n","    #     x = x + self.resid_drop1(self.crossattn(x, y))\n","    #     x = x + self.mlp1(x)\n","    #     x = x + self.resid_drop2(self.selfattn(x))\n","    #     x = x + self.mlp2(x)\n","    #\n","    #     return x\n","\n","\n","'''\n","    Cross Attention GPT neural operator\n","    Trunck Net: geom\n","'''\n","\n","\n","class GNOT(nn.Module):\n","    def __init__(self,\n","                 trunk_size=2,\n","                 branch_sizes=None,  # 分支输入的维度 举例[1，3，3] [sdf,wind,track ...]\n","                 space_dim=2,\n","                 output_size=3,\n","                 n_layers=2,\n","                 n_hidden=64,\n","                 n_head=1,\n","                 n_experts=2,\n","                 n_inner=4,\n","                 mlp_layers=2,\n","                 attn_type='linear',\n","                 act='gelu',\n","                 ffn_dropout=0.0,\n","                 attn_dropout=0.0,\n","                 horiz_fourier_dim=0,\n","                 ):\n","        super(GNOT, self).__init__()\n","\n","        self.horiz_fourier_dim = horiz_fourier_dim\n","        self.trunk_size = trunk_size * (4 * horiz_fourier_dim + 3) if horiz_fourier_dim > 0 else trunk_size\n","        self.branch_sizes = [bsize * (4 * horiz_fourier_dim + 3) for bsize in\n","                             branch_sizes] if horiz_fourier_dim > 0 else branch_sizes\n","        self.n_inputs = len(self.branch_sizes)\n","        self.output_size = output_size\n","        self.space_dim = space_dim\n","        self.gpt_config = MoEGPTConfig(attn_type=attn_type, embd_pdrop=ffn_dropout, resid_pdrop=ffn_dropout,\n","                                       attn_pdrop=attn_dropout, n_embd=n_hidden, n_head=n_head, n_layer=n_layers,\n","                                       block_size=128, act=act, n_experts=n_experts, space_dim=space_dim,\n","                                       branch_sizes=branch_sizes, n_inputs=len(branch_sizes), n_inner=n_inner)\n","\n","        self.trunk_mlp = MLP(self.trunk_size, n_hidden, n_hidden, n_layers=mlp_layers, act=act)\n","        self.branch_mlps = nn.ModuleList(\n","            [MLP(bsize, n_hidden, n_hidden, n_layers=mlp_layers, act=act) for bsize in self.branch_sizes])\n","\n","        self.blocks = nn.Sequential(*[MIOECrossAttentionBlock(self.gpt_config) for _ in range(self.gpt_config.n_layer)])\n","\n","        self.out_mlp = MLP(n_hidden, n_hidden, output_size, n_layers=mlp_layers)\n","\n","        # self.apply(self._init_weights)\n","\n","        self.__name__ = 'MIOEGPT'\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            module.weight.data.normal_(mean=0.0, std=0.0002)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, query_points, conditions, deep=None):\n","\n","        pos = query_points[:, :, 0:self.space_dim]\n","\n","        # x = torch.cat([x, u_p.unsqueeze(1).repeat([1, x.shape[1], 1])], dim=-1)\n","\n","        # if self.horiz_fourier_dim > 0:\n","        #     x = horizontal_fourier_embedding(x, self.horiz_fourier_dim)\n","        #     z = horizontal_fourier_embedding(z, self.horiz_fourier_dim)\n","\n","        x = self.trunk_mlp(query_points)\n","        z = MultipleTensors([self.branch_mlps[i](conditions[i]) for i in range(self.n_inputs)])\n","        if deep == None:\n","            for block in self.blocks:\n","                x = block(x, z, pos)\n","            x_out = self.out_mlp(x)\n","        else:\n","            for i in range(deep):\n","                x = self.blocks[i](x, z, pos)\n","            x_out = self.out_mlp(x)\n","\n","        # x_out = torch.cat([x[i, :num] for i, num in enumerate(g.batch_num_nodes())], dim=0)\n","        return x_out\n","\n","\n"],"metadata":{"id":"gS0mLrw-YUeM","executionInfo":{"status":"ok","timestamp":1721014124798,"user_tz":-480,"elapsed":3627,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Args"],"metadata":{"id":"TA4DWJ3Qn60x"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","# -*- coding:utf-8 _*-\n","\n","import argparse\n","\n","\n","def get_args():\n","    parser = argparse.ArgumentParser(description='trackB')\n","    parser.add_argument('--dataset', type=str,\n","                        default='ns2d',\n","                        choices=['heat2d', 'ns2d', 'inductor2d', 'heatsink3d', 'ns2d_time', 'darcy2d', ])\n","    parser.add_argument('--train_dir', default='./Dataset/data', type=str)\n","    parser.add_argument('--test_dir', default='./Dataset/track_A', type=str)\n","    parser.add_argument('--wind_direction', default='z', type=str)\n","    parser.add_argument('--track', default='track_A', type=str)\n","    parser.add_argument('--train_log', default='train', type=str)\n","    parser.add_argument('--expand_data', type=int,default=0)\n","    parser.add_argument('--val_ratio', default=0.1, type=float)\n","    parser.add_argument('--p', default=0.5, type=float,help='p of using tracka')\n","\n","    parser.add_argument('--iter_per_epoch', default=250, type=int)\n","    parser.add_argument('--cat_area', type=int, default=1,\n","                        help='cat area or use condition')\n","    parser.add_argument('--use_transform', type=int, default=0)\n","    parser.add_argument('--component', type=str,\n","                        default='all', )\n","\n","    parser.add_argument('--seed', type=int, default=2024, metavar='Seed',\n","                        help='random seed (default: 1127802)')\n","\n","    parser.add_argument('--gpu', type=int, default=0, help='gpu id')\n","    parser.add_argument('--use-tb', type=int, default=1, help='whether use tensorboard')\n","    parser.add_argument('--comment', type=str, default=\"\", help=\"comment for the experiment\")\n","\n","    parser.add_argument('--train-num', type=str, default='all')\n","    parser.add_argument('--test-num', type=str, default='all')\n","\n","    parser.add_argument('--sort-data', type=int, default=0)\n","\n","    parser.add_argument('--normalize_x', type=str, default='unit',\n","                        choices=['none', 'minmax', 'unit'])\n","    parser.add_argument('--use-normalizer', type=str, default='unit',\n","                        choices=['none', 'minmax', 'unit', 'quantile'],\n","                        help=\"whether normalize y\")\n","\n","    parser.add_argument('--epochs', type=int, default=250, metavar='N',\n","                        help='number of epochs to train (default: 100)')\n","    parser.add_argument('--val_epochs', type=int, default=10,)\n","    parser.add_argument('--optimizer', type=str, default='AdamW', choices=['Adam', 'AdamW'])\n","\n","    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n","                        help='max learning rate (default: 0.001)')\n","    parser.add_argument('--weight-decay', type=float, default=5e-5\n","                        )\n","    parser.add_argument('--grad-clip', type=str, default=1000.0\n","                        )\n","    parser.add_argument('--batch-size', type=int, default=8, metavar='bsz',\n","                        help='input batch size for training (default: 8)')\n","    parser.add_argument('--val-batch-size', type=int, default=8, metavar='bsz',\n","                        help='input batch size for validation (default: 4)')\n","    parser.add_argument('--no-cuda', default=0,type=int,\n","                        help='disables CUDA training')\n","\n","\n","    parser.add_argument('--lr-method', type=str, default='cycle',\n","                        choices=['cycle', 'step', 'warmup'])\n","    parser.add_argument('--lr-step-size', type=int, default=20\n","                        )\n","    parser.add_argument('--warmup-epochs', type=int, default=50)\n","\n","    parser.add_argument('--loss-name', type=str, default='rel2',\n","                        choices=['rel2', 'rel1', 'l2', 'l1'])\n","    #### public model architecture parameters\n","\n","    parser.add_argument('--model-name', type=str, default='GNOT',\n","                        choices=['CGPT', 'GNOT', ])\n","    parser.add_argument('--n-hidden', type=int, default=32)\n","    parser.add_argument('--n-layers', type=int, default=16)\n","\n","    #### MLP parameters\n","\n","    # common\n","    parser.add_argument('--act', type=str, default='gelu', choices=['gelu', 'relu', 'tanh', 'sigmoid'])\n","    parser.add_argument('--loss', type=str, default='default', choices=['default','mse', 'l1', 'huber', 'smoothl1'])\n","    parser.add_argument('--model', type=str, default='GNOT')\n","    parser.add_argument('--n-head', type=int, default=4)\n","    parser.add_argument('--slice_num', type=int, default=64)\n","    parser.add_argument('--agent_num', type=int, default=32)\n","    parser.add_argument('--ffn-dropout', type=float, default=0.0, metavar='ffn_dropout',\n","                        help='dropout for the FFN in attention (default: 0.0)')\n","    parser.add_argument('--attn-dropout', type=float, default=0.0)\n","    parser.add_argument('--mlp-layers', type=int, default=3)\n","\n","    # Transformer\n","    # parser.add_argument('--subsampled-len',type=int, default=256)\n","    parser.add_argument('--attn-type', type=str, default='linear',\n","                        choices=['random', 'linear', 'gated', 'hydra', 'kernel'])\n","    parser.add_argument('--hfourier-dim', type=int, default=0)\n","\n","    # GNOT\n","    parser.add_argument('--n_experts', type=int, default=2)\n","    parser.add_argument('--input_dim', type=int, default=4)\n","    parser.add_argument('--output_dim', type=int, default=1)\n","    parser.add_argument('--space_dim', type=int, default=3)\n","\n","    parser.add_argument('--branch_sizes', nargs=\"*\", type=int, default=[3, 3])\n","    parser.add_argument('--n-inner', type=int, default=4)\n","\n","    return parser.parse_known_args()[0]\n"],"metadata":{"id":"KmeREhHyjUJP","executionInfo":{"status":"ok","timestamp":1721014124799,"user_tz":-480,"elapsed":27,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"UQxAwmUYnWkp"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","# -*- coding:utf-8 _*-\n","import os\n","import torch\n","import numpy as np\n","import operator\n","import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","import torch.special as ts\n","from torch import nn\n","\n","from scipy import interpolate\n","from functools import reduce\n","from pathlib import Path\n","from torch.nn import  MSELoss,L1Loss,HuberLoss,SmoothL1Loss\n","class LpLoss(nn.Module):\n","    def __init__(self, d=2, p=2, size_average=True, reduction='mean'):\n","        super(LpLoss, self).__init__()\n","        assert d > 0 and p > 0\n","        self.d = d\n","        self.p = p\n","        self.reduction = reduction\n","        self.size_average = size_average\n","\n","    def forward(self, x, y):\n","        num_examples = x.size(0)\n","        h = 1.0 / (x.size(1) - 1.0)\n","        all_norms = (h ** (self.d / self.p)) * torch.norm(\n","            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, dim=1\n","        )\n","        if self.reduction == 'mean':\n","            return all_norms.mean() if self.size_average else all_norms.sum()\n","        elif self.reduction == 'sum':\n","            return all_norms.sum()\n","        else:\n","            return all_norms\n","\n","def extract_condition(data):\n","\n","    keys_to_keep = {'vertices', 'pressure','area'}\n","\n","    filtered_data = [value for key, value in data.items() if key not in keys_to_keep]\n","\n","    return filtered_data\n","\n","\n","\n","\n","def get_model(args):\n","    trunk_size,  branch_sizes, output_size = args.input_dim, args.branch_sizes, args.output_dim\n","    output_size = args.output_dim\n","    agent_num=args.agent_num\n","    if args.model=='GNOT':\n","      # print(1)\n","      return GNOT(trunk_size=trunk_size, branch_sizes=branch_sizes, output_size=output_size, n_layers=args.n_layers,\n","                  n_hidden=args.n_hidden, n_head=args.n_head, attn_type=args.attn_type, ffn_dropout=args.ffn_dropout,\n","                  attn_dropout=args.attn_dropout, mlp_layers=args.mlp_layers, act=args.act,\n","                  horiz_fourier_dim=args.hfourier_dim, space_dim=args.space_dim, n_experts=args.n_experts,\n","                  n_inner=args.n_inner)\n","    else:\n","        # return GNOT_Agent(trunk_size=trunk_size, branch_sizes=branch_sizes, output_size=output_size, n_layers=args.n_layers,\n","        #             n_hidden=args.n_hidden, n_head=args.n_head, attn_type=args.attn_type, ffn_dropout=args.ffn_dropout,\n","        #             attn_dropout=args.attn_dropout, mlp_layers=args.mlp_layers, act=args.act,\n","        #             horiz_fourier_dim=args.hfourier_dim, space_dim=args.space_dim, n_experts=args.n_experts,\n","        #             n_inner=args.n_inner,agent_num=agent_num)\n","        print(1)\n","\n","def get_dataset(args):\n","    return CarDataset(\n","        Path(args.train_dir),\n","        Path(args.test_dir),\n","        expand_data=args.expand_data,\n","        track=args.track,\n","        wind_direction=args.wind_direction,\n","        val_ratio=args.val_ratio,\n","        use_transform=args.use_transform\n","\n","    )\n","\n","def get_loss(args):\n","    if args.loss=='default':\n","        return LpLoss()\n","    if args.loss=='mse':\n","        return MSELoss()\n","    if args.loss=='l1':\n","        return L1Loss()\n","    if args.loss=='huber':\n","        return HuberLoss()\n","    if args.loss=='smoothl1':\n","        return SmoothL1Loss()\n","\n","def get_seed(s, printout=True, cudnn=True):\n","    # rd.seed(s)\n","    os.environ['PYTHONHASHSEED'] = str(s)\n","    np.random.seed(s)\n","    # pd.core.common.random_state(s)\n","    # Torch\n","    torch.manual_seed(s)\n","    torch.cuda.manual_seed(s)\n","    if cudnn:\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(s)\n","\n","    message = f'''\n","    os.environ['PYTHONHASHSEED'] = str({s})\n","    numpy.random.seed({s})\n","    torch.manual_seed({s})\n","    torch.cuda.manual_seed({s})\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all({s})\n","    '''\n","    if printout:\n","        print(\"\\n\")\n","        print(f\"The following code snippets have been run.\")\n","        print(\"=\" * 50)\n","        print(message)\n","        print(\"=\" * 50)\n","\n","\n","def get_num_params(model):\n","    '''\n","    a single entry in cfloat and cdouble count as two parameters\n","    see https://github.com/pytorch/pytorch/issues/57518\n","    '''\n","    # model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    # num_params = 0\n","    # for p in model_parameters:\n","    #     # num_params += np.prod(p.size()+(2,) if p.is_complex() else p.size())\n","    #     num_params += p.numel() * (1 + p.is_complex())\n","    # return num_params\n","\n","    c = 0\n","    for p in list(model.parameters()):\n","        c += reduce(operator.mul, list(p.size() + (2,) if p.is_complex() else p.size()))  #### there is complex weight\n","    return c\n","\n","\n","### x: list of tensors\n","class MultipleTensors():\n","    def __init__(self, x):\n","        self.x = x\n","\n","    def to(self, device):\n","        self.x = [x_.to(device) for x_ in self.x]\n","        return self\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, item):\n","        return self.x[item]\n","\n","\n","# whether need to transpose\n","def plot_heatmap(\n","        x, y, z, path=None, vmin=None, vmax=None, cmap=None,\n","        title=\"\", xlabel=\"x\", ylabel=\"y\", show=False\n","):\n","    '''\n","    Plot heat map for a 3-dimension data\n","    '''\n","    plt.cla()\n","    # plt.figure()\n","    xx = np.linspace(np.min(x), np.max(x))\n","    yy = np.linspace(np.min(y), np.max(y))\n","    xx, yy = np.meshgrid(xx, yy)\n","\n","    vals = interpolate.griddata(np.array([x, y]).T, np.array(z),\n","                                (xx, yy), method='cubic')\n","    vals_0 = interpolate.griddata(np.array([x, y]).T, np.array(z),\n","                                  (xx, yy), method='nearest')\n","    vals[np.isnan(vals)] = vals_0[np.isnan(vals)]\n","\n","    if vmin is not None and vmax is not None:\n","        fig = plt.imshow(vals,\n","                         extent=[np.min(x), np.max(x), np.min(y), np.max(y)],\n","                         aspect=\"equal\", interpolation=\"bicubic\", cmap=cmap,\n","                         vmin=vmin, vmax=vmax, origin='lower')\n","    elif vmin is not None:\n","        fig = plt.imshow(vals,\n","                         extent=[np.min(x), np.max(x), np.min(y), np.max(y)],\n","                         aspect=\"equal\", interpolation=\"bicubic\", cmap=cmap,\n","                         vmin=vmin, origin='lower')\n","    else:\n","        fig = plt.imshow(vals,\n","                         extent=[np.min(x), np.max(x), np.min(y), np.max(y)], cmap=cmap,\n","                         aspect=\"equal\", interpolation=\"bicubic\", origin='lower')\n","    fig.axes.set_autoscale_on(False)\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.colorbar()\n","    if path:\n","        plt.savefig(path)\n","    if show:\n","        plt.show()\n","    plt.close()\n","\n","\n","import contextlib\n","\n","\n","class Interp1d(torch.autograd.Function):\n","    def __call__(self, x, y, xnew, out=None):\n","        return self.forward(x, y, xnew, out)\n","\n","    def forward(ctx, x, y, xnew, out=None):\n","        \"\"\"\n","        Linear 1D interpolation on the GPU for Pytorch.\n","        This function returns interpolated values of a set of 1-D functions at\n","        the desired query points `xnew`.\n","        This function is working similarly to Matlab™ or scipy functions with\n","        the `linear` interpolation mode on, except that it parallelises over\n","        any number of desired interpolation problems.\n","        The code will run on GPU if all the tensors provided are on a cuda\n","        device.\n","        Parameters\n","        ----------\n","        x : (N, ) or (D, N) Pytorch Tensor\n","            A 1-D or 2-D tensor of real values.\n","        y : (N,) or (D, N) Pytorch Tensor\n","            A 1-D or 2-D tensor of real values. The length of `y` along its\n","            last dimension must be the same as that of `x`\n","        xnew : (P,) or (D, P) Pytorch Tensor\n","            A 1-D or 2-D tensor of real values. `xnew` can only be 1-D if\n","            _both_ `x` and `y` are 1-D. Otherwise, its length along the first\n","            dimension must be the same as that of whichever `x` and `y` is 2-D.\n","        out : Pytorch Tensor, same shape as `xnew`\n","            Tensor for the output. If None: allocated automatically.\n","        \"\"\"\n","        # making the vectors at least 2D\n","        is_flat = {}\n","        require_grad = {}\n","        v = {}\n","        device = []\n","        eps = torch.finfo(y.dtype).eps\n","        for name, vec in {'x': x, 'y': y, 'xnew': xnew}.items():\n","            assert len(vec.shape) <= 2, 'interp1d: all inputs must be ' \\\n","                                        'at most 2-D.'\n","            if len(vec.shape) == 1:\n","                v[name] = vec[None, :]\n","            else:\n","                v[name] = vec\n","            is_flat[name] = v[name].shape[0] == 1\n","            require_grad[name] = vec.requires_grad\n","            device = list(set(device + [str(vec.device)]))\n","        assert len(device) == 1, 'All parameters must be on the same device.'\n","        device = device[0]\n","\n","        # Checking for the dimensions\n","        assert (v['x'].shape[1] == v['y'].shape[1]\n","                and (\n","                        v['x'].shape[0] == v['y'].shape[0]\n","                        or v['x'].shape[0] == 1\n","                        or v['y'].shape[0] == 1\n","                )\n","                ), (\"x and y must have the same number of columns, and either \"\n","                    \"the same number of row or one of them having only one \"\n","                    \"row.\")\n","\n","        reshaped_xnew = False\n","        if ((v['x'].shape[0] == 1) and (v['y'].shape[0] == 1)\n","                and (v['xnew'].shape[0] > 1)):\n","            # if there is only one row for both x and y, there is no need to\n","            # loop over the rows of xnew because they will all have to face the\n","            # same interpolation problem. We should just stack them together to\n","            # call interp1d and put them back in place afterwards.\n","            original_xnew_shape = v['xnew'].shape\n","            v['xnew'] = v['xnew'].contiguous().view(1, -1)\n","            reshaped_xnew = True\n","\n","        # identify the dimensions of output and check if the one provided is ok\n","        D = max(v['x'].shape[0], v['xnew'].shape[0])\n","        shape_ynew = (D, v['xnew'].shape[-1])\n","        if out is not None:\n","            if out.numel() != shape_ynew[0] * shape_ynew[1]:\n","                # The output provided is of incorrect shape.\n","                # Going for a new one\n","                out = None\n","            else:\n","                ynew = out.reshape(shape_ynew)\n","        if out is None:\n","            ynew = torch.zeros(*shape_ynew, device=device)\n","\n","        # moving everything to the desired device in case it was not there\n","        # already (not handling the case things do not fit entirely, user will\n","        # do it if required.)\n","        for name in v:\n","            v[name] = v[name].to(device)\n","\n","        # calling searchsorted on the x values.\n","        ind = ynew.long()\n","\n","        # expanding xnew to match the number of rows of x in case only one xnew is\n","        # provided\n","        if v['xnew'].shape[0] == 1:\n","            v['xnew'] = v['xnew'].expand(v['x'].shape[0], -1)\n","\n","        torch.searchsorted(v['x'].contiguous(),\n","                           v['xnew'].contiguous(), out=ind)\n","\n","        # the `-1` is because searchsorted looks for the index where the values\n","        # must be inserted to preserve order. And we want the index of the\n","        # preceeding value.\n","        ind -= 1\n","        # we clamp the index, because the number of intervals is x.shape-1,\n","        # and the left neighbour should hence be at most number of intervals\n","        # -1, i.e. number of columns in x -2\n","        ind = torch.clamp(ind, 0, v['x'].shape[1] - 1 - 1)\n","\n","        # helper function to select stuff according to the found indices.\n","        def sel(name):\n","            if is_flat[name]:\n","                return v[name].contiguous().view(-1)[ind]\n","            return torch.gather(v[name], 1, ind)\n","\n","        # activating gradient storing for everything now\n","        enable_grad = False\n","        saved_inputs = []\n","        for name in ['x', 'y', 'xnew']:\n","            if require_grad[name]:\n","                enable_grad = True\n","                saved_inputs += [v[name]]\n","            else:\n","                saved_inputs += [None, ]\n","        # assuming x are sorted in the dimension 1, computing the slopes for\n","        # the segments\n","        is_flat['slopes'] = is_flat['x']\n","        # now we have found the indices of the neighbors, we start building the\n","        # output. Hence, we start also activating gradient tracking\n","        with torch.enable_grad() if enable_grad else contextlib.suppress():\n","            v['slopes'] = (\n","                    (v['y'][:, 1:] - v['y'][:, :-1])\n","                    /\n","                    (eps + (v['x'][:, 1:] - v['x'][:, :-1]))\n","            )\n","\n","            # now build the linear interpolation\n","            ynew = sel('y') + sel('slopes') * (\n","                    v['xnew'] - sel('x'))\n","\n","            if reshaped_xnew:\n","                ynew = ynew.view(original_xnew_shape)\n","\n","        ctx.save_for_backward(ynew, *saved_inputs)\n","        return ynew\n","\n","    @staticmethod\n","    def backward(ctx, grad_out):\n","        inputs = ctx.saved_tensors[1:]\n","        gradients = torch.autograd.grad(\n","            ctx.saved_tensors[0],\n","            [i for i in inputs if i is not None],\n","            grad_out, retain_graph=True)\n","        result = [None, ] * 5\n","        pos = 0\n","        for index in range(len(inputs)):\n","            if inputs[index] is not None:\n","                result[index] = gradients[pos]\n","                pos += 1\n","        return (*result,)\n","\n","\n","class TorchQuantileTransformer():\n","    '''\n","    QuantileTransformer implemented by PyTorch\n","    '''\n","\n","    def __init__(\n","            self,\n","            output_distribution,\n","            references_,\n","            quantiles_,\n","            device=torch.device('cpu')\n","    ) -> None:\n","        self.quantiles_ = torch.Tensor(quantiles_).to(device)\n","        self.output_distribution = output_distribution\n","        self._norm_pdf_C = np.sqrt(2 * np.pi)\n","        self.references_ = torch.Tensor(references_).to(device)\n","        BOUNDS_THRESHOLD = 1e-7\n","        self.clip_min = self.norm_ppf(torch.Tensor([BOUNDS_THRESHOLD - np.spacing(1)]))\n","        self.clip_max = self.norm_ppf(torch.Tensor([1 - (BOUNDS_THRESHOLD - np.spacing(1))]))\n","\n","    def norm_pdf(self, x):\n","        return torch.exp(-x ** 2 / 2.0) / self._norm_pdf_C\n","\n","    @staticmethod\n","    def norm_cdf(x):\n","        return ts.ndtr(x)\n","\n","    @staticmethod\n","    def norm_ppf(x):\n","        return ts.ndtri(x)\n","\n","    def transform_col(self, X_col, quantiles, inverse):\n","        BOUNDS_THRESHOLD = 1e-7\n","        output_distribution = self.output_distribution\n","\n","        if not inverse:\n","            lower_bound_x = quantiles[0]\n","            upper_bound_x = quantiles[-1]\n","            lower_bound_y = 0\n","            upper_bound_y = 1\n","        else:\n","            lower_bound_x = 0\n","            upper_bound_x = 1\n","            lower_bound_y = quantiles[0]\n","            upper_bound_y = quantiles[-1]\n","            # for inverse transform, match a uniform distribution\n","            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n","                if output_distribution == \"normal\":\n","                    X_col = self.norm_cdf(X_col)\n","                # else output distribution is already a uniform distribution\n","\n","        # find index for lower and higher bounds\n","        with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n","            if output_distribution == \"normal\":\n","                lower_bounds_idx = X_col - BOUNDS_THRESHOLD < lower_bound_x\n","                upper_bounds_idx = X_col + BOUNDS_THRESHOLD > upper_bound_x\n","            if output_distribution == \"uniform\":\n","                lower_bounds_idx = X_col == lower_bound_x\n","                upper_bounds_idx = X_col == upper_bound_x\n","\n","        isfinite_mask = ~torch.isnan(X_col)\n","        X_col_finite = X_col[isfinite_mask]\n","        torch_interp = Interp1d()\n","        X_col_out = X_col.clone()\n","        if not inverse:\n","            # Interpolate in one direction and in the other and take the\n","            # mean. This is in case of repeated values in the features\n","            # and hence repeated quantiles\n","            #\n","            # If we don't do this, only one extreme of the duplicated is\n","            # used (the upper when we do ascending, and the\n","            # lower for descending). We take the mean of these two\n","            X_col_out[isfinite_mask] = 0.5 * (\n","                    torch_interp(quantiles, self.references_, X_col_finite)\n","                    - torch_interp(-torch.flip(quantiles, [0]), -torch.flip(self.references_, [0]), -X_col_finite)\n","            )\n","        else:\n","            X_col_out[isfinite_mask] = torch_interp(self.references_, quantiles, X_col_finite)\n","\n","        X_col_out[upper_bounds_idx] = upper_bound_y\n","        X_col_out[lower_bounds_idx] = lower_bound_y\n","        # for forward transform, match the output distribution\n","        if not inverse:\n","            with np.errstate(invalid=\"ignore\"):  # hide NaN comparison warnings\n","                if output_distribution == \"normal\":\n","                    X_col_out = self.norm_ppf(X_col_out)\n","                    # find the value to clip the data to avoid mapping to\n","                    # infinity. Clip such that the inverse transform will be\n","                    # consistent\n","                    X_col_out = torch.clip(X_col_out, self.clip_min, self.clip_max)\n","                # else output distribution is uniform and the ppf is the\n","                # identity function so we let X_col unchanged\n","\n","        return X_col_out\n","\n","    def transform(self, X, inverse=True, component='all'):\n","        X_out = torch.zeros_like(X, requires_grad=False)\n","        for feature_idx in range(X.shape[1]):\n","            X_out[:, feature_idx] = self.transform_col(\n","                X[:, feature_idx], self.quantiles_[:, feature_idx], inverse\n","            )\n","        return X_out\n","\n","    def to(self, device):\n","        self.quantiles_ = self.quantiles_.to(device)\n","        self.references_ = self.references_.to(device)\n","        return self\n","\n","\n","'''\n","    Simple normalization layer\n","'''\n","\n","\n","class UnitTransformer():\n","    def __init__(self, X):\n","        self.mean = X.mean(dim=0, keepdim=True)\n","        self.std = X.std(dim=0, keepdim=True) + 1e-8\n","\n","    def to(self, device):\n","        self.mean = self.mean.to(device)\n","        self.std = self.std.to(device)\n","        return self\n","\n","    def transform(self, X, inverse=True, component='all'):\n","        if component == 'all' or 'all-reduce':\n","            if inverse:\n","                orig_shape = X.shape\n","                return (X * (self.std - 1e-8) + self.mean).view(orig_shape)\n","            else:\n","                return (X - self.mean) / self.std\n","        else:\n","            if inverse:\n","                orig_shape = X.shape\n","                return (X * (self.std[:, component] - 1e-8) + self.mean[:, component]).view(orig_shape)\n","            else:\n","                return (X - self.mean[:, component]) / self.std[:, component]\n","\n","\n","'''\n","    Simple pointwise normalization layer, all data must contain the same length, used only for FNO datasets\n","    X: B, N, C\n","'''\n","\n","\n","class PointWiseUnitTransformer():\n","    def __init__(self, X):\n","        self.mean = X.mean(dim=0, keepdim=False)\n","        self.std = X.std(dim=0, keepdim=False) + 1e-8\n","\n","    def to(self, device):\n","        self.mean = self.mean.to(device)\n","        self.std = self.std.to(device)\n","        return self\n","\n","    def transform(self, X, inverse=True, component='all'):\n","        if component == 'all' or 'all-reduce':\n","            if inverse:\n","                orig_shape = X.shape\n","                X = X.view(-1, self.mean.shape[0], self.mean.shape[1])  ### align shape for flat tensor\n","                return (X * (self.std - 1e-8) + self.mean).view(orig_shape)\n","            else:\n","                return (X - self.mean) / self.std\n","        else:\n","            if inverse:\n","                orig_shape = X.shape\n","                X = X.view(-1, self.mean.shape[0], self.mean.shape[1])\n","                return (X * (self.std[:, component] - 1e-8) + self.mean[:, component]).view(orig_shape)\n","            else:\n","                return (X - self.mean[:, component]) / self.std[:, component]\n","\n","\n","'''\n","    x: B, N (not necessary sorted)\n","    y: B, N, C (not necessary sorted)\n","    xnew: B, N (sorted)\n","'''\n","\n","\n","def binterp1d(x, y, xnew, eps=1e-9):\n","    x_, x_indice = torch.sort(x, dim=-1)\n","    y_ = y[torch.arange(x_.shape[0]).unsqueeze(1), x_indice]\n","\n","    x_, y_, xnew = x_.contiguous(), y_.contiguous(), xnew.contiguous()\n","\n","    ind = torch.searchsorted(x_, xnew)\n","    ind -= 1\n","    ind = torch.clamp(ind, 0, x_.shape[1] - 1 - 1)\n","    ind = ind.unsqueeze(-1).repeat([1, 1, y_.shape[-1]])\n","    x_ = x_.unsqueeze(-1).repeat([1, 1, y_.shape[-1]])\n","\n","    slopes = ((y_[:, 1:] - y_[:, :-1]) / (eps + (x_[:, 1:] - x_[:, :-1])))\n","\n","    y_sel = torch.gather(y_, 1, ind)\n","    x_sel = torch.gather(x_, 1, ind)\n","    slopes_sel = torch.gather(slopes, 1, ind)\n","\n","    ynew = y_sel + slopes_sel * (xnew.unsqueeze(-1) - x_sel)\n","\n","    return ynew\n"],"metadata":{"id":"hFW1jQ1VnUt0","executionInfo":{"status":"ok","timestamp":1721014124800,"user_tz":-480,"elapsed":26,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import math\n","import torch\n","\n","from torch import Tensor\n","from typing import List, Optional\n","from torch.optim.optimizer import Optimizer\n","\n","\n","def adam(params: List[Tensor],\n","         grads: List[Tensor],\n","         exp_avgs: List[Tensor],\n","         exp_avg_sqs: List[Tensor],\n","         max_exp_avg_sqs: List[Tensor],\n","         state_steps: List[int],\n","         *,\n","         amsgrad: bool,\n","         beta1: float,\n","         beta2: float,\n","         lr: float,\n","         weight_decay: float,\n","         eps: float):\n","    r\"\"\"Functional API that performs Adam algorithm computation.\n","    See :class:`~torch.optim.Adam` for details.\n","    \"\"\"\n","\n","    for i, param in enumerate(params):\n","\n","        grad = grads[i]\n","        exp_avg = exp_avgs[i]\n","        exp_avg_sq = exp_avg_sqs[i]\n","        step = state_steps[i]\n","\n","        bias_correction1 = 1 - beta1 ** step\n","        bias_correction2 = 1 - beta2 ** step\n","\n","        if weight_decay != 0:\n","            grad = grad.add(param, alpha=weight_decay)\n","\n","        # Decay the first and second moment running average coefficient\n","        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n","        if amsgrad:\n","            # Maintains the maximum of all 2nd moment running avg. till now\n","            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n","            # Use the max. for normalizing running avg. of gradient\n","            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","        else:\n","            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","\n","        step_size = lr / bias_correction1\n","\n","        param.addcdiv_(exp_avg, denom, value=-step_size)\n","\n","\n","class Adam(Optimizer):\n","    r\"\"\"Implements Adam algorithm.\n","    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n","    The implementation of the L2 penalty follows changes proposed in\n","    `Decoupled Weight Decay Regularization`_.\n","    Args:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","            (default: False)\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _Decoupled Weight Decay Regularization:\n","        https://arxiv.org/abs/1711.05101\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        if not 0.0 <= weight_decay:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(Adam, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(Adam, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Args:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            with torch.enable_grad():\n","                loss = closure()\n","\n","        for group in self.param_groups:\n","            params_with_grad = []\n","            grads = []\n","            exp_avgs = []\n","            exp_avg_sqs = []\n","            max_exp_avg_sqs = []\n","            state_steps = []\n","            beta1, beta2 = group['betas']\n","\n","            for p in group['params']:\n","                if p.grad is not None:\n","                    params_with_grad.append(p)\n","                    if p.grad.is_sparse:\n","                        raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","                    grads.append(p.grad)\n","\n","                    state = self.state[p]\n","                    # Lazy state initialization\n","                    if len(state) == 0:\n","                        state['step'] = 0\n","                        # Exponential moving average of gradient values\n","                        state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n","                        # Exponential moving average of squared gradient values\n","                        state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n","                        if group['amsgrad']:\n","                            # Maintains max of all exp. moving avg. of sq. grad. values\n","                            state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n","\n","                    exp_avgs.append(state['exp_avg'])\n","                    exp_avg_sqs.append(state['exp_avg_sq'])\n","\n","                    if group['amsgrad']:\n","                        max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n","\n","                    # update the steps for each param group update\n","                    state['step'] += 1\n","                    # record the step after step update\n","                    state_steps.append(state['step'])\n","\n","            adam(params_with_grad,\n","                 grads,\n","                 exp_avgs,\n","                 exp_avg_sqs,\n","                 max_exp_avg_sqs,\n","                 state_steps,\n","                 amsgrad=group['amsgrad'],\n","                 beta1=beta1,\n","                 beta2=beta2,\n","                 lr=group['lr'],\n","                 weight_decay=group['weight_decay'],\n","                 eps=group['eps'])\n","        return loss\n","\n","\n","\n","\n","\n","def adamw(params: List[Tensor],\n","          grads: List[Tensor],\n","          exp_avgs: List[Tensor],\n","          exp_avg_sqs: List[Tensor],\n","          max_exp_avg_sqs: List[Tensor],\n","          state_steps: List[int],\n","          *,\n","          amsgrad: bool,\n","          beta1: float,\n","          beta2: float,\n","          lr: float,\n","          weight_decay: float,\n","          eps: float):\n","    r\"\"\"Functional API that performs AdamW algorithm computation.\n","\n","    See :class:`~torch.optim.AdamW` for details.\n","    \"\"\"\n","    for i, param in enumerate(params):\n","        grad = grads[i]\n","        exp_avg = exp_avgs[i]\n","        exp_avg_sq = exp_avg_sqs[i]\n","        step = state_steps[i]\n","\n","        # Perform stepweight decay\n","        param.mul_(1 - lr * weight_decay)\n","\n","        bias_correction1 = 1 - beta1 ** step\n","        bias_correction2 = 1 - beta2 ** step\n","\n","        # Decay the first and second moment running average coefficient\n","        exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","        exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n","        if amsgrad:\n","            # Maintains the maximum of all 2nd moment running avg. till now\n","            torch.maximum(max_exp_avg_sqs[i], exp_avg_sq, out=max_exp_avg_sqs[i])\n","            # Use the max. for normalizing running avg. of gradient\n","            denom = (max_exp_avg_sqs[i].sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","        else:\n","            denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)\n","\n","        step_size = lr / bias_correction1\n","\n","        param.addcdiv_(exp_avg, denom, value=-step_size)\n","\n","\n","\n","\n","class Adamw(Optimizer):\n","    r\"\"\"Implements AdamW algorithm.\n","\n","    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n","    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n","\n","    Args:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","            (default: False)\n","\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _Decoupled Weight Decay Regularization:\n","        https://arxiv.org/abs/1711.05101\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=1e-2, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n","        if not 0.0 <= weight_decay:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        defaults = dict(lr=lr, betas=betas, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(Adamw, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(Adamw, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    @torch.no_grad()\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Args:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            with torch.enable_grad():\n","                loss = closure()\n","\n","        for group in self.param_groups:\n","            params_with_grad = []\n","            grads = []\n","            exp_avgs = []\n","            exp_avg_sqs = []\n","            state_sums = []\n","            max_exp_avg_sqs = []\n","            state_steps = []\n","            amsgrad = group['amsgrad']\n","            beta1, beta2 = group['betas']\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                params_with_grad.append(p)\n","                if p.grad.is_sparse:\n","                    raise RuntimeError('AdamW does not support sparse gradients')\n","                grads.append(p.grad)\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n","                    if amsgrad:\n","                        # Maintains max of all exp. moving avg. of sq. grad. values\n","                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n","\n","                exp_avgs.append(state['exp_avg'])\n","                exp_avg_sqs.append(state['exp_avg_sq'])\n","\n","                if amsgrad:\n","                    max_exp_avg_sqs.append(state['max_exp_avg_sq'])\n","\n","                # update the steps for each param group update\n","                state['step'] += 1\n","                # record the step after step update\n","                state_steps.append(state['step'])\n","\n","            adamw(params_with_grad,\n","                    grads,\n","                    exp_avgs,\n","                    exp_avg_sqs,\n","                    max_exp_avg_sqs,\n","                    state_steps,\n","                    amsgrad=amsgrad,\n","                    beta1=beta1,\n","                    beta2=beta2,\n","                    lr=group['lr'],\n","                    weight_decay=group['weight_decay'],\n","                    eps=group['eps'])\n","\n","        return loss"],"metadata":{"id":"-nHl5bmAk8-L","executionInfo":{"status":"ok","timestamp":1721014124800,"user_tz":-480,"elapsed":25,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Dataloader\n"],"metadata":{"id":"SjjX6nDgnKwI"}},{"cell_type":"code","source":["# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","# Code is heavily based on paper \"Geometry-Informed Neural Operator for Large-Scale 3D PDEs\", we use paddle to reproduce the results of the paper\n","\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class BaseDataModule:\n","\n","    @property\n","    def train_dataset(self) -> Dataset:\n","        raise NotImplementedError\n","\n","    @property\n","    def val_dataset(self) -> Dataset:\n","        raise NotImplementedError\n","\n","    @property\n","    def test_dataset(self) -> Dataset:\n","        raise NotImplementedError\n","\n","    def train_dataloader(self, **kwargs) -> DataLoader:\n","        collate_fn = getattr(self, 'collate_fn', None)\n","        return DataLoader(self.train_data, collate_fn=\n","        collate_fn, **kwargs)\n","\n","    def val_dataloader(self, **kwargs) -> DataLoader:\n","        collate_fn = getattr(self, 'collate_fn', None)\n","        return DataLoader(self.val_data, collate_fn=\n","        collate_fn, **kwargs)\n","\n","    def test_dataloader(self, **kwargs) -> DataLoader:\n","        collate_fn = getattr(self, 'collate_fn', None)\n","        return DataLoader(self.test_data, collate_fn=\n","        collate_fn, **kwargs)\n","\n","import torch\n","import open3d as o3d\n","import numpy as np\n","import random\n","import os\n","from pathlib import Path\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","\n","class AddGaussianNoiseWithProbability(object):\n","    def __init__(self, mean=0.0, std=1.0, probability=0.4):\n","        self.mean = mean\n","        self.std = std\n","        self.probability = probability\n","\n","    def __call__(self, tensor):\n","        if random.random() < self.probability:\n","            noise = torch.randn(tensor.size()) * self.std + self.mean\n","            return tensor + noise\n","        else:\n","            return tensor\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + '(mean={0}, std={1}, probability={2})'.format(self.mean, self.std,\n","                                                                                       self.probability)\n","\n","class DictDataset(Dataset):\n","    def __init__(self, data_dict: dict):\n","        self.data_dict = data_dict\n","        for k, v in data_dict.items():\n","            assert len(v) == len(\n","                data_dict[list(data_dict.keys())[0]]\n","            ), \"All data must have the same length\"\n","\n","    def __getitem__(self, index):\n","        return {k: v[index] for k, v in self.data_dict.items()}\n","\n","    def __len__(self):\n","        return len(self.data_dict[list(self.data_dict.keys())[0]])\n","\n","\n","class DictDatasetWithConstant(DictDataset):\n","    def __init__(self, data_dict: dict, constant_dict: dict,use_transform=False):\n","        super().__init__(data_dict)\n","        self.constant_dict = constant_dict\n","        self.use_transform=use_transform\n","        self.transform = AddGaussianNoiseWithProbability(0.0, 0.05)\n","    def __getitem__(self, index):\n","        return_dict = {k: v[index] for k, v in self.data_dict.items()}\n","        return_dict.update(self.constant_dict)\n","        if self.use_transform:\n","            return_dict['vertices'] = self.transform(return_dict['vertices'])\n","        return return_dict\n","\n","\n","class CarDataset(BaseDataModule):\n","    def __init__(\n","            self,\n","            train_data_dir,\n","            test_data_dir,\n","            n_train: int = 500,\n","            n_test: int = 5,\n","            val_ratio=0.1,\n","            expand_data=False,\n","            track='track_A',\n","            wind_direction='z',use_transform=False):\n","        BaseDataModule.__init__(self)\n","\n","        valid_train_indices = self.load_valid_mesh_indices(train_data_dir)\n","        valid_test_indices = self.load_valid_mesh_indices(test_data_dir)\n","        random.shuffle(valid_train_indices)\n","        val_num = int(n_train * val_ratio)\n","        valid_val_indices = valid_train_indices[:val_num]\n","        valid_train_indices = valid_train_indices[val_num:]\n","        self.all_train_mesh_path = [self.get_mesh_path_train(train_data_dir, i) for i in valid_train_indices]\n","        self.all_val_mesh_path = [self.get_mesh_path_train(train_data_dir, i) for i in valid_val_indices]\n","        self.all_test_mesh_path = [self.get_mesh_path_test(test_data_dir, i) for i in valid_test_indices]\n","\n","        all_train_vertices = torch.stack(\n","            [\n","                torch.tensor(self.vertices_from_mesh(i)) for i in self.all_train_mesh_path\n","            ]\n","        )\n","        all_val_vertices = torch.stack(\n","            [\n","                torch.tensor(self.vertices_from_mesh(i)) for i in self.all_val_mesh_path\n","            ]\n","        )\n","        all_test_vertices = torch.stack(\n","            [\n","                torch.tensor(self.vertices_from_mesh(i)) for i in self.all_test_mesh_path\n","            ]\n","        )\n","        all_train_press = torch.stack(\n","            [\n","                torch.tensor(\n","                    self.load_pressure(train_data_dir, meshidx), dtype=torch.float32\n","                )\n","                for meshidx in valid_train_indices\n","            ]\n","        )\n","        all_val_press = torch.stack(\n","            [\n","                torch.tensor(\n","                    self.load_pressure(train_data_dir, meshidx), dtype=torch.float32\n","                )\n","                for meshidx in valid_val_indices\n","            ]\n","        )\n","        all_test_press = torch.stack(\n","            [\n","                torch.tensor(\n","                    self.load_pressure(test_data_dir, meshidx), dtype=torch.float32\n","                )\n","                for meshidx in valid_test_indices\n","            ]\n","        )\n","        if track == 'track_A':\n","            track_token = torch.tensor([1, 0, 0], dtype=torch.float32).unsqueeze(0)\n","        else:\n","            track_token = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n","        if wind_direction == 'x':\n","            wind_token = torch.tensor([1, 0, 0], dtype=torch.float32).unsqueeze(0)\n","        elif wind_direction == 'z':\n","            wind_token = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n","        if expand_data:\n","            all_train_vertices = torch.cat([all_train_vertices, all_train_vertices], dim=0)\n","            all_train_press = torch.cat([all_train_press, all_train_press], dim=0)\n","        print(f'use {len(all_train_press)} training data!')\n","        self._train_data = DictDatasetWithConstant(\n","            {\"vertices\": all_train_vertices,\n","             \"pressure\": all_train_press},\n","            {\"track\": track_token,\n","             'wind_direction': wind_token},\n","            use_transform\n","        )\n","        self._val_data = DictDatasetWithConstant(\n","            {\"vertices\": all_val_vertices,\n","             \"pressure\": all_val_press},\n","            {\"track\": track_token,\n","             'wind_direction': wind_token},\n","        )\n","        self._test_data = DictDatasetWithConstant(\n","            {\"vertices\": all_test_vertices,\n","             \"pressure\": all_test_press},\n","            {\"track\": track_token,\n","             'wind_direction': wind_token},\n","        )\n","\n","    @property\n","    def train_data(self):\n","        return self._train_data\n","\n","    @property\n","    def test_data(self):\n","        return self._test_data\n","\n","    @property\n","    def val_data(self):\n","        return self._val_data\n","\n","    def load_pressure(self, data_dir: Path, mesh_index: int) -> np.ndarray:\n","        press_path = self.get_pressure_data_path(data_dir, mesh_index)\n","        assert press_path.exists(), \"Pressure data does not exist\"\n","        press = np.load(press_path).reshape((-1,)).astype(np.float32)\n","        press = np.concatenate((press[0:16], press[112:]), axis=0)\n","        return press\n","\n","    def get_mesh_path(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / (\"mesh_\" + str(mesh_ind).zfill(3) + \".ply\")\n","\n","    def get_mesh_path_train(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'Feature' / (\"mesh_\" + str(mesh_ind).zfill(3) + \".ply\")\n","\n","    def get_mesh_path_test(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'Inference' / (\"mesh_\" + str(mesh_ind).zfill(3) + \".ply\")\n","\n","    def get_pressure_data_path(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'Label' / (\"press_\" + str(mesh_ind).zfill(3) + \".npy\")\n","\n","    def load_mesh(self, mesh_path: Path) -> o3d.t.geometry.TriangleMesh:\n","        assert mesh_path.exists(), \"Mesh path does not exist\"\n","        mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n","        mesh = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n","        return mesh\n","\n","    def vertices_from_mesh(self, mesh_path: Path):\n","        mesh = self.load_mesh(mesh_path)\n","        vertices = mesh.vertex.positions.numpy()\n","        return vertices\n","\n","    def load_valid_mesh_indices(\n","            self, data_dir, filename=\"watertight_meshes.txt\"\n","    ):\n","        with open(data_dir / filename, \"r\") as fp:\n","            mesh_ind = fp.read().split(\"\\n\")\n","            mesh_ind = [int(a) for a in mesh_ind if a.strip()]\n","        return mesh_ind\n","\n","\n","class CarDataset_trackB(Dataset):\n","    def __init__(\n","            self,\n","            train_mesh_dir,\n","            train_centroid_dir,\n","            test_mesh_dir,\n","            n_train: int = 500,\n","            n_test: int = 5,\n","            val_ratio=0.1,\n","            mode='train',\n","            track='track_B',\n","            wind_direction='x',\n","            valid_val_indices=None):\n","        Dataset.__init__(self)\n","\n","        valid_train_indices = self.get_all_mesh_indices(train_mesh_dir)\n","        # valid_test_indices = self.get_all_mesh_indices(test_mesh_dir)\n","        random.shuffle(valid_train_indices)\n","        val_num = int(n_train * val_ratio)\n","        if valid_val_indices is not None:\n","            valid_val_indices = valid_val_indices\n","        else:\n","            valid_val_indices = valid_train_indices[:val_num]\n","        valid_train_indices = valid_train_indices[val_num:]\n","        self.mode = mode\n","        self.valid_val_indices = valid_val_indices\n","        self.all_train_mesh_path = [self.get_mesh_path_train(train_mesh_dir, i) for i in valid_train_indices]\n","        self.all_train_press_path = [self.get_pressure_data_path(train_centroid_dir, i) for i in valid_train_indices]\n","        self.all_val_mesh_path = [self.get_mesh_path_train(train_mesh_dir, i) for i in valid_val_indices]\n","        self.all_val_press_path = [self.get_pressure_data_path(train_centroid_dir, i) for i in valid_val_indices]\n","        self.all_test_mesh_path = [self.get_mesh_path_test(test_mesh_dir, i) for i in range(1, 51)]\n","        self.all_train_centroid_path = [self.get_centroid_data_path(train_centroid_dir, i) for i in valid_train_indices]\n","        self.all_val_centroid_path = [self.get_centroid_data_path(train_centroid_dir, i) for i in valid_val_indices]\n","        self.all_test_centroid_path = [self.get_centroid_data_path_test(test_mesh_dir, i) for i in range(1, 51)]\n","        self.all_train_area_path = [self.get_area_data_path(train_centroid_dir, i) for i in valid_train_indices]\n","        self.all_val_area_path = [self.get_area_data_path(train_centroid_dir, i) for i in valid_val_indices]\n","        self.all_test_area_path = [self.get_area_data_path_test(test_mesh_dir, i) for i in range(1, 51)]\n","\n","        if track == 'track_A':\n","            self.track_token = torch.tensor([1, 0, 0], dtype=torch.float32).unsqueeze(0)\n","        else:\n","            self.track_token = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n","        if wind_direction == 'x':\n","            self.wind_token = torch.tensor([1, 0, 0], dtype=torch.float32).unsqueeze(0)\n","        elif wind_direction == 'z':\n","            self.wind_token = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(0)\n","\n","        print(f'use {len(self.all_train_mesh_path)} training data!')\n","\n","    def __getitem__(self, idx):\n","        if self.mode == 'train':\n","            data_dict = {\n","                'vertices': self.load_centroid(self.all_train_centroid_path[idx]),\n","                'pressure': self.load_pressure(self.all_train_press_path[idx]),\n","                'area': self.load_area(self.all_train_area_path[idx]),\n","                'track': self.track_token,\n","                'wind_direction': self.wind_token,\n","\n","            }\n","        elif self.mode == 'val':\n","            data_dict = {\n","                'vertices': self.load_centroid(self.all_val_centroid_path[idx]),\n","                'pressure': self.load_pressure(self.all_val_press_path[idx]),\n","                'area': self.load_area(self.all_val_area_path[idx]),\n","                'track': self.track_token,\n","                'wind_direction': self.wind_token,\n","\n","            }\n","        else:\n","            data_dict = {\n","                'vertices': self.load_centroid(self.all_test_centroid_path[idx]),\n","                # 'pressure':self.load_pressure(self.all_te_press_path[idx]),\n","                'area': self.load_area(self.all_test_area_path[idx]),\n","                'track': self.track_token,\n","                'wind_direction': self.wind_token,\n","\n","            }\n","        return data_dict\n","\n","    def __len__(self):\n","        if self.mode == 'train':\n","            return len(self.all_train_mesh_path)\n","        elif self.mode == 'val':\n","            return len(self.all_val_mesh_path)\n","        else:\n","            return len(self.all_test_mesh_path)\n","\n","    def load_pressure(self, data_path: Path) -> np.ndarray:\n","        # press_path = self.get_pressure_data_path(data_dir, mesh_index)\n","        assert data_path.exists(), \"Pressure data does not exist\"\n","        press = np.load(data_path).reshape((-1,)).astype(np.float32)\n","        # press = np.concatenate((press[0:16], press[112:]), axis=0)\n","        return press\n","\n","    def load_centroid(self, data_path: Path) -> np.ndarray:\n","        assert data_path.exists(), \"Pressure data does not exist\"\n","        centroid = np.load(data_path).astype(np.float32)\n","        return centroid\n","\n","    def load_area(self, data_path):\n","        assert data_path.exists(), \"area data does not exist\"\n","        area = np.load(data_path).reshape((-1,)).astype(np.float32)\n","\n","        return area\n","\n","    def get_mesh_path(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / (\"mesh_\" + str(mesh_ind).zfill(4) + \".ply\")\n","\n","    def get_mesh_path_train(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'train_mesh_0603' / (\"mesh_\" + str(mesh_ind).zfill(4) + \".ply\")\n","\n","    def get_mesh_path_test(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'Inference' / (\"mesh_\" + str(mesh_ind) + \".ply\")\n","\n","    def get_pressure_data_path(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / (\"press_\" + str(mesh_ind).zfill(4) + \".npy\")\n","\n","    def get_area_data_path(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / (\"area_\" + str(mesh_ind).zfill(4) + \".npy\")\n","\n","    def get_area_data_path_test(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'Auxiliary' / (\"area_\" + str(mesh_ind) + \".npy\")\n","\n","    def get_centroid_data_path(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / (\"centroid_\" + str(mesh_ind).zfill(4) + \".npy\")\n","\n","    def get_centroid_data_path_test(self, data_dir: Path, mesh_ind: int) -> Path:\n","        return data_dir / 'Inference' / (\"centroid_\" + str(mesh_ind) + \".npy\")\n","\n","    def load_mesh(self, mesh_path: Path) -> o3d.t.geometry.TriangleMesh:\n","        assert mesh_path.exists(), \"Mesh path does not exist\"\n","        mesh = o3d.io.read_triangle_mesh(str(mesh_path))\n","        mesh = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n","        return mesh\n","\n","    def vertices_from_mesh(self, mesh_path: Path):\n","        mesh = self.load_mesh(mesh_path)\n","        vertices = mesh.vertex.positions.numpy()\n","        return vertices\n","\n","    def get_all_mesh_indices(\n","            self, mesh_data_dir\n","    ):\n","        mesh_ind = []\n","        all_mesh_files = os.listdir(str(mesh_data_dir))\n","        all_mesh_files.sort()\n","        for mesh_file_name in all_mesh_files:\n","            mesh_idx = mesh_file_name.split('_')[1].split('.')[0]\n","            mesh_ind.append(mesh_idx)\n","        return mesh_ind\n","\n","    def get_valid_val_indices(self):\n","        return self.valid_val_indices\n","\n","\n","\n"],"metadata":{"id":"xVtHRKMfkbhK","executionInfo":{"status":"ok","timestamp":1721014127184,"user_tz":-480,"elapsed":2409,"user":{"displayName":"知否","userId":"12758421836332615961"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["#Test"],"metadata":{"id":"Ktwbrdh7m_gv"}},{"cell_type":"markdown","source":["wget导入权重"],"metadata":{"id":"9mssn11Uyhku"}},{"cell_type":"code","source":["!wget  --no-check-certificate 'https://docs.google.com/uc?export=download&id=1caLwP6aQZqckOZTPWtAeyjXevju_x2rw' -O best_trackc.pth\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XdIpBndZrRLG","executionInfo":{"status":"ok","timestamp":1721014132305,"user_tz":-480,"elapsed":5133,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"c374644a-6eae-4457-960f-f16dc5d6d3ad"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-07-15 03:28:46--  https://docs.google.com/uc?export=download&id=1caLwP6aQZqckOZTPWtAeyjXevju_x2rw\n","Resolving docs.google.com (docs.google.com)... 74.125.143.101, 74.125.143.138, 74.125.143.100, ...\n","Connecting to docs.google.com (docs.google.com)|74.125.143.101|:443... connected.\n","HTTP request sent, awaiting response... 303 See Other\n","Location: https://drive.usercontent.google.com/download?id=1caLwP6aQZqckOZTPWtAeyjXevju_x2rw&export=download [following]\n","--2024-07-15 03:28:46--  https://drive.usercontent.google.com/download?id=1caLwP6aQZqckOZTPWtAeyjXevju_x2rw&export=download\n","Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.96.132, 2a00:1450:4013:c06::84\n","Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.96.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4339491 (4.1M) [application/octet-stream]\n","Saving to: ‘best_trackc.pth’\n","\n","best_trackc.pth     100%[===================>]   4.14M  --.-KB/s    in 0.1s    \n","\n","2024-07-15 03:28:51 (27.8 MB/s) - ‘best_trackc.pth’ saved [4339491/4339491]\n","\n"]}]},{"cell_type":"code","source":["#!/usr/bin/env python\n","# -*- coding:utf-8 _*-\n","import sys\n","import os\n","\n","sys.path.append('../..')\n","sys.path.append('..')\n","\n","import re\n","import time\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import warnings\n","import random\n","import torch\n","\n","\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","\n","from tqdm import tqdm\n","\n","'''\n","    A general code framework for training neural operator on irregular domains\n","'''\n","\n","EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n","                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n","\n","\n","def extract_numbers(s):\n","    return [int(digit) for digit in re.findall(r'\\d+', s)]\n","\n","\n","def write_to_vtk(p, point_data_pos=\"press on mesh cells\", mesh_path=None):\n","    import meshio\n","    # p = out_dict[\"pressure\"]\n","    index = extract_numbers(mesh_path.name)[0]\n","    index = str(index)\n","\n","    if point_data_pos == \"press on mesh points\":\n","        mesh = meshio.read(mesh_path)\n","        mesh.point_data[\"p\"] = p.cpu().numpy()\n","        # if \"pred wss_x\" in out_dict:\n","        #     wss_x = out_dict[\"pred wss_x\"]\n","        #     mesh.point_data[\"wss_x\"] = wss_x.numpy()\n","    elif point_data_pos == \"press on mesh cells\":\n","        points = np.load(mesh_path.parent / f\"centroid_{index}.npy\")\n","        npoint = points.shape[0]\n","        mesh = meshio.Mesh(\n","            points=points, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n","        )\n","        mesh.point_data = {\"p\": p.cpu().numpy()}\n","    pressure = p.cpu().numpy().flatten()\n","    # print(f\"write : {config.run_name}/output/{mesh_path.parent.name}_{index}.vtk\")\n","    os.makedirs(f\"./result\", exist_ok=True)\n","    # os.makedirs(f\"./result/output_pressure\", exist_ok=True)\n","    np.save(f\"./result/press_{index}.npy\", pressure)\n","    # mesh.write(f\"./runs_trackC/{args.track}/output/{mesh_path.parent.name}_{index}.vtk\")\n","\n","def extract_numbers_from_filename(filename):\n","    # 提取文件名中的数字\n","    numbers = re.findall(r'\\d+', filename)\n","    return numbers\n","\n","def write_numbers_to_file(directory, output_file):\n","    # 打开输出文件\n","    with open(output_file, 'w') as file:\n","        # 遍历目录中的所有文件\n","        for filename in os.listdir(directory):\n","            # 提取文件名中的数字\n","            numbers = extract_numbers_from_filename(filename)\n","            if numbers:\n","                # 写入文件\n","                file.write(' '.join(numbers) + '\\n')\n","\n","\n","def test_epoch(model, metric_func, test_loader, device, model_weight, track='tracka'):\n","    model.load_state_dict(model_weight)\n","    model.eval()\n","    metric_val = []\n","    if track == 'tracka':\n","        for i, data in enumerate(test_loader):\n","            with torch.no_grad():\n","                query_points, y = data['vertices'], data['pressure']\n","                batchsize, points_num = data['vertices'].shape[0], data['vertices'].shape[1]\n","                area = torch.zeros(batchsize, points_num).unsqueeze(-1)\n","                query_points = torch.cat([query_points, area], dim=-1)\n","                query_points, y, = query_points.to(device), y.to(device)\n","                conditions = MultipleTensors(extract_condition(data))\n","                conditions = conditions.to(device)\n","\n","                out = model(query_points, conditions)\n","\n","                y_pred = out.squeeze(-1)\n","                metric = metric_func(y_pred, y)\n","\n","                metric_val.append(metric.cpu().numpy())\n","\n","                write_to_vtk(y_pred.unsqueeze(-1).squeeze(0), point_data_pos='press on mesh points',mesh_path=CARdatasetA.all_test_mesh_path[i])\n","    else:\n","        deep=5\n","        for i, data in enumerate(test_loader):\n","            with torch.no_grad():\n","                query_points, area = data['vertices'], data['area'].unsqueeze(-1)\n","                query_points = query_points.to(device)\n","                conditions = extract_condition(data)\n","                if args.cat_area:\n","                    area = area.to(device)\n","                    query_points = torch.cat([query_points, area], dim=-1)\n","                else:\n","                    conditions.append(area)\n","                conditions = MultipleTensors(conditions)\n","                conditions = conditions.to(device)\n","\n","                out = model(query_points, conditions,deep)\n","\n","                y_pred = out.squeeze(-1)\n","                write_to_vtk(y_pred.unsqueeze(-1).squeeze(0), mesh_path=CARdataset_test.all_test_mesh_path[i])\n","            # metric = metric_func(y_pred, y)\n","\n","            # metric_val.append(metric.cpu().numpy())\n","\n","    # return dict(metric=np.mean(metric_val, axis=0))\n","\n","\n","if __name__ == \"__main__\":\n","    warnings.filterwarnings('ignore')\n","    args = get_args()\n","\n","    if not os.path.exists(f'{args.test_dir}/watertight_meshes.txt'):\n","        directory_path = f'{args.test_dir}/Inference'\n","        output_file_name = f'{args.test_dir}/watertight_meshes.txt'\n","        write_numbers_to_file(directory_path, output_file_name)\n","\n","    # 生成fake pressure\n","    if not os.path.exists(f'{args.test_dir}/Label'):\n","        os.makedirs(f'{args.test_dir}/Label', exist_ok=True)\n","        for file in os.listdir(f'{args.test_dir}/Inference'):\n","            id = file.split('.')[0].split('_')[1]\n","            fake_label = np.ones(3682)\n","            np.save(f'{args.test_dir}/Label/press_{id}.npy', fake_label)\n","\n","\n","    if not args.no_cuda and torch.cuda.is_available():\n","        device = torch.device('cuda:{}'.format(str(args.gpu)))\n","    else:\n","        device = torch.device(\"cpu\")\n","\n","    get_seed(args.seed, printout=False)\n","\n","    train_dir = Path('./Dataset/train_mesh_0603')\n","    test_dir = Path('./Dataset/track_B')\n","    train_cent_dir = Path('./Dataset/train_track_B')\n","    test_aux_dir = Path('./Dataset/track_B/Auxiliary')\n","    CARdatasetA = get_dataset(args)\n","    # test_dataset = get_dataset(args)\n","\n","\n","    test_loaderA = CARdatasetA.test_dataloader(\n","        batch_size=1, shuffle=False\n","    )\n","    CARdataset_test = CarDataset_trackB(train_dir, train_cent_dir, test_dir, mode='test')\n","    # test_dataset = get_dataset(args)\n","\n","    test_loaderB = DataLoader(CARdataset_test,\n","                              batch_size=1, shuffle=False\n","                              )\n","\n","    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n","    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n","\n","    #### set random seeds\n","    get_seed(args.seed)\n","    torch.cuda.empty_cache()\n","\n","    loss_func = LpLoss()\n","    metric_func = LpLoss()\n","\n","    model = get_model(args)\n","    model = model.to(device)\n","\n","    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n","\n","    best_state_dict = torch.load('./best_trackc.pth',map_location='cuda:0')\n","\n","    test_epoch(model, metric_func, test_loaderA, device, best_state_dict)  # test tracka\n","\n","    test_epoch(model, metric_func, test_loaderB, device, best_state_dict, track='trackb')  # test trackb\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oMfabw83kmqT","executionInfo":{"status":"ok","timestamp":1721014162362,"user_tz":-480,"elapsed":30079,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"e00c1961-5127-4396-aa17-7962c951e5cf"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["use 450 training data!\n","use 450 training data!\n","\n","\n","The following code snippets have been run.\n","==================================================\n","\n","    os.environ['PYTHONHASHSEED'] = str(2024)\n","    numpy.random.seed(2024)\n","    torch.manual_seed(2024)\n","    torch.cuda.manual_seed(2024)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(2024)\n","    \n","==================================================\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","\n","Model: MIOEGPT\t Number of params: 1003489\n"]}]},{"cell_type":"code","source":["# !rm -rf result/\n","!zip -r ./B_result.zip ./result/*"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"5hq2LJUH69Ra","executionInfo":{"status":"ok","timestamp":1721014166734,"user_tz":-480,"elapsed":4406,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"b42b472f-f640-47c6-a8d5-6ee465b23ad1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: result/press_10.npy (deflated 11%)\n","  adding: result/press_11.npy (deflated 12%)\n","  adding: result/press_12.npy (deflated 11%)\n","  adding: result/press_13.npy (deflated 11%)\n","  adding: result/press_14.npy (deflated 11%)\n","  adding: result/press_15.npy (deflated 11%)\n","  adding: result/press_16.npy (deflated 11%)\n","  adding: result/press_17.npy (deflated 12%)\n","  adding: result/press_18.npy (deflated 12%)\n","  adding: result/press_19.npy (deflated 11%)\n","  adding: result/press_1.npy (deflated 11%)\n","  adding: result/press_20.npy (deflated 12%)\n","  adding: result/press_21.npy (deflated 12%)\n","  adding: result/press_22.npy (deflated 11%)\n","  adding: result/press_23.npy (deflated 12%)\n","  adding: result/press_24.npy (deflated 11%)\n","  adding: result/press_25.npy (deflated 11%)\n","  adding: result/press_26.npy (deflated 11%)\n","  adding: result/press_27.npy (deflated 11%)\n","  adding: result/press_28.npy (deflated 11%)\n","  adding: result/press_29.npy (deflated 11%)\n","  adding: result/press_2.npy (deflated 11%)\n","  adding: result/press_30.npy (deflated 11%)\n","  adding: result/press_31.npy (deflated 11%)\n","  adding: result/press_32.npy (deflated 11%)\n","  adding: result/press_33.npy (deflated 11%)\n","  adding: result/press_34.npy (deflated 11%)\n","  adding: result/press_35.npy (deflated 12%)\n","  adding: result/press_36.npy (deflated 11%)\n","  adding: result/press_37.npy (deflated 11%)\n","  adding: result/press_38.npy (deflated 12%)\n","  adding: result/press_39.npy (deflated 11%)\n","  adding: result/press_3.npy (deflated 11%)\n","  adding: result/press_40.npy (deflated 11%)\n","  adding: result/press_41.npy (deflated 12%)\n","  adding: result/press_42.npy (deflated 12%)\n","  adding: result/press_43.npy (deflated 11%)\n","  adding: result/press_44.npy (deflated 12%)\n","  adding: result/press_45.npy (deflated 11%)\n","  adding: result/press_46.npy (deflated 11%)\n","  adding: result/press_47.npy (deflated 12%)\n","  adding: result/press_48.npy (deflated 12%)\n","  adding: result/press_49.npy (deflated 11%)\n","  adding: result/press_4.npy (deflated 12%)\n","  adding: result/press_50.npy (deflated 11%)\n","  adding: result/press_5.npy (deflated 11%)\n","  adding: result/press_658.npy (deflated 13%)\n","  adding: result/press_659.npy (deflated 13%)\n","  adding: result/press_660.npy (deflated 13%)\n","  adding: result/press_662.npy (deflated 13%)\n","  adding: result/press_663.npy (deflated 12%)\n","  adding: result/press_664.npy (deflated 13%)\n","  adding: result/press_665.npy (deflated 11%)\n","  adding: result/press_666.npy (deflated 13%)\n","  adding: result/press_667.npy (deflated 12%)\n","  adding: result/press_668.npy (deflated 13%)\n","  adding: result/press_672.npy (deflated 13%)\n","  adding: result/press_673.npy (deflated 13%)\n","  adding: result/press_674.npy (deflated 13%)\n","  adding: result/press_675.npy (deflated 13%)\n","  adding: result/press_676.npy (deflated 12%)\n","  adding: result/press_677.npy (deflated 13%)\n","  adding: result/press_678.npy (deflated 12%)\n","  adding: result/press_679.npy (deflated 13%)\n","  adding: result/press_681.npy (deflated 13%)\n","  adding: result/press_683.npy (deflated 13%)\n","  adding: result/press_684.npy (deflated 13%)\n","  adding: result/press_686.npy (deflated 12%)\n","  adding: result/press_687.npy (deflated 13%)\n","  adding: result/press_688.npy (deflated 13%)\n","  adding: result/press_689.npy (deflated 13%)\n","  adding: result/press_690.npy (deflated 11%)\n","  adding: result/press_691.npy (deflated 13%)\n","  adding: result/press_692.npy (deflated 13%)\n","  adding: result/press_693.npy (deflated 11%)\n","  adding: result/press_695.npy (deflated 12%)\n","  adding: result/press_696.npy (deflated 13%)\n","  adding: result/press_697.npy (deflated 13%)\n","  adding: result/press_6.npy (deflated 11%)\n","  adding: result/press_700.npy (deflated 13%)\n","  adding: result/press_701.npy (deflated 13%)\n","  adding: result/press_702.npy (deflated 12%)\n","  adding: result/press_703.npy (deflated 11%)\n","  adding: result/press_704.npy (deflated 13%)\n","  adding: result/press_705.npy (deflated 13%)\n","  adding: result/press_708.npy (deflated 13%)\n","  adding: result/press_709.npy (deflated 12%)\n","  adding: result/press_710.npy (deflated 11%)\n","  adding: result/press_711.npy (deflated 12%)\n","  adding: result/press_712.npy (deflated 13%)\n","  adding: result/press_713.npy (deflated 13%)\n","  adding: result/press_715.npy (deflated 13%)\n","  adding: result/press_717.npy (deflated 12%)\n","  adding: result/press_718.npy (deflated 13%)\n","  adding: result/press_719.npy (deflated 13%)\n","  adding: result/press_721.npy (deflated 12%)\n","  adding: result/press_722.npy (deflated 13%)\n","  adding: result/press_7.npy (deflated 11%)\n","  adding: result/press_8.npy (deflated 12%)\n","  adding: result/press_9.npy (deflated 12%)\n"]}]},{"cell_type":"markdown","source":["# Train!\n"],"metadata":{"id":"Ce0Ef-gOobJn"}},{"cell_type":"markdown","source":["训练结束后可以在 runs/train/output_pressure下得到输出结果"],"metadata":{"id":"P51Ja8cw0pgk"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","# -*- coding:utf-8 _*-\n","import sys\n","import os\n","\n","sys.path.append('../..')\n","sys.path.append('..')\n","\n","import re\n","import time\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import warnings\n","import random\n","import torch\n","\n","from torch.optim.lr_scheduler import OneCycleLR, StepLR, LambdaLR\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","\n","\n","\n","\n","EPOCH_SCHEDULERS = ['ReduceLROnPlateau', 'StepLR', 'MultiplicativeLR',\n","                    'MultiStepLR', 'ExponentialLR', 'LambdaLR']\n","\n","def check_loss(loss, name):\n","    if torch.isnan(loss):\n","        raise ValueError(f\"{name} is NaN.\")\n","def extract_numbers(s):\n","    return [int(digit) for digit in re.findall(r'\\d+', s)]\n","\n","\n","def write_to_vtk(p, point_data_pos=\"press on mesh cells\", mesh_path=None):\n","    import meshio\n","    # p = out_dict[\"pressure\"]\n","    index = extract_numbers(mesh_path.name)[0]\n","    index = str(index)\n","\n","    if point_data_pos == \"press on mesh points\":\n","        mesh = meshio.read(mesh_path)\n","        mesh.point_data[\"p\"] = p.cpu().numpy()\n","        # if \"pred wss_x\" in out_dict:\n","        #     wss_x = out_dict[\"pred wss_x\"]\n","        #     mesh.point_data[\"wss_x\"] = wss_x.numpy()\n","    elif point_data_pos == \"press on mesh cells\":\n","        points = np.load(mesh_path.parent / f\"centroid_{index}.npy\")\n","        npoint = points.shape[0]\n","        mesh = meshio.Mesh(\n","            points=points, cells=[(\"vertex\", np.arange(npoint).reshape(npoint, 1))]\n","        )\n","        mesh.point_data = {\"p\": p.cpu().numpy()}\n","    pressure = p.cpu().numpy().flatten()\n","    # print(f\"write : {config.run_name}/output/{mesh_path.parent.name}_{index}.vtk\")\n","    os.makedirs(f\"./runs/{args.train_log}/output\", exist_ok=True)\n","    os.makedirs(f\"./runs/{args.train_log}/output_pressure\", exist_ok=True)\n","    np.save(f\"./runs/{args.train_log}/output_pressure/press_{index}.npy\", pressure)\n","    mesh.write(f\"./runs/{args.train_log}/output/{mesh_path.parent.name}_{index}.vtk\")\n","\n","\n","def train(model, loss_func, metric_func,\n","          train_loaderA, train_loaderB, valid_loaderA, valid_loaderB,\n","          optimizer, lr_scheduler,\n","          epochs=10,\n","          writer=None,\n","          device=\"cuda\",\n","          patience=10,\n","          grad_clip=0.999,\n","          start_epoch: int = 0,\n","          model_save_path='./data/checkpoints/',\n","          save_mode='state_dict',  # 'state_dict' or 'entire'\n","          val_epoch=10,\n","          ):\n","    loss_train = []\n","    loss_val = []\n","    loss_epoch = []\n","    lr_history = []\n","    it = 0\n","\n","    if patience is None or patience == 0:\n","        patience = epochs\n","    result = None\n","    start_epoch = start_epoch\n","    end_epoch = start_epoch + epochs\n","    best_val_metricA = np.inf\n","    best_val_metricB = np.inf\n","    best_val_metric = np.inf\n","    best_val_epoch = None\n","    save_mode = 'state_dict' if save_mode is None else save_mode\n","    stop_counter = 0\n","    iter_per_epoch = args.iter_per_epoch\n","    is_epoch_scheduler = any(s in str(lr_scheduler.__class__) for s in EPOCH_SCHEDULERS)\n","    iter_tracka = iter(train_loaderA)\n","    iter_trackb = iter(train_loaderB)\n","\n","    for epoch in range(start_epoch, end_epoch):\n","        model.train()\n","        torch.cuda.empty_cache()\n","        for _ in tqdm(range(iter_per_epoch)):\n","            # print(i)\n","            loss_epoch = []\n","            optimizer.zero_grad()\n","            # 使用loader_a的数据\n","\n","            try:\n","                batch_b = next(iter_trackb)\n","                loss_b = train_batch(model, loss_func, batch_b, optimizer, lr_scheduler, device,\n","                                     grad_clip=grad_clip)\n","                check_loss(loss_b, 'lossb')\n","            except StopIteration:\n","                iter_trackb = iter(train_loaderB)\n","                batch_b = next(iter_trackb)\n","                loss_b = train_batch(model, loss_func, batch_b, optimizer, lr_scheduler, device,\n","                                     grad_clip=grad_clip)\n","                check_loss(loss_b, 'lossb')\n","            try:\n","                batch_a = next(iter_tracka)\n","                loss_a = train_batch(model, loss_func, batch_a, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n","                check_loss(loss_a,'lossa')\n","\n","            except StopIteration:\n","                iter_tracka = iter(train_loaderA)\n","                batch_a = next(iter_tracka)\n","                loss_a = train_batch(model, loss_func, batch_a, optimizer, lr_scheduler, device, grad_clip=grad_clip)\n","                check_loss(loss_a,'lossa')\n","            LOSS = loss_a + loss_b\n","            LOSS.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","            optimizer.step()\n","\n","            if lr_scheduler:\n","                lr_scheduler.step()\n","            loss_epoch.append(LOSS.item())\n","            it += 1\n","            lr = optimizer.param_groups[0]['lr']\n","            writer.add_scalar('LR', lr, it)\n","            lr_history.append(lr)\n","        _loss_mean = np.mean(loss_epoch)\n","        #     for j in range(len(_loss_mean)):\n","        #         log += \" | loss {}: {:.6f}\".format(j, _loss_mean[j])\n","        # log += \" | current lr: {:.3e}\".format(lr)\n","        #\n","        # if it % print_freq == 0:\n","        #     print(log)\n","\n","        if writer is not None:\n","            writer.add_scalar(\"train_loss\", _loss_mean, epoch)  #### loss 0 seems to be the sum of all loss\n","\n","        loss_train.append(_loss_mean)\n","        if epoch % val_epoch == 0:\n","            val_resultA = validate_epoch(model, metric_func, valid_loaderA, device)  # valid tracka\n","            val_resultB = validate_epoch(model, metric_func, valid_loaderB, device, track='trackb')  # valid trackb\n","\n","            # loss_val.append(val_result[\"metric\"])\n","            val_metricA = val_resultA[\"metric\"]\n","            val_metricB = val_resultB[\"metric\"]\n","            val_metric_mean = (val_metricA + val_metricB) / 2\n","            print(\n","                f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f} val_lossA:{val_metricA:.4f} val_lossB:{val_metricB:.4f} val_loss:{val_metric_mean:.4f} lr:{lr_history[-1]:.6f}')\n","        else:\n","            print(\n","                f'epoch:[{epoch + 1}/{end_epoch}] train_loss:{_loss_mean:.4f} lr:{lr_history[-1]:.6f}')\n","        if val_metricA < best_val_metricA:\n","            best_val_epoch = epoch\n","            best_val_metricA = val_metricA\n","            torch.save(model.state_dict(), os.path.join(model_save_path, 'best_valA.pth'))\n","\n","        if val_metricB < best_val_metricB:\n","            best_val_epoch = epoch\n","            best_val_metricB = val_metricB\n","            torch.save(model.state_dict(), os.path.join(model_save_path, 'best_valB.pth'))\n","\n","        if val_metric_mean < best_val_metric:\n","            best_val_epoch = epoch\n","            best_val_metric = val_metric_mean\n","            torch.save(model.state_dict(), os.path.join(model_save_path, 'best_valMean.pth'))\n","\n","        if lr_scheduler and is_epoch_scheduler:\n","            if 'ReduceLROnPlateau' in str(lr_scheduler.__class__):\n","                lr_scheduler.step(val_metric_mean)\n","            else:\n","                lr_scheduler.step()\n","\n","        if writer is not None:\n","            writer.add_scalar('val lossA', val_metricA, epoch)\n","            writer.add_scalar('val lossB', val_metricB, epoch)\n","            writer.add_scalar('val loss Mean', val_metric_mean, epoch)\n","\n","        torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n","        result = dict(\n","            # best_val_epoch=best_val_epoch,\n","            # best_val_metric=best_val_metric,\n","            # loss_train=np.asarray(loss_train),\n","            # loss_val=np.asarray(loss_val),\n","            # lr_history=np.asarray(lr_history),\n","            # best_model=best_model_state_dict,\n","            # optimizer_state=optimizer.state_dict()\n","        )\n","        # pickle.dump(result, open(os.path.join(model_save_path, result_name), 'wb'))\n","    return result\n","\n","\n","def train_batch(model, loss_func, data, optimizer, lr_scheduler, device, grad_clip=0.999):\n","    # scaler = GradScaler()\n","    # optimizer.zero_grad()\n","    flag = data['track'][0]\n","    if data['track'][0].squeeze(0).argmax() == 0:  # tracka\n","        batchsize, points_num = data['vertices'].shape[0], data['vertices'].shape[1]\n","        query_points, y, area = data['vertices'], data['pressure'], torch.zeros(batchsize, points_num).unsqueeze(-1)\n","        query_points = torch.cat([query_points, area], dim=-1)  # make sure all data have same input chanels\n","        query_points, y = query_points.to(device), y.to(device)\n","        conditions = extract_condition(data)\n","        deep = None\n","    else:  # trackb\n","        query_points, y, area = data['vertices'], data['pressure'], data['area'].unsqueeze(-1)\n","        query_points, y = query_points.to(device), y.to(device),\n","        area = area.to(device)\n","        query_points = torch.cat([query_points, area], dim=-1)\n","        conditions = extract_condition(data)\n","        deep = 5\n","    conditions = MultipleTensors(conditions)\n","    conditions = conditions.to(device)\n","\n","    out = model(query_points, conditions,deep)\n","    if torch.isnan(out).any() or torch.isinf(out).any():\n","        raise ValueError(\"Model output contains NaN or Inf values.\")\n","\n","    y_pred = out.squeeze(-1)\n","    loss = loss_func(y_pred, y)\n","    # check_loss(loss,'loss')\n","    # loss.backward()\n","    # nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","    # optimizer.step()\n","    #\n","    # if lr_scheduler:\n","    #     lr_scheduler.step()\n","\n","    return loss\n","\n","\n","def validate_epoch(model, metric_func, valid_loader, device, track='tracka'):\n","    model.eval()\n","    metric_val = []\n","    if track == 'tracka':\n","        for i, data in enumerate(valid_loader):\n","            with torch.no_grad():\n","                query_points, y = data['vertices'], data['pressure']\n","                batchsize, points_num = data['vertices'].shape[0], data['vertices'].shape[1]\n","                area = torch.zeros(batchsize, points_num).unsqueeze(-1)\n","                query_points = torch.cat([query_points, area], dim=-1)\n","                query_points, y, = query_points.to(device), y.to(device)\n","                conditions = MultipleTensors(extract_condition(data))\n","                conditions = conditions.to(device)\n","                out = model(query_points, conditions)\n","                y_pred = out.squeeze(-1)\n","                metric = metric_func(y_pred, y)\n","                metric_val.append(metric.cpu().numpy())\n","    else:\n","        deep = 5\n","        for i, data in enumerate(valid_loader):\n","            with torch.no_grad():\n","                query_points, y, area = data['vertices'], data['pressure'], data['area'].unsqueeze(-1)\n","                query_points, y = query_points.to(device), y.to(device),\n","                conditions = extract_condition(data)\n","                if args.cat_area:\n","                    area = area.to(device)\n","                    query_points = torch.cat([query_points, area], dim=-1)\n","                else:\n","                    conditions.append(area)\n","                conditions = MultipleTensors(conditions)\n","                conditions = conditions.to(device)\n","\n","                out = model(query_points, conditions, deep)\n","\n","                y_pred = out.squeeze(-1)\n","                metric = metric_func(y_pred, y)\n","\n","                metric_val.append(metric.cpu().numpy())\n","\n","    return dict(metric=np.mean(metric_val, axis=0))\n","\n","\n","def test_epoch(model, metric_func, test_loader, device, model_weight, track='tracka'):\n","    model.load_state_dict(model_weight)\n","    model.eval()\n","    metric_val = []\n","    if track == 'tracka':\n","        for i, data in enumerate(test_loader):\n","            with torch.no_grad():\n","                query_points, y = data['vertices'], data['pressure']\n","                batchsize, points_num = data['vertices'].shape[0], data['vertices'].shape[1]\n","                area = torch.zeros(batchsize, points_num).unsqueeze(-1)\n","                query_points = torch.cat([query_points, area], dim=-1)\n","                query_points, y, = query_points.to(device), y.to(device)\n","                conditions = MultipleTensors(extract_condition(data))\n","                conditions = conditions.to(device)\n","\n","                out = model(query_points, conditions)\n","\n","                y_pred = out.squeeze(-1)\n","                metric = metric_func(y_pred, y)\n","\n","                metric_val.append(metric.cpu().numpy())\n","\n","                write_to_vtk(y_pred.unsqueeze(-1).squeeze(0), point_data_pos='press on mesh points',\n","                             mesh_path=CARdatasetA.all_test_mesh_path[i])\n","    else:\n","        deep = 5\n","        for i, data in enumerate(test_loader):\n","            with torch.no_grad():\n","                query_points, area = data['vertices'], data['area'].unsqueeze(-1)\n","                query_points = query_points.to(device)\n","                conditions = extract_condition(data)\n","                if args.cat_area:\n","                    area = area.to(device)\n","                    query_points = torch.cat([query_points, area], dim=-1)\n","                else:\n","                    conditions.append(area)\n","                conditions = MultipleTensors(conditions)\n","                conditions = conditions.to(device)\n","\n","                out = model(query_points, conditions, deep)\n","\n","                y_pred = out.squeeze(-1)\n","                write_to_vtk(y_pred.unsqueeze(-1).squeeze(0), mesh_path=CARdataset_test.all_test_mesh_path[i])\n","            # metric = metric_func(y_pred, y)\n","\n","            # metric_val.append(metric.cpu().numpy())\n","\n","    # return dict(metric=np.mean(metric_val, axis=0))\n","\n","\n","if __name__ == \"__main__\":\n","    warnings.filterwarnings('ignore')\n","    args = get_args()\n","    print(args)\n","    if args.use_tb:\n","        writer = SummaryWriter(f'runs/{args.train_log}')\n","\n","    else:\n","        writer = None\n","        log_path = None\n","    if not args.no_cuda and torch.cuda.is_available():\n","        device = torch.device('cuda:{}'.format(str(args.gpu)))\n","    else:\n","        device = torch.device(\"cpu\")\n","    # 获取命令行参数\n","    arg = sys.argv[1:]\n","\n","    # 将参数写入文件\n","    with open(f'runs/{args.train_log}/args.txt', 'w') as f:\n","        for a in arg:\n","            f.write(a + '\\n')\n","\n","    kwargs = {'pin_memory': False} if args.gpu else {}\n","    get_seed(args.seed, printout=False)\n","\n","    train_dir = Path('./Dataset/train_mesh_0603')\n","    test_dir = Path('./Dataset/track_B')\n","    train_cent_dir = Path('./Dataset/train_track_B')\n","    test_aux_dir = Path('./Dataset/track_B/Auxiliary')\n","    CARdatasetA = get_dataset(args)\n","    # test_dataset = get_dataset(args)\n","\n","    train_loaderA = CARdatasetA.train_dataloader(\n","        batch_size=args.batch_size, shuffle=True\n","    )\n","    val_loaderA = CARdatasetA.val_dataloader(\n","        batch_size=1, shuffle=False\n","    )\n","    test_loaderA = CARdatasetA.test_dataloader(\n","        batch_size=1, shuffle=False\n","    )\n","\n","    CARdataset_train = CarDataset_trackB(train_dir, train_cent_dir, test_dir, val_ratio=args.val_ratio)\n","    valid_val_indices = CARdataset_train.get_valid_val_indices()\n","    CARdataset_val = CarDataset_trackB(train_dir, train_cent_dir, test_dir, mode='val', val_ratio=args.val_ratio,\n","                                       valid_val_indices=valid_val_indices)\n","    CARdataset_test = CarDataset_trackB(train_dir, train_cent_dir, test_dir, mode='test')\n","    # test_dataset = get_dataset(args)\n","\n","    train_loaderB = DataLoader(CARdataset_train,\n","                               batch_size=1, shuffle=True\n","                               )\n","    val_loaderB = DataLoader(CARdataset_val,\n","                             batch_size=1, shuffle=False\n","                             )\n","    test_loaderB = DataLoader(CARdataset_test,\n","                              batch_size=1, shuffle=False\n","                              )\n","\n","    # args.space_dim = int(re.search(r'\\d', args.dataset).group())\n","    # args.normalizer =  train_dataset.y_normalizer.to(device) if train_dataset.y_normalizer is not None else None\n","\n","    #### set random seeds\n","    get_seed(args.seed)\n","    torch.cuda.empty_cache()\n","\n","    loss_func = LpLoss()\n","    metric_func = LpLoss()\n","\n","    model = get_model(args)\n","    model = model.to(device)\n","\n","    print(f\"\\nModel: {model.__name__}\\t Number of params: {get_num_params(model)}\")\n","\n","    # path_prefix = args.dataset + '_{}_'.format(args.component) + model.__name__ + args.comment + time.strftime(\n","    #     '_%m%d_%H_%M_%S')\n","    # model_path, result_path = path_prefix + '.pt', path_prefix + '.pkl'\n","\n","    # print(f\"Saving model and result in ./../models/checkpoints/{model_path}\\n\")\n","\n","    # print(model)\n","    # print(config)\n","\n","    epochs = args.epochs\n","    lr = args.lr\n","\n","    if args.optimizer == 'Adam':\n","        optimizer = Adam(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n","    elif args.optimizer == \"AdamW\":\n","        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=args.weight_decay, betas=(0.9, 0.999))\n","    else:\n","        raise NotImplementedError\n","\n","    if args.lr_method == 'cycle':\n","        print('Using cycle learning rate schedule')\n","        scheduler = OneCycleLR(optimizer, max_lr=lr, div_factor=1e4, pct_start=0.2, final_div_factor=1e4,\n","                               steps_per_epoch=args.iter_per_epoch, epochs=epochs)\n","    elif args.lr_method == 'step':\n","        print('Using step learning rate schedule')\n","        scheduler = StepLR(optimizer, step_size=args.lr_step_size * args.iter_per_epoch, gamma=0.5)\n","    elif args.lr_method == 'warmup':\n","        print('Using warmup learning rate schedule')\n","        scheduler = LambdaLR(optimizer, lambda steps: min((steps + 1) / (args.warmup_epochs * 200),\n","                                                          np.power(\n","                                                              args.warmup_epochs * 200 / float(steps + 1),\n","                                                              0.5)))\n","\n","    time_start = time.time()\n","\n","    result = train(model, loss_func, metric_func,\n","                   train_loaderA, train_loaderB,\n","                   val_loaderA, val_loaderB,\n","                   optimizer, scheduler,\n","                   epochs=epochs,\n","                   grad_clip=args.grad_clip,\n","                   patience=10,\n","                   model_save_path=f'runs/{args.train_log}',\n","                   writer=writer,\n","                   device=device,\n","                   val_epoch=args.val_epochs)\n","\n","    print('Training takes {} seconds.'.format(time.time() - time_start))\n","\n","    # result['args'], result['config'] = args, config\n","    # checkpoint = {'args': args, 'model': model.state_dict(), 'optimizer': optimizer.state_dict()}\n","    torch.save(model.state_dict(), os.path.join(f'runs/{args.train_log}/last.pth'))\n","    # model.eval()\n","    best_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'best_valMean.pth'))\n","    last_state_dict = torch.load(os.path.join(f'runs/{args.train_log}', 'last.pth'))\n","\n","    test_epoch(model, metric_func, test_loaderA, device, best_state_dict)  # test tracka\n","\n","    test_epoch(model, metric_func, test_loaderB, device, best_state_dict, track='trackb')  # test trackb\n","\n","    # with open(f'runs/{args.train_log}/result.txt', 'w') as f:\n","    #     f.write(f\"Best model's validation metric in this run: {val_metric_best['metric']} \\n\")\n","    #     f.write(f\"Last model's validation metric in this run: {val_metric_last['metric']} \\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"85tvzBxnocZH","executionInfo":{"status":"error","timestamp":1721014828986,"user_tz":-480,"elapsed":662277,"user":{"displayName":"知否","userId":"12758421836332615961"}},"outputId":"6dfa2a36-e8c3-4665-a0ae-5a78b5fea6ed"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(dataset='ns2d', train_dir='./Dataset/data', test_dir='./Dataset/track_A', wind_direction='z', track='track_A', train_log='train', expand_data=0, val_ratio=0.1, p=0.5, iter_per_epoch=250, cat_area=1, use_transform=0, component='all', seed=2024, gpu=0, use_tb=1, comment='', train_num='all', test_num='all', sort_data=0, normalize_x='unit', use_normalizer='unit', epochs=250, val_epochs=10, optimizer='AdamW', lr=0.001, weight_decay=5e-05, grad_clip=1000.0, batch_size=8, val_batch_size=8, no_cuda=0, lr_method='cycle', lr_step_size=20, warmup_epochs=50, loss_name='rel2', model_name='GNOT', n_hidden=32, n_layers=16, act='gelu', loss='default', model='GNOT', n_head=4, slice_num=64, agent_num=32, ffn_dropout=0.0, attn_dropout=0.0, mlp_layers=3, attn_type='linear', hfourier_dim=0, n_experts=2, input_dim=4, output_dim=1, space_dim=3, branch_sizes=[3, 3], n_inner=4)\n","use 450 training data!\n","use 450 training data!\n","use 450 training data!\n","use 450 training data!\n","\n","\n","The following code snippets have been run.\n","==================================================\n","\n","    os.environ['PYTHONHASHSEED'] = str(2024)\n","    numpy.random.seed(2024)\n","    torch.manual_seed(2024)\n","    torch.cuda.manual_seed(2024)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(2024)\n","    \n","==================================================\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","Using Linear Attention\n","\n","Model: MIOEGPT\t Number of params: 1003489\n","Using cycle learning rate schedule\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [07:42<00:00,  1.85s/it]\n"]},{"output_type":"stream","name":"stdout","text":["epoch:[1/250] train_loss:1.2343 val_lossA:0.9743 val_lossB:0.2430 val_loss:0.6087 lr:0.000001\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▋      | 91/250 [02:50<04:57,  1.87s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-c54241562331>\u001b[0m in \u001b[0;36m<cell line: 336>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     result = train(model, loss_func, metric_func,\n\u001b[0m\u001b[1;32m    446\u001b[0m                    \u001b[0mtrain_loaderA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loaderB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                    \u001b[0mval_loaderA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loaderB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c54241562331>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, metric_func, train_loaderA, train_loaderB, valid_loaderA, valid_loaderB, optimizer, lr_scheduler, epochs, writer, device, patience, grad_clip, start_epoch, model_save_path, save_mode, val_epoch)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mloss_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOSS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}